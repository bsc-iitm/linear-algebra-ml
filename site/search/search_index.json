{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Foundations \u00b6 Notes for the MLF course","title":"Home"},{"location":"#machine-learning-foundations","text":"Notes for the MLF course","title":"Machine Learning Foundations"},{"location":"contribute/","text":"Contribute \u00b6 The notes are prepared using markdown. Markdown editor \u00b6 There are quite a few markdown editors out there. Here are three options: Typora Marktext Visual Studio Code BSc-GitHub \u00b6 Log in to the BSc-GitHub page with your online degree account. Clone \u00b6 Clone the MLF repository . If you are on a Linux system or are using bash for Windows , you can run the following command: git clone https://github.com/bsc-iitm/machine-learning-foundations.git Edit a chapter \u00b6 All the chapters of the notes are arranged in the form of markdown files. There is one markdown file for each concept. These files are organized in a folder called docs . You can open any one chapter and start editing it or you can add a new chapter. Markdown is quite easy to learn. Typora has one of the best tutorials for learning markdown. Navigation \u00b6 In case you are adding a new chapter, you have to add a navigation link in the file mkdocs.yml . Refer to this file and look at the section nav to get an idea of how navigation works. Mkdocs \u00b6 If you want to see how the notes looks like, you need to install a tool called mkdocs . You can install it as follows: pip install mkdocs-material To see how the site looks, you would have to serve it: mkdocs serve When you serve a page, you can access it via local host: http://127.0.0.1:8000/","title":"Contribute"},{"location":"contribute/#contribute","text":"The notes are prepared using markdown.","title":"Contribute"},{"location":"contribute/#markdown-editor","text":"There are quite a few markdown editors out there. Here are three options: Typora Marktext Visual Studio Code","title":"Markdown editor"},{"location":"contribute/#bsc-github","text":"Log in to the BSc-GitHub page with your online degree account.","title":"BSc-GitHub"},{"location":"contribute/#clone","text":"Clone the MLF repository . If you are on a Linux system or are using bash for Windows , you can run the following command: git clone https://github.com/bsc-iitm/machine-learning-foundations.git","title":"Clone"},{"location":"contribute/#edit-a-chapter","text":"All the chapters of the notes are arranged in the form of markdown files. There is one markdown file for each concept. These files are organized in a folder called docs . You can open any one chapter and start editing it or you can add a new chapter. Markdown is quite easy to learn. Typora has one of the best tutorials for learning markdown.","title":"Edit a chapter"},{"location":"contribute/#navigation","text":"In case you are adding a new chapter, you have to add a navigation link in the file mkdocs.yml . Refer to this file and look at the section nav to get an idea of how navigation works.","title":"Navigation"},{"location":"contribute/#mkdocs","text":"If you want to see how the notes looks like, you need to install a tool called mkdocs . You can install it as follows: pip install mkdocs-material To see how the site looks, you would have to serve it: mkdocs serve When you serve a page, you can access it via local host: http://127.0.0.1:8000/","title":"Mkdocs"},{"location":"appendix/cauchy_schwarz/","text":"Cauchy-Schwarz Inequality \u00b6 Cauchy-Schwarz inequality is a popular inequality that can be derived from the idea of projections. This is the statement of the inequality: Statement \u00b6 If \\(x\\) and \\(y\\) are two vectors in \\(\\mathbb{R}^{n}\\) , then: \\[ |x^T y| \\leq ||x|| \\cdot ||y|| \\] Projection \u00b6 If \\(y = 0\\) , then \\(x^Ty = 0\\) and \\(||y||=0\\) . The inequality holds in this case. For the rest of the proof, we will assume that \\(y \\neq 0\\) . First, let us look at the projection of \\(x\\) on \\(y\\) . This is going to be some scalar multiple of \\(y\\) that we call \\(ty\\) . Since the error vector is orthogonal to \\(y\\) , we have: \\[ \\begin{aligned} e^T y &= 0\\\\\\\\ (x - ty)^T y &= 0\\\\\\\\ (x^T - ty^T) y &=0\\\\\\\\ x^Ty - ty^Ty &= 0\\\\\\\\ \\therefore\\ t &= \\cfrac{x^Ty}{y^Ty} \\end{aligned} \\] Note that we have used the fact that \\(y^Ty > 0\\) in the last step. The projection of \\(x\\) on \\(y\\) is therefore: \\[ \\cfrac{x^Ty}{y^Ty} y \\] Now that we have the projection, we can move to the inequality. Inequality \u00b6 The basic idea behind the inequality is that the length of the error vector, \\(||e||\\) , is greater than or equal to zero. It is zero if and only if \\(x\\) is parallel to the vector \\(y\\) : \\[ \\begin{aligned} ||e||^2 &\\geq 0\\\\\\\\ e^T e &\\geq 0\\\\\\\\ (x - ty)^T(x - ty) &\\geq 0\\\\\\\\ x^Tx - 2 t x^T y + t^2y^Ty &\\geq 0 \\end{aligned} \\] Let us now substitute \\(t = \\cfrac{x^Ty}{y^Ty}\\) : \\[ \\begin{aligned} x^Tx - \\cfrac{(x^Ty)^2}{y^Ty} &\\geq 0\\\\\\\\ (x^T y)^2 &\\leq (x^Tx) (y^Ty)\\\\\\\\ |x^Ty| &\\leq ||x|| \\cdot ||y|| \\end{aligned} \\] The equality occurs when \\(x = ty\\) or when \\(x\\) is parallel (anti-parallel) to \\(y\\) .","title":"Cauchy-Schwarz Inequality"},{"location":"appendix/cauchy_schwarz/#cauchy-schwarz-inequality","text":"Cauchy-Schwarz inequality is a popular inequality that can be derived from the idea of projections. This is the statement of the inequality:","title":"Cauchy-Schwarz Inequality"},{"location":"appendix/cauchy_schwarz/#statement","text":"If \\(x\\) and \\(y\\) are two vectors in \\(\\mathbb{R}^{n}\\) , then: \\[ |x^T y| \\leq ||x|| \\cdot ||y|| \\]","title":"Statement"},{"location":"appendix/cauchy_schwarz/#projection","text":"If \\(y = 0\\) , then \\(x^Ty = 0\\) and \\(||y||=0\\) . The inequality holds in this case. For the rest of the proof, we will assume that \\(y \\neq 0\\) . First, let us look at the projection of \\(x\\) on \\(y\\) . This is going to be some scalar multiple of \\(y\\) that we call \\(ty\\) . Since the error vector is orthogonal to \\(y\\) , we have: \\[ \\begin{aligned} e^T y &= 0\\\\\\\\ (x - ty)^T y &= 0\\\\\\\\ (x^T - ty^T) y &=0\\\\\\\\ x^Ty - ty^Ty &= 0\\\\\\\\ \\therefore\\ t &= \\cfrac{x^Ty}{y^Ty} \\end{aligned} \\] Note that we have used the fact that \\(y^Ty > 0\\) in the last step. The projection of \\(x\\) on \\(y\\) is therefore: \\[ \\cfrac{x^Ty}{y^Ty} y \\] Now that we have the projection, we can move to the inequality.","title":"Projection"},{"location":"appendix/cauchy_schwarz/#inequality","text":"The basic idea behind the inequality is that the length of the error vector, \\(||e||\\) , is greater than or equal to zero. It is zero if and only if \\(x\\) is parallel to the vector \\(y\\) : \\[ \\begin{aligned} ||e||^2 &\\geq 0\\\\\\\\ e^T e &\\geq 0\\\\\\\\ (x - ty)^T(x - ty) &\\geq 0\\\\\\\\ x^Tx - 2 t x^T y + t^2y^Ty &\\geq 0 \\end{aligned} \\] Let us now substitute \\(t = \\cfrac{x^Ty}{y^Ty}\\) : \\[ \\begin{aligned} x^Tx - \\cfrac{(x^Ty)^2}{y^Ty} &\\geq 0\\\\\\\\ (x^T y)^2 &\\leq (x^Tx) (y^Ty)\\\\\\\\ |x^Ty| &\\leq ||x|| \\cdot ||y|| \\end{aligned} \\] The equality occurs when \\(x = ty\\) or when \\(x\\) is parallel (anti-parallel) to \\(y\\) .","title":"Inequality"},{"location":"appendix/density_estimation/","text":"Density Estimation \u00b6 Let us say that we have a dataset given to us: \\[ D = \\{x_1, \\cdots, x_n\\} \\] We don't have access to the labels corresponding to these data points. What do we do in this case? One approach is to try and understand the data using probabilistic methods. The basic question we ask is this: Question How is this dataset generated? We could think of some underlying probability distribution that is generating this data. In other words, there is some probability distribution from which this data is sampled. Let us call the density of probability distribution \\(P(x)\\) and its parameters \\(\\theta\\) . In this case, the likelihood of observing this data is: $$ L(\\theta\\ |\\ D) = P(x_1) \\cdots P(x_n) $$ One important assumption here is that the data-points are independently and identically distributed \u2014 i.i.d . This is what lets us use the same density for all the points and also express the likelihood as a product of the densities. Once we have this, we need to maximize the likelihood: \\[ \\max L(\\theta\\ |\\ D) \\] As probabilities are small numbers, it is easier to work with log probabilities: \\[ \\max \\log (L(\\theta\\ | \\ D)) \\] Rather than maximizing the log likelihood, we choose to minimize the negative log likelihood: \\[ \\min -\\log(L(\\theta\\ |\\ D)) \\] Expanding this out, we get: \\[ \\min \\sum \\limits_{i = 1}^{n} -\\log P(x_{i}) \\] The arguments of the optimization problem are the parameters of the probability distribution \u2014 \\(\\theta\\) \u2014 that we choose to model the data. For example, if we choose a Gaussian distribution, then the parameters would be the mean and the variance.","title":"Density Estimation"},{"location":"appendix/density_estimation/#density-estimation","text":"Let us say that we have a dataset given to us: \\[ D = \\{x_1, \\cdots, x_n\\} \\] We don't have access to the labels corresponding to these data points. What do we do in this case? One approach is to try and understand the data using probabilistic methods. The basic question we ask is this: Question How is this dataset generated? We could think of some underlying probability distribution that is generating this data. In other words, there is some probability distribution from which this data is sampled. Let us call the density of probability distribution \\(P(x)\\) and its parameters \\(\\theta\\) . In this case, the likelihood of observing this data is: $$ L(\\theta\\ |\\ D) = P(x_1) \\cdots P(x_n) $$ One important assumption here is that the data-points are independently and identically distributed \u2014 i.i.d . This is what lets us use the same density for all the points and also express the likelihood as a product of the densities. Once we have this, we need to maximize the likelihood: \\[ \\max L(\\theta\\ |\\ D) \\] As probabilities are small numbers, it is easier to work with log probabilities: \\[ \\max \\log (L(\\theta\\ | \\ D)) \\] Rather than maximizing the log likelihood, we choose to minimize the negative log likelihood: \\[ \\min -\\log(L(\\theta\\ |\\ D)) \\] Expanding this out, we get: \\[ \\min \\sum \\limits_{i = 1}^{n} -\\log P(x_{i}) \\] The arguments of the optimization problem are the parameters of the probability distribution \u2014 \\(\\theta\\) \u2014 that we choose to model the data. For example, if we choose a Gaussian distribution, then the parameters would be the mean and the variance.","title":"Density Estimation"},{"location":"appendix/gradients/","text":"Gradients \u00b6 Question How do we compute the gradient of the expression \\(||X\\theta - y||^2\\) with respect to \\(\\theta\\) ? Setting \u00b6 Let us consider the objective function in the minimzation problem for linear regression: \\[ L = ||X\\theta - y||^2 \\] This can be expressed as: \\[ L = (X\\theta - y)^T(X\\theta - y) \\] The gradient of \\(L\\) with respect to \\(\\theta\\) is given by: \\[ \\nabla_{\\theta} L = \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\\\ \\vdots\\\\ \\cfrac{\\partial L}{\\partial \\theta_n} \\end{bmatrix} \\] How do we compute this gradient? First, let us rewrite the expression by introducing an error vector, \\(e = X\\theta - y\\) : \\[ L = e^Te \\] The gradient of \\(L\\) with respect to \\(\\theta\\) can be obtained in a two step process: \\(\\nabla_e L\\) : first compute the gradient of \\(L\\) with respect to the error Use the chain rule of differentiation to compute the gradient of \\(L\\) with respect to \\(\\theta\\) The chain rule when multiple variables are involved can be a bit tricky. This is what will take up next. Chain rule for multiple variables \u00b6 Warning Heavy use of algebra. Go slowly. Intuitively, what does \\(\\frac{\\partial L}{\\partial \\theta_i}\\) mean? This partial derivative measures the effect on the loss \\(L\\) when \\(x_i\\) is perturbed a little keeping all other variables constant. This perturbation propagates to \\(L\\) via \\(e\\) . There is a chain reaction: \\(x_1\\) affects some of the variables in the vector \\(e\\) , which in turn affects \\(L\\) . To get a better feel for this, let us take an example: \\[ \\begin{aligned} e &= X\\theta - b\\\\\\\\ \\begin{bmatrix} e_1\\\\ e_2\\\\ e_3 \\end{bmatrix} &= \\begin{bmatrix} x_{11} & x_{12}\\\\ x_{21} & x_{22}\\\\ x_{31} & x_{32}\\\\ \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\theta_2 \\end{bmatrix} - \\begin{bmatrix} y_1\\\\ y_2 \\end{bmatrix}\\\\\\\\ L &= e^T e = e_1^2 + e_2^2 + e_3^2 \\end{aligned} \\] The variable \\(\\theta_1\\) influences \\(L\\) via three independent routes: \\(\\theta_1 \\rightarrow e_1 \\rightarrow L\\) \\(\\theta_1 \\rightarrow e_2 \\rightarrow L\\) \\(\\theta_1 \\rightarrow e_3 \\rightarrow L\\) The partial derivative needs to take into account the contributions from each of these three routes. Pictorially: Symbolically: \\[ \\cfrac{\\partial L}{\\partial \\theta_1} = \\cfrac{\\partial L}{\\partial e_1} \\cfrac{\\partial e_1}{\\partial \\theta_1} + \\cfrac{\\partial L}{\\partial e_2} \\cfrac{\\partial e_2}{\\partial \\theta_1} + \\cfrac{\\partial L}{\\partial e_3} \\cfrac{\\partial e_3}{\\partial \\theta_1} \\] Observe that the RHS is in the form of a dot product between two vectors: \\[ \\cfrac{\\partial L}{\\partial \\theta_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_1} & \\cfrac{\\partial e_2}{\\partial \\theta_1} & \\cfrac{\\partial e_3}{\\partial \\theta_1} \\end{bmatrix} \\begin{bmatrix} \\cfrac{\\partial L}{\\partial e_1}\\\\ \\cfrac{\\partial L}{\\partial e_2}\\\\ \\cfrac{\\partial L}{\\partial e_3} \\end{bmatrix} \\] Notice, that the second vector is nothing but \\(\\nabla_e L\\) : $$ \\cfrac{\\partial L}{\\partial \\theta_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_1} & \\cfrac{\\partial e_2}{\\partial \\theta_1} & \\cfrac{\\partial e_3}{\\partial \\theta_1} \\end{bmatrix} \\nabla_e L $$ We can now add \\(\\cfrac{\\partial L}{\\partial \\theta_2}\\) to the mix: \\[ \\begin{aligned} \\nabla_{\\theta} L &= \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\\\ \\cfrac{\\partial L}{\\partial \\theta_2} \\end{bmatrix}\\\\ \\\\ &= \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_1} & \\cfrac{\\partial e_2}{\\partial \\theta_1} & \\cfrac{\\partial e_3}{\\partial \\theta_1}\\\\ \\cfrac{\\partial e_1}{\\partial \\theta_2} & \\cfrac{\\partial e_2}{\\partial \\theta_2} & \\cfrac{\\partial e_3}{\\partial \\theta_2} \\end{bmatrix} \\nabla_e L\\\\\\\\ &= J\\ \\nabla_e L \\end{aligned} \\] The matrix \\(J\\) is called the Jacobian matrix. It represents the partial derivatives of each component of the vector \\(e\\) with respect to each component of the vector, \\(\\theta\\) . That is, every element of the Jacobian is of the form: \\[ J_{ij} = \\cfrac{\\partial e_j}{\\partial \\theta_i} \\] Back to the gradients \u00b6 Let us resume our computation, but this time with a general \\(m \\times n\\) matrix \\(X\\) , \\(m\\) dimensional vector \\(y\\) and \\(n\\) dimensional vector \\(\\theta\\) . Since \\(L = e^Te\\) , we have: \\[ \\nabla_e L = 2 e \\] We now need to compute the Jacobian. We have \\(e = X\\theta - y\\) . Since \\(y\\) is a constant, it will vanish while computing the derivatives. We only need to worry about \\(X\\theta\\) for which we can use the following relationship. If \\(x_i\\) is the \\(i^{th}\\) column of \\(A\\) , then: \\[ X\\theta = \\theta_1x_1 + \\cdots + \\theta_nx_n \\] The \\(i^{th}\\) row of the Jacobian is: \\[ \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_i} & \\cdots & \\cfrac{\\partial e_m}{\\partial \\theta_i} \\end{bmatrix} = x_i^T \\] As simple as that: the \\(i^{th}\\) row of the Jacobian is just the \\(i^{th}\\) column of \\(X\\) . So, the Jacobian is nothing but \\(X^T\\) . We now have the complete expression for \\(\\nabla_{\\theta} L\\) : \\[ \\nabla_{\\theta} L = 2 X^T (X\\theta - y) = 2(X^TX \\theta - X^T y) \\]","title":"Gradients"},{"location":"appendix/gradients/#gradients","text":"Question How do we compute the gradient of the expression \\(||X\\theta - y||^2\\) with respect to \\(\\theta\\) ?","title":"Gradients"},{"location":"appendix/gradients/#setting","text":"Let us consider the objective function in the minimzation problem for linear regression: \\[ L = ||X\\theta - y||^2 \\] This can be expressed as: \\[ L = (X\\theta - y)^T(X\\theta - y) \\] The gradient of \\(L\\) with respect to \\(\\theta\\) is given by: \\[ \\nabla_{\\theta} L = \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\\\ \\vdots\\\\ \\cfrac{\\partial L}{\\partial \\theta_n} \\end{bmatrix} \\] How do we compute this gradient? First, let us rewrite the expression by introducing an error vector, \\(e = X\\theta - y\\) : \\[ L = e^Te \\] The gradient of \\(L\\) with respect to \\(\\theta\\) can be obtained in a two step process: \\(\\nabla_e L\\) : first compute the gradient of \\(L\\) with respect to the error Use the chain rule of differentiation to compute the gradient of \\(L\\) with respect to \\(\\theta\\) The chain rule when multiple variables are involved can be a bit tricky. This is what will take up next.","title":"Setting"},{"location":"appendix/gradients/#chain-rule-for-multiple-variables","text":"Warning Heavy use of algebra. Go slowly. Intuitively, what does \\(\\frac{\\partial L}{\\partial \\theta_i}\\) mean? This partial derivative measures the effect on the loss \\(L\\) when \\(x_i\\) is perturbed a little keeping all other variables constant. This perturbation propagates to \\(L\\) via \\(e\\) . There is a chain reaction: \\(x_1\\) affects some of the variables in the vector \\(e\\) , which in turn affects \\(L\\) . To get a better feel for this, let us take an example: \\[ \\begin{aligned} e &= X\\theta - b\\\\\\\\ \\begin{bmatrix} e_1\\\\ e_2\\\\ e_3 \\end{bmatrix} &= \\begin{bmatrix} x_{11} & x_{12}\\\\ x_{21} & x_{22}\\\\ x_{31} & x_{32}\\\\ \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\theta_2 \\end{bmatrix} - \\begin{bmatrix} y_1\\\\ y_2 \\end{bmatrix}\\\\\\\\ L &= e^T e = e_1^2 + e_2^2 + e_3^2 \\end{aligned} \\] The variable \\(\\theta_1\\) influences \\(L\\) via three independent routes: \\(\\theta_1 \\rightarrow e_1 \\rightarrow L\\) \\(\\theta_1 \\rightarrow e_2 \\rightarrow L\\) \\(\\theta_1 \\rightarrow e_3 \\rightarrow L\\) The partial derivative needs to take into account the contributions from each of these three routes. Pictorially: Symbolically: \\[ \\cfrac{\\partial L}{\\partial \\theta_1} = \\cfrac{\\partial L}{\\partial e_1} \\cfrac{\\partial e_1}{\\partial \\theta_1} + \\cfrac{\\partial L}{\\partial e_2} \\cfrac{\\partial e_2}{\\partial \\theta_1} + \\cfrac{\\partial L}{\\partial e_3} \\cfrac{\\partial e_3}{\\partial \\theta_1} \\] Observe that the RHS is in the form of a dot product between two vectors: \\[ \\cfrac{\\partial L}{\\partial \\theta_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_1} & \\cfrac{\\partial e_2}{\\partial \\theta_1} & \\cfrac{\\partial e_3}{\\partial \\theta_1} \\end{bmatrix} \\begin{bmatrix} \\cfrac{\\partial L}{\\partial e_1}\\\\ \\cfrac{\\partial L}{\\partial e_2}\\\\ \\cfrac{\\partial L}{\\partial e_3} \\end{bmatrix} \\] Notice, that the second vector is nothing but \\(\\nabla_e L\\) : $$ \\cfrac{\\partial L}{\\partial \\theta_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_1} & \\cfrac{\\partial e_2}{\\partial \\theta_1} & \\cfrac{\\partial e_3}{\\partial \\theta_1} \\end{bmatrix} \\nabla_e L $$ We can now add \\(\\cfrac{\\partial L}{\\partial \\theta_2}\\) to the mix: \\[ \\begin{aligned} \\nabla_{\\theta} L &= \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\\\ \\cfrac{\\partial L}{\\partial \\theta_2} \\end{bmatrix}\\\\ \\\\ &= \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_1} & \\cfrac{\\partial e_2}{\\partial \\theta_1} & \\cfrac{\\partial e_3}{\\partial \\theta_1}\\\\ \\cfrac{\\partial e_1}{\\partial \\theta_2} & \\cfrac{\\partial e_2}{\\partial \\theta_2} & \\cfrac{\\partial e_3}{\\partial \\theta_2} \\end{bmatrix} \\nabla_e L\\\\\\\\ &= J\\ \\nabla_e L \\end{aligned} \\] The matrix \\(J\\) is called the Jacobian matrix. It represents the partial derivatives of each component of the vector \\(e\\) with respect to each component of the vector, \\(\\theta\\) . That is, every element of the Jacobian is of the form: \\[ J_{ij} = \\cfrac{\\partial e_j}{\\partial \\theta_i} \\]","title":"Chain rule for multiple variables"},{"location":"appendix/gradients/#back-to-the-gradients","text":"Let us resume our computation, but this time with a general \\(m \\times n\\) matrix \\(X\\) , \\(m\\) dimensional vector \\(y\\) and \\(n\\) dimensional vector \\(\\theta\\) . Since \\(L = e^Te\\) , we have: \\[ \\nabla_e L = 2 e \\] We now need to compute the Jacobian. We have \\(e = X\\theta - y\\) . Since \\(y\\) is a constant, it will vanish while computing the derivatives. We only need to worry about \\(X\\theta\\) for which we can use the following relationship. If \\(x_i\\) is the \\(i^{th}\\) column of \\(A\\) , then: \\[ X\\theta = \\theta_1x_1 + \\cdots + \\theta_nx_n \\] The \\(i^{th}\\) row of the Jacobian is: \\[ \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial \\theta_i} & \\cdots & \\cfrac{\\partial e_m}{\\partial \\theta_i} \\end{bmatrix} = x_i^T \\] As simple as that: the \\(i^{th}\\) row of the Jacobian is just the \\(i^{th}\\) column of \\(X\\) . So, the Jacobian is nothing but \\(X^T\\) . We now have the complete expression for \\(\\nabla_{\\theta} L\\) : \\[ \\nabla_{\\theta} L = 2 X^T (X\\theta - y) = 2(X^TX \\theta - X^T y) \\]","title":"Back to the gradients"},{"location":"appendix/matrix_product/","text":"Matrix product \u00b6 There are several ways to understand the product of two matrices \\(A\\) and \\(B\\) of compatible dimensions. We will look at four different ways to do the same. Specifically, let \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) be a \\(n \\times p\\) matrix. Let \\(C = AB\\) . Row-column picture \u00b6 \\(A\\) is expressed in terms of \\(m\\) row vectors. \\(B\\) is expressed in terms of \\(p\\) column vectors. \\[ \\begin{bmatrix} \\cdots & u_1^T & \\cdots\\\\ & \\vdots & \\\\ \\cdots & u_m^T & \\cdots \\end{bmatrix} \\begin{bmatrix} \\vdots & & \\vdots\\\\ v_1 & \\cdots & v_p\\\\ \\vdots & & \\vdots \\end{bmatrix} \\] The \\(ij^{th}\\) element of the matrix \\(C\\) is: \\[ C_{ij} = u_i^Tv_j \\] This is by far the most popular form of matrix multiplication and is the first thing that we learn in high school. Each element of the matrix is represented as the inner product of two vectors. Matrix-column picture \u00b6 \\(B\\) is expressed as a sequence of \\(p\\) column vectors. \\[ A \\begin{bmatrix} \\vdots & & \\vdots\\\\ v_1 & \\cdots & v_p\\\\ \\vdots & & \\vdots \\end{bmatrix} \\] Each column of \\(C\\) can be represented as \\(Av_i\\) , a linear combination of the columns of \\(A\\) , where the coefficients of the combination are given by the columns of \\(B\\) : \\[ C = \\begin{bmatrix} \\vdots & & \\vdots\\\\ Av_1 & \\cdots & Av_p\\\\ \\vdots & & \\vdots \\end{bmatrix} \\] Row-matrix picture \u00b6 \\(A\\) is expressed as a sequence of \\(m\\) row vectors: \\[ \\begin{bmatrix} \\cdots & u_1^T & \\cdots\\\\ & \\vdots & \\\\ \\cdots & u_m^T & \\cdots \\end{bmatrix} B \\] Each row of the matrix \\(C\\) is of the form \\(u_i^TB\\) , a linear combination of the rows of \\(B\\) , where the coefficients of the combination are given by the rows of \\(A\\) : \\[ C = \\begin{bmatrix} \\cdots & u_1^TB & \\cdots\\\\ & \\vdots & \\\\ \\cdots & u_m^TB & \\cdots\\\\ \\end{bmatrix} \\] Column-row picture \u00b6 \\(A\\) is expressed in terms of \\(n\\) column vectors. \\(B\\) is expressed in terms of \\(n\\) row vectors. \\[ \\begin{bmatrix} \\vdots & & \\vdots\\\\ u_1 & \\cdots & u_n\\\\ \\vdots & & \\vdots\\\\ \\end{bmatrix} \\begin{bmatrix} \\cdots & v_1^T & \\cdots\\\\ & \\vdots & \\\\ \\cdots & v_n^T & \\cdots \\end{bmatrix} \\] Here is another way to multiply two matrices: \\[ C = \\sum \\limits_{i = 1}^{n} u_i v_i^T \\] Matrix \\(C\\) is expressed as the sum of \\(n\\) matrices of unit rank . Alternatively, the matrix is represented as the sum of \\(n\\) outer products. To see why this is true, consider an unit vector \\(e_k\\) in the standard basis for \\(\\mathbb{R}^{p}\\) . Now, let us look the product of the RHS with \\(e_k\\) . This is nothing but the \\(k^{th}\\) column of this matrix: \\[ \\sum \\limits_{i = 1}^{n} u_i v_i^T e_k = \\sum \\limits_{i = 1}^{n} (v_i^Te_k) u_i \\] The RHS of the above expression is a linear combinations of the columns of the matrix \\(A\\) . The coefficients of the combination are the columns of the matrix \\(B\\) . This is the same as the matrix-column picture.","title":"Matrix product"},{"location":"appendix/matrix_product/#matrix-product","text":"There are several ways to understand the product of two matrices \\(A\\) and \\(B\\) of compatible dimensions. We will look at four different ways to do the same. Specifically, let \\(A\\) be an \\(m \\times n\\) matrix and \\(B\\) be a \\(n \\times p\\) matrix. Let \\(C = AB\\) .","title":"Matrix product"},{"location":"appendix/matrix_product/#row-column-picture","text":"\\(A\\) is expressed in terms of \\(m\\) row vectors. \\(B\\) is expressed in terms of \\(p\\) column vectors. \\[ \\begin{bmatrix} \\cdots & u_1^T & \\cdots\\\\ & \\vdots & \\\\ \\cdots & u_m^T & \\cdots \\end{bmatrix} \\begin{bmatrix} \\vdots & & \\vdots\\\\ v_1 & \\cdots & v_p\\\\ \\vdots & & \\vdots \\end{bmatrix} \\] The \\(ij^{th}\\) element of the matrix \\(C\\) is: \\[ C_{ij} = u_i^Tv_j \\] This is by far the most popular form of matrix multiplication and is the first thing that we learn in high school. Each element of the matrix is represented as the inner product of two vectors.","title":"Row-column picture"},{"location":"appendix/matrix_product/#matrix-column-picture","text":"\\(B\\) is expressed as a sequence of \\(p\\) column vectors. \\[ A \\begin{bmatrix} \\vdots & & \\vdots\\\\ v_1 & \\cdots & v_p\\\\ \\vdots & & \\vdots \\end{bmatrix} \\] Each column of \\(C\\) can be represented as \\(Av_i\\) , a linear combination of the columns of \\(A\\) , where the coefficients of the combination are given by the columns of \\(B\\) : \\[ C = \\begin{bmatrix} \\vdots & & \\vdots\\\\ Av_1 & \\cdots & Av_p\\\\ \\vdots & & \\vdots \\end{bmatrix} \\]","title":"Matrix-column picture"},{"location":"appendix/matrix_product/#row-matrix-picture","text":"\\(A\\) is expressed as a sequence of \\(m\\) row vectors: \\[ \\begin{bmatrix} \\cdots & u_1^T & \\cdots\\\\ & \\vdots & \\\\ \\cdots & u_m^T & \\cdots \\end{bmatrix} B \\] Each row of the matrix \\(C\\) is of the form \\(u_i^TB\\) , a linear combination of the rows of \\(B\\) , where the coefficients of the combination are given by the rows of \\(A\\) : \\[ C = \\begin{bmatrix} \\cdots & u_1^TB & \\cdots\\\\ & \\vdots & \\\\ \\cdots & u_m^TB & \\cdots\\\\ \\end{bmatrix} \\]","title":"Row-matrix picture"},{"location":"appendix/matrix_product/#column-row-picture","text":"\\(A\\) is expressed in terms of \\(n\\) column vectors. \\(B\\) is expressed in terms of \\(n\\) row vectors. \\[ \\begin{bmatrix} \\vdots & & \\vdots\\\\ u_1 & \\cdots & u_n\\\\ \\vdots & & \\vdots\\\\ \\end{bmatrix} \\begin{bmatrix} \\cdots & v_1^T & \\cdots\\\\ & \\vdots & \\\\ \\cdots & v_n^T & \\cdots \\end{bmatrix} \\] Here is another way to multiply two matrices: \\[ C = \\sum \\limits_{i = 1}^{n} u_i v_i^T \\] Matrix \\(C\\) is expressed as the sum of \\(n\\) matrices of unit rank . Alternatively, the matrix is represented as the sum of \\(n\\) outer products. To see why this is true, consider an unit vector \\(e_k\\) in the standard basis for \\(\\mathbb{R}^{p}\\) . Now, let us look the product of the RHS with \\(e_k\\) . This is nothing but the \\(k^{th}\\) column of this matrix: \\[ \\sum \\limits_{i = 1}^{n} u_i v_i^T e_k = \\sum \\limits_{i = 1}^{n} (v_i^Te_k) u_i \\] The RHS of the above expression is a linear combinations of the columns of the matrix \\(A\\) . The coefficients of the combination are the columns of the matrix \\(B\\) . This is the same as the matrix-column picture.","title":"Column-row picture"},{"location":"appendix/rank_one/","text":"Rank-1 matrices \u00b6 Outer product \u00b6 Consider the following matrix product: \\[ \\begin{bmatrix} a\\\\ b\\\\ c\\\\ \\end{bmatrix} \\begin{bmatrix} x & y & z \\end{bmatrix} = \\begin{bmatrix} ax & ay & az\\\\ bx & by & bz\\\\ cx & cy & cz \\end{bmatrix} \\] If we notice the matrix on the right, each row is a multiple of the row vector \\(\\begin{bmatrix}x\\\\ y \\\\z\\end{bmatrix}^T\\) and each column is a multiple of the column vector \\(\\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix}\\) . From this we can deduce that this matrix has rank \\(1\\) . The product of the two vectors on the left is called the outer product . We can go the other way and claim that every matrix of unit rank can be expressed as the outer product of two vectors: \\[ \\huge \\boxed{uv^T} \\] To see why this is true, start with any \\(m \\times n\\) matrix \\(A\\) of unit rank. If the rank is \\(1\\) , then every vector in the column space of \\(A\\) is of the form \\(ku\\) , where \\(u = \\begin{bmatrix}u_1 & \\cdots & u_m\\end{bmatrix}^T\\) . Then, for each vector in the standard basis, we have scalars \\(v_1, \\cdots, v_n\\) such that \\(Ae_i = v_i u\\) . Note that \\(Ae_i\\) is nothing but the \\(i^{th}\\) column of the matrix \\(A\\) . Therefore, we have: \\[ A = \\begin{bmatrix} u_1\\\\ \\vdots\\\\ u_m \\end{bmatrix} \\begin{bmatrix} v_1 & \\cdots v_n \\end{bmatrix} = uv^T \\] Note Every \\(m \\times n\\) matrix of unit rank is of the form \\(uv^T\\) where \\(u \\in \\mathbb{R}^m\\) and \\(v \\in \\mathbb{R}^n\\) .","title":"Rank-1 matrices"},{"location":"appendix/rank_one/#rank-1-matrices","text":"","title":"Rank-1 matrices"},{"location":"appendix/rank_one/#outer-product","text":"Consider the following matrix product: \\[ \\begin{bmatrix} a\\\\ b\\\\ c\\\\ \\end{bmatrix} \\begin{bmatrix} x & y & z \\end{bmatrix} = \\begin{bmatrix} ax & ay & az\\\\ bx & by & bz\\\\ cx & cy & cz \\end{bmatrix} \\] If we notice the matrix on the right, each row is a multiple of the row vector \\(\\begin{bmatrix}x\\\\ y \\\\z\\end{bmatrix}^T\\) and each column is a multiple of the column vector \\(\\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix}\\) . From this we can deduce that this matrix has rank \\(1\\) . The product of the two vectors on the left is called the outer product . We can go the other way and claim that every matrix of unit rank can be expressed as the outer product of two vectors: \\[ \\huge \\boxed{uv^T} \\] To see why this is true, start with any \\(m \\times n\\) matrix \\(A\\) of unit rank. If the rank is \\(1\\) , then every vector in the column space of \\(A\\) is of the form \\(ku\\) , where \\(u = \\begin{bmatrix}u_1 & \\cdots & u_m\\end{bmatrix}^T\\) . Then, for each vector in the standard basis, we have scalars \\(v_1, \\cdots, v_n\\) such that \\(Ae_i = v_i u\\) . Note that \\(Ae_i\\) is nothing but the \\(i^{th}\\) column of the matrix \\(A\\) . Therefore, we have: \\[ A = \\begin{bmatrix} u_1\\\\ \\vdots\\\\ u_m \\end{bmatrix} \\begin{bmatrix} v_1 & \\cdots v_n \\end{bmatrix} = uv^T \\] Note Every \\(m \\times n\\) matrix of unit rank is of the form \\(uv^T\\) where \\(u \\in \\mathbb{R}^m\\) and \\(v \\in \\mathbb{R}^n\\) .","title":"Outer product"},{"location":"slides/links/","text":"Links \u00b6 Week-3","title":"Links"},{"location":"slides/links/#links","text":"Week-3","title":"Links"},{"location":"slides/session/","text":"Course Outline \u00b6 Week 1: Introduction to ML Week 2: Recap of calculus for ML Weeks 3 - 6: Linear algebra for ML Weeks 7 - 9: Optimization for ML Weeks 10 - 12: Probability and Statistics for ML Session Outline \u00b6 Linear Algebra for ML Regression problem Dataset Linear regression model Algebra \\(X \\theta = 0\\) \\(X \\theta = y\\) \\(X \\theta \\approx y\\) Geometry Best-fit Projections Linear Algebra for ML \u00b6 Linear Algebra for ML \u00b6 Why should we study linear algebra in ML? Linear Algebra for ML \u00b6 Why should we study linear algebra in ML? ::: {.columns align=center} ::: {.column width=\"100%\"} Data \u00b6 ::: ::: Linear Algebra for ML \u00b6 Housing dataset lattitude longitue age num_of_rooms area distance_from_school Linear Algebra for ML \u00b6 \\(1\\) house ::: {.columns align=center} ::: {.column width=\"50%\"} Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 rooms 2 area 1000 distance 3 price 40 ::: ::: {.column width=\"50%\"} ::: ::: Linear Algebra for ML \u00b6 \\(1\\) house ::: {.columns align=center} ::: {.column width=\"50%\"} Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 rooms 2 area 1000 distance 3 price 40 ::: ::: {.column width=\"50%\"} $$ \\begin{bmatrix} 12.9\\ 80.2\\ 3\\ 2\\ 1000\\ 3\\ \\end{bmatrix} $$ Vector ::: ::: Linear Algebra for ML \u00b6 \\(100\\) houses? Linear Algebra for ML \u00b6 \\(100\\) houses ::: {.columns align=center} ::: {.column width=\"100%\"} \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] \\(100 \\times 6\\) matrix Each row is a house ::: ::: {.column width=\"0%\"} ::: ::: Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} Given lattitude longitue age num_of_rooms area distance_from_school ::: ::: {.column width=\"50%\"} Predict selling_price ::: ::: Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} Given $$ \\huge{x \\in \\mathbb{R}^{6}} $$ ::: ::: {.column width=\"50%\"} Predict $$ \\huge{y \\in \\mathbb{R}} $$ ::: ::: Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} Given $$ \\huge{x \\in \\mathbb{R}^{n}} $$ ::: ::: {.column width=\"50%\"} Predict $$ \\huge{y \\in \\mathbb{R}} $$ ::: ::: Regression \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} Given $$ \\huge{x \\in \\mathbb{R}^{n}} $$ Feature-vector ::: ::: {.column width=\"50%\"} Predict $$ \\huge{y \\in \\mathbb{R}} $$ Label ::: ::: Model \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f: \\mathbb{R}^{n}} \\rightarrow \\mathbb{R} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Model \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f: \\mathbb{R}^{n}} \\rightarrow \\mathbb{R} $$ $$ \\huge{f(x)=y} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Model \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f: \\mathbb{R}^{n}} \\rightarrow \\mathbb{R} $$ $$ \\huge{f(x)=y} $$ Learning a model? ::: ::: {.column width=\"0%\"} ::: ::: Labeled Dataset \u00b6 Data for \\(m\\) houses; each house is described by \\(n\\) features: ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\cdots & x_1 & \\cdots\\ & \\vdots &\\ \\cdots & x_m & \\cdots\\ \\end{bmatrix} $$ \\(m \\times n\\) data-matrix (feature matrix) ::: ::: {.column width=\"50%\"} $$ y = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ \\(m \\times 1\\) label vector ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\large{\\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant}} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} y &= \\theta_0 + x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 \\\\ \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} y &= \\theta_0 + x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 \\\\ &= \\theta^T x \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ y = \\theta^T x = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & \\theta_3 & \\theta_4 & \\theta_5 & \\theta_6 \\end{bmatrix}\\begin{bmatrix} 1\\ x_1\\ x_2\\ x_3\\ x_4\\ x_5\\ x_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f(x) = \\theta^T x} $$ \\(\\large \\theta\\) is a vector of parameters (weights) of the model ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ y = \\theta^T x = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & \\theta_3 & \\theta_4 & \\theta_5 & \\theta_6 \\end{bmatrix}\\begin{bmatrix} 1\\ x_1\\ x_2\\ x_3\\ x_4\\ x_5\\ x_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ y = x^T \\theta = \\begin{bmatrix} 1 & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\ \\theta_1 \\ \\theta_2 \\ \\theta_3 \\ \\theta_4 \\ \\theta_5 \\ \\theta_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} y_{1}\\ \\vdots\\ y_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} & x_{1,5} & x_{1,6}\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\ 1 & x_{100,1} & x_{100,2} & x_{100,3} & x_{100,4} & x_{100,5} & x_{100,6}\\ \\end{bmatrix}\\begin{bmatrix} \\theta_0\\ \\theta_1\\ \\theta_2\\ \\theta_3\\ \\theta_4\\ \\theta_5\\ \\theta_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta} = y $$ ::: ::: {.column width=\"0%\"} right ::: ::: Linear regression \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta} = y $$ Enter Linear Algebra ::: ::: {.column width=\"0%\"} right ::: ::: Setting \u00b6 ::: incremental \\(X\\) is a data-matrix of dimensions \\(m \\times n\\) \\(y\\) is a column-vector of size \\(m\\) \\(\\theta \\in \\mathbb{R}^n\\) \\(X\\theta \\in \\mathbb{R}^{m}\\) ::: ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta} = y $$ ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta = 0} $$ ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} If: \\(X \\theta_1 = 0\\) \\(X \\theta_2 = 0\\) ::: ::: {.column width=\"50%\"} ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} If: \\(X \\theta_1 = 0\\) \\(X \\theta_2 = 0\\) ::: ::: {.column width=\"50%\"} Then: \\(X (\\theta_1 + \\theta_2) = X\\theta_1 + X \\theta_2 = 0\\) \\(X(k \\theta_1) = k(X \\theta_1) = 0\\) ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(\\underline{} \\underline{} \\underline{} \\underline{}\\) . ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(N(X)\\) . ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(N(X)\\) . Find a basis for \\(N(X)\\) . ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(N(X)\\) . Find a basis for \\(N(X)\\) . \\(N(X) = \\text{span}(B)\\) ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = 0\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta = 0} $$ Gaussian elimination ::: ::: {.column width=\"0%\"} right ::: ::: Row-operations \u00b6 ::: incremental swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row ::: Example \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ X = \\begin{bmatrix} 1 & 0 & -1 & 0\\ 2 & 1 & 0 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Example: Step-1 \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 2 & 1 & 0 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Example: Step-2 \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 1 & 2 & 1 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Example: Step-3 \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Row-echelon matrix \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Row-echelon matrix \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} \\color{blue}{1} & 0 & -1 & 0\\ 0 & \\color{blue}{1} & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Solve \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} \\color{blue}{1} & 0 & -1 & 0\\ 0 & \\color{blue}{1} & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\color{blue}{\\theta_1}\\ \\color{blue}{\\theta_2}\\ \\theta_3\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} 0\\ 0\\ 0\\ 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Algorithm \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} \\(B = \\{ \\}\\) ::: incremental For each independent variable \\(\\theta_i\\) : Set \\(\\theta_i = 1\\) and \\(\\theta_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(\\theta\\) to \\(B\\) ::: ::: ::: {.column width=\"0%\"} right ::: ::: Solved \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\color{blue}{1} & 0 & -1 & 0\\ 0 & \\color{blue}{1} & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\color{blue}{\\theta_1}\\ \\color{blue}{\\theta_2}\\ \\theta_3\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} 0\\ 0\\ 0\\ 0 \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} $$ B = \\left { \\begin{bmatrix}1\\ -2\\ 1\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\ -1\\ 0\\ 1\\end{bmatrix} \\right } $$ ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} When will \\(X \\theta = y\\) have a solution? ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\ x_1 & \\cdots & x_n\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\ \\vdots\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\ x_1 & \\cdots & x_n\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\ \\vdots\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} $$ \\theta_1 x_1 + \\cdots + \\theta_n x_n = y $$ ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\ x_1 & \\cdots & x_n\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\ \\vdots\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} $$ \\theta_1 x_1 + \\cdots + \\theta_n x_n = y $$ $$ y \\in C(X) $$ ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} When will \\(X \\theta = y\\) have a solution? \\(y \\in C(X)\\) ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: incremental \\(X\\) is the data-matrix \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features ::: ::: ::: {.column width=\"50%\"} ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} \\(X\\) is the data-matrix \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features ::: ::: {.column width=\"50%\"} Typical dataset: ::: incremental \\(m = 10000\\) \\(n = 10\\) Can \\(10\\) vectors span \\(\\mathbb{R}^{10000}\\) ? ::: ::: ::: \\(X \\theta = y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} \\(X\\) is the data-matrix \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features ::: ::: {.column width=\"50%\"} Typical dataset: \\(m = 10000\\) \\(n = 10\\) Can \\(10\\) vectors span \\(\\mathbb{R}^{10000}\\) ? ::: ::: ::: {.columns align=center} ::: {.column width=\"100%\"} \\(X \\theta = y\\) is generally unsolvable. ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"100%\"} What does the \\(\\approx\\) symbol mean? ::: incremental \\(1.234 \\approx 1\\) \\(1.234 \\approx 1.2\\) \\(1.234 \\approx 1.23\\) ::: ::: ::: {.column width=\"0%\"} right ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ ||\\hat{y} - y|| $$ ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ ||\\hat{y} - y||^2 $$ ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ ||\\hat{y} - y||^2 = (\\hat{y}_1 - y_1)^2 + \\cdots + (\\hat{y}_m - y_m)^2 $$ ::: ::: \\(X \\theta \\approx y\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ X\\hat{\\theta} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ \\hat{\\theta} = \\arg \\min \\limits_{\\theta} ||X\\theta - y||^2 $$ ::: ::: Loss \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{L = ||X \\theta - y||^2} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Loss \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L &= ||X \\theta - y||^2 \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Loss \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L &= ||X \\theta - y||^2\\\\ &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Optimization \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\begin{aligned} L &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"50%\"} ::: ::: Optimization \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\begin{aligned} L &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_{\\theta} L = \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\ \\vdots\\ \\cfrac{\\partial L}{\\partial \\theta_n} \\end{bmatrix} = 0 $$ ::: ::: Optimization \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\begin{aligned} L &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_{\\theta} L = 2(X^TX)\\theta - 2X^Ty = 0 $$ ::: ::: Normal Equations \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{(X^TX)\\theta = X^Ty} $$ ::: ::: {.column width=\"0%\"} right ::: ::: Solution \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} If \\(X^TX\\) is invertible, then: $$ \\hat{\\theta} = (X^TX)^{-1} X^Ty $$ When is \\(X^TX\\) invertible? ::: ::: {.column width=\"0%\"} right ::: ::: \\(X^TX\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: incremental \\(n \\times n\\) matrix Symmetric ::: ::: ::: {.column width=\"50%\"} ::: ::: \\(X^TX\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ N(X) = N(X^TX) $$ ::: ::: {.column width=\"50%\"} ::: ::: \\(X^TX\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ N(X) = N(X^TX) $$ ::: ::: {.column width=\"50%\"} Proof If \\(\\theta \\in N(X)\\) : ::: incremental \\(X\\theta =0\\) \\(X^TX \\theta = 0\\) ::: ::: ::: \\(X^TX\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} $$ N(X) = N(X^TX) $$ ::: ::: {.column width=\"50%\"} Proof If \\(\\theta \\in N(X)\\) : \\(X\\theta =0\\) \\(X^TX \\theta = 0\\) If \\(\\theta \\in N(X^T X)\\) ::: incremental \\(X^TX \\theta = 0\\) \\(\\theta^T X^TX \\theta = 0\\) \\((X \\theta)^T (X \\theta) = 0\\) \\(X \\theta = 0\\) ::: ::: ::: \\(X^TX\\) \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} If \\(\\text{rank}(X) = n\\) , \\(X^TX\\) is invertible ::: ::: {.column width=\"50%\"} Proof If \\(\\text{rank}(X) = n\\) : ::: incremental \\(\\text{nullity}(X) = 0\\) \\(\\text{nullity}(X^TX) = 0\\) \\(\\text{rank}(X^TX) = n\\) \\(X^TX\\) is full rank, hence invertible ::: ::: ::: Geometry \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} What does a linear regression model look like? ::: ::: {.column width=\"0%\"} right ::: ::: Data \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: ::: Linear model \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: ::: Errors \u00b6 ::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ (X^TX)\\hat{\\theta} = X^Ty $$ \\(X \\hat{\\theta}\\) is an approximation for \\(y\\) How are \\(X \\hat{\\theta}\\) and \\(y\\) related geometrically? ::: ::: {.column width=\"50%\"} ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ (X^TX)\\hat{\\theta} = X^Ty $$ \\(X \\hat{\\theta}\\) is an approximation for \\(y\\) How are \\(X \\hat{\\theta}\\) and \\(y\\) related geometrically? ::: ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} 2 & 6\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\ 4 \\end{bmatrix}, X \\hat{\\theta} = \\begin{bmatrix} 4\\ 2 \\end{bmatrix} $$ ::: ::: Projections \u00b6 ::: {.columns align=left} ::: {.column width=\"70%\"} ::: ::: {.column width=\"30%\"} $$ X = \\begin{bmatrix} 2 & 6\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\ 4 \\end{bmatrix}, X \\hat{\\theta} = \\begin{bmatrix} 4\\ 2 \\end{bmatrix} $$ ::: ::: Projections \u00b6 ::: {.columns align=left} ::: {.column width=\"70%\"} ::: ::: {.column width=\"30%\"} $$ X = \\begin{bmatrix} 2 & 6\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\ 4 \\end{bmatrix}, X \\hat{\\theta} = \\begin{bmatrix} 4\\ 2 \\end{bmatrix} $$ ::: ::: Projections \u00b6 ::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ e = y - X \\hat{\\theta} $$ $$ e \\perp C(X) $$ ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ $$ X^T e = 0 $$ ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ $$ X^T e = 0 $$ $$ X^T(y - X\\hat{\\theta}) = 0 $$ ::: ::: Projections \u00b6 ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ $$ X^T e = 0 $$ $$ X^T(y - X\\hat{\\theta}) = 0 $$ $$ X^TX \\hat{\\theta} = X^T y $$ ::: ::: Summary \u00b6 Linear Algebra for ML Regression problem Dataset Linear regression model Algebra \\(X \\theta = 0\\) \\(X \\theta = y\\) \\(X \\theta \\approx y\\) Geometry Best-fit Projections","title":"MLF | Live Session | Week-3"},{"location":"slides/session/#course-outline","text":"Week 1: Introduction to ML Week 2: Recap of calculus for ML Weeks 3 - 6: Linear algebra for ML Weeks 7 - 9: Optimization for ML Weeks 10 - 12: Probability and Statistics for ML","title":"Course Outline"},{"location":"slides/session/#session-outline","text":"Linear Algebra for ML Regression problem Dataset Linear regression model Algebra \\(X \\theta = 0\\) \\(X \\theta = y\\) \\(X \\theta \\approx y\\) Geometry Best-fit Projections","title":"Session Outline"},{"location":"slides/session/#linear-algebra-for-ml","text":"","title":"Linear Algebra for ML"},{"location":"slides/session/#linear-algebra-for-ml_1","text":"Why should we study linear algebra in ML?","title":"Linear Algebra for ML"},{"location":"slides/session/#linear-algebra-for-ml_2","text":"Why should we study linear algebra in ML? ::: {.columns align=center} ::: {.column width=\"100%\"}","title":"Linear Algebra for ML"},{"location":"slides/session/#data","text":"::: :::","title":"Data"},{"location":"slides/session/#linear-algebra-for-ml_3","text":"Housing dataset lattitude longitue age num_of_rooms area distance_from_school","title":"Linear Algebra for ML"},{"location":"slides/session/#linear-algebra-for-ml_4","text":"\\(1\\) house ::: {.columns align=center} ::: {.column width=\"50%\"} Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 rooms 2 area 1000 distance 3 price 40 ::: ::: {.column width=\"50%\"} ::: :::","title":"Linear Algebra for ML"},{"location":"slides/session/#linear-algebra-for-ml_5","text":"\\(1\\) house ::: {.columns align=center} ::: {.column width=\"50%\"} Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 rooms 2 area 1000 distance 3 price 40 ::: ::: {.column width=\"50%\"} $$ \\begin{bmatrix} 12.9\\ 80.2\\ 3\\ 2\\ 1000\\ 3\\ \\end{bmatrix} $$ Vector ::: :::","title":"Linear Algebra for ML"},{"location":"slides/session/#linear-algebra-for-ml_6","text":"\\(100\\) houses?","title":"Linear Algebra for ML"},{"location":"slides/session/#linear-algebra-for-ml_7","text":"\\(100\\) houses ::: {.columns align=center} ::: {.column width=\"100%\"} \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] \\(100 \\times 6\\) matrix Each row is a house ::: ::: {.column width=\"0%\"} ::: :::","title":"Linear Algebra for ML"},{"location":"slides/session/#regression","text":"::: {.columns align=center} ::: {.column width=\"50%\"} Given lattitude longitue age num_of_rooms area distance_from_school ::: ::: {.column width=\"50%\"} Predict selling_price ::: :::","title":"Regression"},{"location":"slides/session/#regression_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} Given $$ \\huge{x \\in \\mathbb{R}^{6}} $$ ::: ::: {.column width=\"50%\"} Predict $$ \\huge{y \\in \\mathbb{R}} $$ ::: :::","title":"Regression"},{"location":"slides/session/#regression_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} Given $$ \\huge{x \\in \\mathbb{R}^{n}} $$ ::: ::: {.column width=\"50%\"} Predict $$ \\huge{y \\in \\mathbb{R}} $$ ::: :::","title":"Regression"},{"location":"slides/session/#regression_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} Given $$ \\huge{x \\in \\mathbb{R}^{n}} $$ Feature-vector ::: ::: {.column width=\"50%\"} Predict $$ \\huge{y \\in \\mathbb{R}} $$ Label ::: :::","title":"Regression"},{"location":"slides/session/#model","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f: \\mathbb{R}^{n}} \\rightarrow \\mathbb{R} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Model"},{"location":"slides/session/#model_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f: \\mathbb{R}^{n}} \\rightarrow \\mathbb{R} $$ $$ \\huge{f(x)=y} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Model"},{"location":"slides/session/#model_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f: \\mathbb{R}^{n}} \\rightarrow \\mathbb{R} $$ $$ \\huge{f(x)=y} $$ Learning a model? ::: ::: {.column width=\"0%\"} ::: :::","title":"Model"},{"location":"slides/session/#labeled-dataset","text":"Data for \\(m\\) houses; each house is described by \\(n\\) features: ::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\cdots & x_1 & \\cdots\\ & \\vdots &\\ \\cdots & x_m & \\cdots\\ \\end{bmatrix} $$ \\(m \\times n\\) data-matrix (feature matrix) ::: ::: {.column width=\"50%\"} $$ y = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ \\(m \\times 1\\) label vector ::: :::","title":"Labeled Dataset"},{"location":"slides/session/#linear-regression","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\large{\\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant}} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} y &= \\theta_0 + x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 \\\\ \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} y &= \\theta_0 + x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 \\\\ &= \\theta^T x \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ y = \\theta^T x = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & \\theta_3 & \\theta_4 & \\theta_5 & \\theta_6 \\end{bmatrix}\\begin{bmatrix} 1\\ x_1\\ x_2\\ x_3\\ x_4\\ x_5\\ x_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_4","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{f(x) = \\theta^T x} $$ \\(\\large \\theta\\) is a vector of parameters (weights) of the model ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_5","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ y = \\theta^T x = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & \\theta_3 & \\theta_4 & \\theta_5 & \\theta_6 \\end{bmatrix}\\begin{bmatrix} 1\\ x_1\\ x_2\\ x_3\\ x_4\\ x_5\\ x_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_6","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ y = x^T \\theta = \\begin{bmatrix} 1 & x_1 & x_2 & x_3 & x_4 & x_5 & x_6 \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\ \\theta_1 \\ \\theta_2 \\ \\theta_3 \\ \\theta_4 \\ \\theta_5 \\ \\theta_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_7","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} y_{1}\\ \\vdots\\ y_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} & x_{1,5} & x_{1,6}\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\ 1 & x_{100,1} & x_{100,2} & x_{100,3} & x_{100,4} & x_{100,5} & x_{100,6}\\ \\end{bmatrix}\\begin{bmatrix} \\theta_0\\ \\theta_1\\ \\theta_2\\ \\theta_3\\ \\theta_4\\ \\theta_5\\ \\theta_6 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_8","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta} = y $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#linear-regression_9","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta} = y $$ Enter Linear Algebra ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear regression"},{"location":"slides/session/#setting","text":"::: incremental \\(X\\) is a data-matrix of dimensions \\(m \\times n\\) \\(y\\) is a column-vector of size \\(m\\) \\(\\theta \\in \\mathbb{R}^n\\) \\(X\\theta \\in \\mathbb{R}^{m}\\) ::: ::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta} = y $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Setting"},{"location":"slides/session/#x-theta-0","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta = 0} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_1","text":"::: {.columns align=left} ::: {.column width=\"50%\"} If: \\(X \\theta_1 = 0\\) \\(X \\theta_2 = 0\\) ::: ::: {.column width=\"50%\"} ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_2","text":"::: {.columns align=left} ::: {.column width=\"50%\"} If: \\(X \\theta_1 = 0\\) \\(X \\theta_2 = 0\\) ::: ::: {.column width=\"50%\"} Then: \\(X (\\theta_1 + \\theta_2) = X\\theta_1 + X \\theta_2 = 0\\) \\(X(k \\theta_1) = k(X \\theta_1) = 0\\) ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(\\underline{} \\underline{} \\underline{} \\underline{}\\) . ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_4","text":"::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(N(X)\\) . ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_5","text":"::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(N(X)\\) . Find a basis for \\(N(X)\\) . ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_6","text":"::: {.columns align=center} ::: {.column width=\"100%\"} The set of all solutions of \\(X \\theta = 0\\) is \\(N(X)\\) . Find a basis for \\(N(X)\\) . \\(N(X) = \\text{span}(B)\\) ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#x-theta-0_7","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{X \\theta = 0} $$ Gaussian elimination ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = 0\\)"},{"location":"slides/session/#row-operations","text":"::: incremental swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row :::","title":"Row-operations"},{"location":"slides/session/#example","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ X = \\begin{bmatrix} 1 & 0 & -1 & 0\\ 2 & 1 & 0 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Example"},{"location":"slides/session/#example-step-1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 2 & 1 & 0 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Example: Step-1"},{"location":"slides/session/#example-step-2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 1 & 2 & 1 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Example: Step-2"},{"location":"slides/session/#example-step-3","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Example: Step-3"},{"location":"slides/session/#row-echelon-matrix","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} 1 & 0 & -1 & 0\\ 0 & 1 & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Row-echelon matrix"},{"location":"slides/session/#row-echelon-matrix_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} \\color{blue}{1} & 0 & -1 & 0\\ 0 & \\color{blue}{1} & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Row-echelon matrix"},{"location":"slides/session/#solve","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{bmatrix} \\color{blue}{1} & 0 & -1 & 0\\ 0 & \\color{blue}{1} & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\color{blue}{\\theta_1}\\ \\color{blue}{\\theta_2}\\ \\theta_3\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} 0\\ 0\\ 0\\ 0 \\end{bmatrix} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Solve"},{"location":"slides/session/#algorithm","text":"::: {.columns align=left} ::: {.column width=\"100%\"} \\(B = \\{ \\}\\) ::: incremental For each independent variable \\(\\theta_i\\) : Set \\(\\theta_i = 1\\) and \\(\\theta_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(\\theta\\) to \\(B\\) ::: ::: ::: {.column width=\"0%\"} right ::: :::","title":"Algorithm"},{"location":"slides/session/#solved","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\color{blue}{1} & 0 & -1 & 0\\ 0 & \\color{blue}{1} & 2 & 1\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\color{blue}{\\theta_1}\\ \\color{blue}{\\theta_2}\\ \\theta_3\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} 0\\ 0\\ 0\\ 0 \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} $$ B = \\left { \\begin{bmatrix}1\\ -2\\ 1\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\ -1\\ 0\\ 1\\end{bmatrix} \\right } $$ ::: :::","title":"Solved"},{"location":"slides/session/#x-theta-y","text":"::: {.columns align=center} ::: {.column width=\"100%\"} When will \\(X \\theta = y\\) have a solution? ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\ x_1 & \\cdots & x_n\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\ \\vdots\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_2","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\ x_1 & \\cdots & x_n\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\ \\vdots\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} $$ \\theta_1 x_1 + \\cdots + \\theta_n x_n = y $$ ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_3","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\ x_1 & \\cdots & x_n\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\ \\vdots\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\ \\vdots\\ y_m \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} $$ \\theta_1 x_1 + \\cdots + \\theta_n x_n = y $$ $$ y \\in C(X) $$ ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_4","text":"::: {.columns align=center} ::: {.column width=\"100%\"} When will \\(X \\theta = y\\) have a solution? \\(y \\in C(X)\\) ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_5","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: incremental \\(X\\) is the data-matrix \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features ::: ::: ::: {.column width=\"50%\"} ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_6","text":"::: {.columns align=left} ::: {.column width=\"50%\"} \\(X\\) is the data-matrix \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features ::: ::: {.column width=\"50%\"} Typical dataset: ::: incremental \\(m = 10000\\) \\(n = 10\\) Can \\(10\\) vectors span \\(\\mathbb{R}^{10000}\\) ? ::: ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-y_7","text":"::: {.columns align=left} ::: {.column width=\"50%\"} \\(X\\) is the data-matrix \\(m \\times n\\) \\(m\\) data-points, \\(n\\) features ::: ::: {.column width=\"50%\"} Typical dataset: \\(m = 10000\\) \\(n = 10\\) Can \\(10\\) vectors span \\(\\mathbb{R}^{10000}\\) ? ::: ::: ::: {.columns align=center} ::: {.column width=\"100%\"} \\(X \\theta = y\\) is generally unsolvable. ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta = y\\)"},{"location":"slides/session/#x-theta-approx-y","text":"::: {.columns align=left} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#x-theta-approx-y_1","text":"::: {.columns align=left} ::: {.column width=\"100%\"} What does the \\(\\approx\\) symbol mean? ::: incremental \\(1.234 \\approx 1\\) \\(1.234 \\approx 1.2\\) \\(1.234 \\approx 1.23\\) ::: ::: ::: {.column width=\"0%\"} right ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#x-theta-approx-y_2","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#x-theta-approx-y_3","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ ||\\hat{y} - y|| $$ ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#x-theta-approx-y_4","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ ||\\hat{y} - y||^2 $$ ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#x-theta-approx-y_5","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\hat{y} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ ||\\hat{y} - y||^2 = (\\hat{y}_1 - y_1)^2 + \\cdots + (\\hat{y}_m - y_m)^2 $$ ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#x-theta-approx-y_6","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ X\\hat{\\theta} \\approx y $$ ::: ::: {.column width=\"50%\"} $$ \\hat{\\theta} = \\arg \\min \\limits_{\\theta} ||X\\theta - y||^2 $$ ::: :::","title":"\\(X \\theta \\approx y\\)"},{"location":"slides/session/#loss","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{L = ||X \\theta - y||^2} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Loss"},{"location":"slides/session/#loss_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L &= ||X \\theta - y||^2 \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Loss"},{"location":"slides/session/#loss_2","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\begin{aligned} L &= ||X \\theta - y||^2\\\\ &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Loss"},{"location":"slides/session/#optimization","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\begin{aligned} L &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Optimization"},{"location":"slides/session/#optimization_1","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\begin{aligned} L &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_{\\theta} L = \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\ \\vdots\\ \\cfrac{\\partial L}{\\partial \\theta_n} \\end{bmatrix} = 0 $$ ::: :::","title":"Optimization"},{"location":"slides/session/#optimization_2","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ \\begin{aligned} L &= (X \\theta - y)^T (X \\theta - y) \\end{aligned} $$ ::: ::: {.column width=\"50%\"} $$ \\nabla_{\\theta} L = 2(X^TX)\\theta - 2X^Ty = 0 $$ ::: :::","title":"Optimization"},{"location":"slides/session/#normal-equations","text":"::: {.columns align=center} ::: {.column width=\"100%\"} $$ \\huge{(X^TX)\\theta = X^Ty} $$ ::: ::: {.column width=\"0%\"} right ::: :::","title":"Normal Equations"},{"location":"slides/session/#solution","text":"::: {.columns align=center} ::: {.column width=\"100%\"} If \\(X^TX\\) is invertible, then: $$ \\hat{\\theta} = (X^TX)^{-1} X^Ty $$ When is \\(X^TX\\) invertible? ::: ::: {.column width=\"0%\"} right ::: :::","title":"Solution"},{"location":"slides/session/#xtx","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: incremental \\(n \\times n\\) matrix Symmetric ::: ::: ::: {.column width=\"50%\"} ::: :::","title":"\\(X^TX\\)"},{"location":"slides/session/#xtx_1","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ N(X) = N(X^TX) $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"\\(X^TX\\)"},{"location":"slides/session/#xtx_2","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ N(X) = N(X^TX) $$ ::: ::: {.column width=\"50%\"} Proof If \\(\\theta \\in N(X)\\) : ::: incremental \\(X\\theta =0\\) \\(X^TX \\theta = 0\\) ::: ::: :::","title":"\\(X^TX\\)"},{"location":"slides/session/#xtx_3","text":"::: {.columns align=left} ::: {.column width=\"50%\"} $$ N(X) = N(X^TX) $$ ::: ::: {.column width=\"50%\"} Proof If \\(\\theta \\in N(X)\\) : \\(X\\theta =0\\) \\(X^TX \\theta = 0\\) If \\(\\theta \\in N(X^T X)\\) ::: incremental \\(X^TX \\theta = 0\\) \\(\\theta^T X^TX \\theta = 0\\) \\((X \\theta)^T (X \\theta) = 0\\) \\(X \\theta = 0\\) ::: ::: :::","title":"\\(X^TX\\)"},{"location":"slides/session/#xtx_4","text":"::: {.columns align=left} ::: {.column width=\"50%\"} If \\(\\text{rank}(X) = n\\) , \\(X^TX\\) is invertible ::: ::: {.column width=\"50%\"} Proof If \\(\\text{rank}(X) = n\\) : ::: incremental \\(\\text{nullity}(X) = 0\\) \\(\\text{nullity}(X^TX) = 0\\) \\(\\text{rank}(X^TX) = n\\) \\(X^TX\\) is full rank, hence invertible ::: ::: :::","title":"\\(X^TX\\)"},{"location":"slides/session/#geometry","text":"::: {.columns align=center} ::: {.column width=\"100%\"} What does a linear regression model look like? ::: ::: {.column width=\"0%\"} right ::: :::","title":"Geometry"},{"location":"slides/session/#data_1","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: :::","title":"Data"},{"location":"slides/session/#linear-model","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: :::","title":"Linear model"},{"location":"slides/session/#errors","text":"::: {.columns align=center} ::: {.column width=\"100%\"} ::: ::: {.column width=\"0%\"} right ::: :::","title":"Errors"},{"location":"slides/session/#projections","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ (X^TX)\\hat{\\theta} = X^Ty $$ \\(X \\hat{\\theta}\\) is an approximation for \\(y\\) How are \\(X \\hat{\\theta}\\) and \\(y\\) related geometrically? ::: ::: {.column width=\"50%\"} ::: :::","title":"Projections"},{"location":"slides/session/#projections_1","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ (X^TX)\\hat{\\theta} = X^Ty $$ \\(X \\hat{\\theta}\\) is an approximation for \\(y\\) How are \\(X \\hat{\\theta}\\) and \\(y\\) related geometrically? ::: ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} 2 & 6\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\ 4 \\end{bmatrix}, X \\hat{\\theta} = \\begin{bmatrix} 4\\ 2 \\end{bmatrix} $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_2","text":"::: {.columns align=left} ::: {.column width=\"70%\"} ::: ::: {.column width=\"30%\"} $$ X = \\begin{bmatrix} 2 & 6\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\ 4 \\end{bmatrix}, X \\hat{\\theta} = \\begin{bmatrix} 4\\ 2 \\end{bmatrix} $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_3","text":"::: {.columns align=left} ::: {.column width=\"70%\"} ::: ::: {.column width=\"30%\"} $$ X = \\begin{bmatrix} 2 & 6\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\ 4 \\end{bmatrix}, X \\hat{\\theta} = \\begin{bmatrix} 4\\ 2 \\end{bmatrix} $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_4","text":"::: {.columns align=left} ::: {.column width=\"50%\"} ::: ::: {.column width=\"50%\"} $$ e = y - X \\hat{\\theta} $$ $$ e \\perp C(X) $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_5","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Projections"},{"location":"slides/session/#projections_6","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} ::: :::","title":"Projections"},{"location":"slides/session/#projections_7","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_8","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ $$ X^T e = 0 $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_9","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ $$ X^T e = 0 $$ $$ X^T(y - X\\hat{\\theta}) = 0 $$ ::: :::","title":"Projections"},{"location":"slides/session/#projections_10","text":"::: {.columns align=center} ::: {.column width=\"50%\"} $$ X = \\begin{bmatrix} \\vert & & \\vert\\ x_1 & \\cdots & x_n\\ \\vert & & \\vert \\end{bmatrix} $$ $$ e \\perp x_i $$ ::: ::: {.column width=\"50%\"} $$ x_i^T e = 0 $$ $$ X^T e = 0 $$ $$ X^T(y - X\\hat{\\theta}) = 0 $$ $$ X^TX \\hat{\\theta} = X^T y $$ ::: :::","title":"Projections"},{"location":"slides/session/#summary","text":"Linear Algebra for ML Regression problem Dataset Linear regression model Algebra \\(X \\theta = 0\\) \\(X \\theta = y\\) \\(X \\theta \\approx y\\) Geometry Best-fit Projections","title":"Summary"},{"location":"week-1/dataset/","text":"Datasets \u00b6 Question What is a dataset and why is it important? Datasets \u00b6 There are different kinds of datasets. The housing dataset that we saw right at the beginning is tabular data. Each column is called an attribute or a feature and each row represents one record or observation. By far, this is the most common form in which data is represented. Tabular data can be neatly packed into comma-separated files or CSVs. Few other forms of data: image text speech Image, text and speech data cannot be packed into simple CSVs. Courses in the degree level will discuss each of these data-formats in more detail. Whence comes data? \u00b6 How do we obtain data? Where does data come from? This seems like a simple question but it doesn't have a simple answer. Here are some scenarios: Scenario-1 An FMCG company has given you some historical data concerning its sales over the last three years. It wants you to predict the average sales in the coming quarter. Here we are lucky. Someone comes to our doorstep and gives us the data. In addition, we also have a very precise definition of the problem statement. We have to predict a real number by looking at the data. It is a regression problem. Scenario-2 Twitter is developing an algorithm to detect tweets that contain offensive content. As a data scientist, you are given a dump of \\(1\\) million tweets and asked to develop an algorithm to solve the problem. This is a much more challenging problem. First, this is a clear instance of a classification problem. For each tweet, the prediction is going to be binary: \"offensive\" or \"not-offensive\". In order to learn a classifier, we need to know which tweets are offensive and which are not. Unfortunately, we don't have that information. If that information is absent, how can we teach the computer to differentiate between the two? So, the first task here is to get the dataset labeled. Amazon Mechanical Turk is a common solution that companies go for when they have to label massive quantities of data. Once the dataset is labeled, the problem becomes more tractable. Scenario-3 You are a research scientist at a manufacturing company. You want to set up a facility that automates the segregation of defective products from the non-defective ones. This is by far the most challenging scenario. We don't have access to the data. We need to gather data in the first place. Once we have the data, we need to label it or annotate it. Only then can we start building classifiers. Supervision \u00b6 Labeling is an important part of the ML pipeline, especially if we want to do some prediction on top of the data. However, there may be situations where labeling is not practically feasible. In such cases, we will have to settle with unlabeled data. Therefore, datasets in ML can be classified into two categories: labeled dataset unlabeled dataset Techniques that work with labeled data fall under the category of supervised learning. Those that work with unlabeled data come under unsupervised learning. What is so special about the term \"supervised\"? Cambridge dictionary defines the verb supervise as follows: Supervise: Definition to watch a person or activity to make certain that everything is done correctly, safely By a slight extension of this definition, we could say that a supervisor is a teacher who tells us whether we are right or wrong. In this sense, the label performs the role of a supervisor for the machine as it is learning. With unlabeled data, there is no supervision available. Partitioning the dataset \u00b6 As humans, how do we know if someone has learnt a skill or not? Tests or exams are the way to go. Exams are so ubiquitous that we often conflate learning with scoring well in exams. However, for a machine, getting a good score in an exam is a good enough proxy for learning. For almost every skill that we can think of, there is some test or exam to evaluate our competency in that skill. Take the analogy that we have been working with: three-digit addition. To know if kids have learnt addition, teachers conduct tests that have problems on three digit addition. An important feature of testing is to make sure that it is challenging. If we ask the same questions that are there in the textbook, kids might score high marks. But chances are that a lot of them would have memorized the answers. Therefore, whenever we have a dataset, we always partition it into two parts: train-dataset test-dataset We train the model on the train-dataset and evaluate its performance on the test-dataset. But often, we don't stop with two partitions, we go for three partitions: train-dataset validation-dataset test-dataset Think about the validation-dataset as additional problems for practice or a mock exam that helps the machine learn a good model. The test-dataset is not shown to the model during the learning stage. The learning algorithm has access to only the train-dataset and the validation-dataset. Once the learning process is complete, the model is then evaluated on the test-dataset.","title":"Datasets"},{"location":"week-1/dataset/#datasets","text":"Question What is a dataset and why is it important?","title":"Datasets"},{"location":"week-1/dataset/#datasets_1","text":"There are different kinds of datasets. The housing dataset that we saw right at the beginning is tabular data. Each column is called an attribute or a feature and each row represents one record or observation. By far, this is the most common form in which data is represented. Tabular data can be neatly packed into comma-separated files or CSVs. Few other forms of data: image text speech Image, text and speech data cannot be packed into simple CSVs. Courses in the degree level will discuss each of these data-formats in more detail.","title":"Datasets"},{"location":"week-1/dataset/#whence-comes-data","text":"How do we obtain data? Where does data come from? This seems like a simple question but it doesn't have a simple answer. Here are some scenarios: Scenario-1 An FMCG company has given you some historical data concerning its sales over the last three years. It wants you to predict the average sales in the coming quarter. Here we are lucky. Someone comes to our doorstep and gives us the data. In addition, we also have a very precise definition of the problem statement. We have to predict a real number by looking at the data. It is a regression problem. Scenario-2 Twitter is developing an algorithm to detect tweets that contain offensive content. As a data scientist, you are given a dump of \\(1\\) million tweets and asked to develop an algorithm to solve the problem. This is a much more challenging problem. First, this is a clear instance of a classification problem. For each tweet, the prediction is going to be binary: \"offensive\" or \"not-offensive\". In order to learn a classifier, we need to know which tweets are offensive and which are not. Unfortunately, we don't have that information. If that information is absent, how can we teach the computer to differentiate between the two? So, the first task here is to get the dataset labeled. Amazon Mechanical Turk is a common solution that companies go for when they have to label massive quantities of data. Once the dataset is labeled, the problem becomes more tractable. Scenario-3 You are a research scientist at a manufacturing company. You want to set up a facility that automates the segregation of defective products from the non-defective ones. This is by far the most challenging scenario. We don't have access to the data. We need to gather data in the first place. Once we have the data, we need to label it or annotate it. Only then can we start building classifiers.","title":"Whence comes data?"},{"location":"week-1/dataset/#supervision","text":"Labeling is an important part of the ML pipeline, especially if we want to do some prediction on top of the data. However, there may be situations where labeling is not practically feasible. In such cases, we will have to settle with unlabeled data. Therefore, datasets in ML can be classified into two categories: labeled dataset unlabeled dataset Techniques that work with labeled data fall under the category of supervised learning. Those that work with unlabeled data come under unsupervised learning. What is so special about the term \"supervised\"? Cambridge dictionary defines the verb supervise as follows: Supervise: Definition to watch a person or activity to make certain that everything is done correctly, safely By a slight extension of this definition, we could say that a supervisor is a teacher who tells us whether we are right or wrong. In this sense, the label performs the role of a supervisor for the machine as it is learning. With unlabeled data, there is no supervision available.","title":"Supervision"},{"location":"week-1/dataset/#partitioning-the-dataset","text":"As humans, how do we know if someone has learnt a skill or not? Tests or exams are the way to go. Exams are so ubiquitous that we often conflate learning with scoring well in exams. However, for a machine, getting a good score in an exam is a good enough proxy for learning. For almost every skill that we can think of, there is some test or exam to evaluate our competency in that skill. Take the analogy that we have been working with: three-digit addition. To know if kids have learnt addition, teachers conduct tests that have problems on three digit addition. An important feature of testing is to make sure that it is challenging. If we ask the same questions that are there in the textbook, kids might score high marks. But chances are that a lot of them would have memorized the answers. Therefore, whenever we have a dataset, we always partition it into two parts: train-dataset test-dataset We train the model on the train-dataset and evaluate its performance on the test-dataset. But often, we don't stop with two partitions, we go for three partitions: train-dataset validation-dataset test-dataset Think about the validation-dataset as additional problems for practice or a mock exam that helps the machine learn a good model. The test-dataset is not shown to the model during the learning stage. The learning algorithm has access to only the train-dataset and the validation-dataset. Once the learning process is complete, the model is then evaluated on the test-dataset.","title":"Partitioning the dataset"},{"location":"week-1/linear_algebra/","text":"Linear Algebra for ML \u00b6 Question Why should we study linear algebra in a course on data science? Data \u00b6 The simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it: lattitude longitue age num_of_rooms area distance_from_school A typical problem in ML is this: Given these attributes or features of a house, can we predict its selling price? This is called a regression problem : given a set of features, map it to a real number. Vectors \u00b6 Let us take a concrete example of a single data-point : Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 num_of_rooms 2 area 1000 distance_from_nearest_school 3 selling_price 40 Lattitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs. But none of these really matter for an ML algorithm: it is going to abstract out the details (such as units) and look at this as a column of numbers, which is nothing but a vector: \\[ \\begin{bmatrix} 12.9\\\\ 80.2\\\\ 3\\\\ 2\\\\ 1000\\\\ 3\\\\ \\end{bmatrix} \\] Note that the selling price is not included as an element in the vector as that is usually unkown to us. This unkown quantity which we have to estimate or predict is called the target . Note Vectors are usually represented as column vectors or \\(n \\times 1\\) matrices. Matrices \u00b6 We cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tablular form: lattitude longitude age rooms area distance price 1 12.9 80.2 3 2 1000 3 40 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 50 14.3 75.9 30 2 1200 5 20 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 100 20.8 90.5 1 3 1500 2 35 This data for \\(100\\) houses is nothing but a \\(100 \\times 6\\) matrix: \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] Each row of this matrix corresponds to one data-point. In general, if a dataset has \\(n\\) features and \\(m\\) data-points, it is represented as a \\(m \\times n\\) data-matrix. Tip If you find yourself lost when working with matrices, remember that a matrix is a way to represent a collection of data-points (dataset). Summary \u00b6 Data is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra \u2014 the study of vectors and matrices \u2014 if we wish to understand how ML algorithms work.","title":"Linear Algebra for ML"},{"location":"week-1/linear_algebra/#linear-algebra-for-ml","text":"Question Why should we study linear algebra in a course on data science?","title":"Linear Algebra for ML"},{"location":"week-1/linear_algebra/#data","text":"The simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it: lattitude longitue age num_of_rooms area distance_from_school A typical problem in ML is this: Given these attributes or features of a house, can we predict its selling price? This is called a regression problem : given a set of features, map it to a real number.","title":"Data"},{"location":"week-1/linear_algebra/#vectors","text":"Let us take a concrete example of a single data-point : Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 num_of_rooms 2 area 1000 distance_from_nearest_school 3 selling_price 40 Lattitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs. But none of these really matter for an ML algorithm: it is going to abstract out the details (such as units) and look at this as a column of numbers, which is nothing but a vector: \\[ \\begin{bmatrix} 12.9\\\\ 80.2\\\\ 3\\\\ 2\\\\ 1000\\\\ 3\\\\ \\end{bmatrix} \\] Note that the selling price is not included as an element in the vector as that is usually unkown to us. This unkown quantity which we have to estimate or predict is called the target . Note Vectors are usually represented as column vectors or \\(n \\times 1\\) matrices.","title":"Vectors"},{"location":"week-1/linear_algebra/#matrices","text":"We cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tablular form: lattitude longitude age rooms area distance price 1 12.9 80.2 3 2 1000 3 40 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 50 14.3 75.9 30 2 1200 5 20 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 100 20.8 90.5 1 3 1500 2 35 This data for \\(100\\) houses is nothing but a \\(100 \\times 6\\) matrix: \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] Each row of this matrix corresponds to one data-point. In general, if a dataset has \\(n\\) features and \\(m\\) data-points, it is represented as a \\(m \\times n\\) data-matrix. Tip If you find yourself lost when working with matrices, remember that a matrix is a way to represent a collection of data-points (dataset).","title":"Matrices"},{"location":"week-1/linear_algebra/#summary","text":"Data is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra \u2014 the study of vectors and matrices \u2014 if we wish to understand how ML algorithms work.","title":"Summary"},{"location":"week-1/ml_problem/","text":"ML problem \u00b6 Question What does a typical ML problem look like? Regression \u00b6 Analogy \u00b6 Think about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and learns how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers: \\(103 + 205 = 308\\) \\(123 + 409 = 532\\) \\(185 + 483 = 668\\) During the instructional hours, the student has access to both the questions and the answers. In the exam, she will not have access to the answers! But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. She needs to have a mental model of addition. In other words, she would have to learn a function from the input (question) to the output (answer). This is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels . A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset . A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy. Data Representation \u00b6 We are given a collection of \\(m\\) data-points and \\(m\\) labels which are real numbers. Each data-point is described by \\(n\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as lattitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(n\\) . Arranging the \\(m\\) data-points in a matrix, we get a \\(m \\times n\\) data-matrix. Let us call this matrix \\(X\\) : \\[ X = \\begin{bmatrix} x_{11} & \\cdots & x_{1n}\\\\ \\vdots & x_{ij} & \\vdots\\\\ x_{m1} & \\cdots & x_{mn} \\end{bmatrix} \\] Each row of this matrix is the feature vector for one data-point. The labels can be put together in a vector of size \\(n\\) . Let us call this \\(y\\) : \\[ y = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_m \\end{bmatrix} \\] Model \u00b6 As stated earlier, a regression model can be viewed as a function that transforms a data-point into a label. Formally: \\[ f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} \\] Each feature-vector is of size \\(n\\) . So, the feature-vectors reside in the \\(n\\) dimensional space \\(\\mathbb{R}^{n}\\) . The labels are real numbers, so they reside in \\(\\mathbb{R}\\) . Mathematically, this is the action of a model on a data-point \\(x\\) : \\[ y = f(x) \\] Pictorially: graph LR A[Feature-vector] --> B[Model]; B[Model] --> C[Label]; What is so special about a ML model? All models take some input and produce a corresponding output. The key difference is that in a classical programming setting, we are usually given the input and the function, we need to find the output. In machine learning we are given both the input and the output, we have to learn a model \\(f\\) . The function or the model is the unknown. This is what has to be learnt. Tip You can think about an ML model as a function that maps a feature-vector to a label. Learning \u00b6 ML is all about learning from data. But who or what is learning? More importantly, who or what enables the learning? There is a learning algorithm which drives the learning. We can think of the model as the outcome of the learning process. During the learning stage, the dataset is fed as input to a learning algorithm, which in turn outputs a model. graph LR A[Labeled-dataset] --> B[Learning-algorithm]; B[Learning-algorithm] --> C[Model]; There is one important detail that is missing in this diagram. There are several models from which we could choose from. Going back to our analogy, there are different ways to understand three digit addition: representing numbers as counts of physical objects representing numbers as abstract entities that can be manipulated Teachers might choose the first model to help kids understand addition. As kids grow up, teachers might move to the second model, which is more sophisticated. Something similar happens in ML. We are the teachers for the machines. Our responsibility is to choose a family or a space of models and present it to the machine. graph LR A[Labeled-dataset] --> B[Learning-algorithm]; D[Model-family] --> B[Learnin-algorithm] B[Learning-algorithm] --> C[Model]; Thus there are two inputs to the learning algorithm: labeled dataset family of models The task of the algorithm is to explore the space of models and pick the one that best fits the labeled dataset. Note For some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in the week-3. Classification \u00b6 Regression is one type of ML problem. Classification is another kind of ML problem, and perhaps the most popular. We shall begin with an example. Summary \u00b6 Regression is a classic ML problem where we use labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(X\\) . The labels are arranged in a label vector called \\(y\\) . A model is a function that transforms a feature-vector to a label.","title":"ML problem"},{"location":"week-1/ml_problem/#ml-problem","text":"Question What does a typical ML problem look like?","title":"ML problem"},{"location":"week-1/ml_problem/#regression","text":"","title":"Regression"},{"location":"week-1/ml_problem/#analogy","text":"Think about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and learns how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers: \\(103 + 205 = 308\\) \\(123 + 409 = 532\\) \\(185 + 483 = 668\\) During the instructional hours, the student has access to both the questions and the answers. In the exam, she will not have access to the answers! But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. She needs to have a mental model of addition. In other words, she would have to learn a function from the input (question) to the output (answer). This is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels . A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset . A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy.","title":"Analogy"},{"location":"week-1/ml_problem/#data-representation","text":"We are given a collection of \\(m\\) data-points and \\(m\\) labels which are real numbers. Each data-point is described by \\(n\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as lattitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(n\\) . Arranging the \\(m\\) data-points in a matrix, we get a \\(m \\times n\\) data-matrix. Let us call this matrix \\(X\\) : \\[ X = \\begin{bmatrix} x_{11} & \\cdots & x_{1n}\\\\ \\vdots & x_{ij} & \\vdots\\\\ x_{m1} & \\cdots & x_{mn} \\end{bmatrix} \\] Each row of this matrix is the feature vector for one data-point. The labels can be put together in a vector of size \\(n\\) . Let us call this \\(y\\) : \\[ y = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_m \\end{bmatrix} \\]","title":"Data Representation"},{"location":"week-1/ml_problem/#model","text":"As stated earlier, a regression model can be viewed as a function that transforms a data-point into a label. Formally: \\[ f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} \\] Each feature-vector is of size \\(n\\) . So, the feature-vectors reside in the \\(n\\) dimensional space \\(\\mathbb{R}^{n}\\) . The labels are real numbers, so they reside in \\(\\mathbb{R}\\) . Mathematically, this is the action of a model on a data-point \\(x\\) : \\[ y = f(x) \\] Pictorially: graph LR A[Feature-vector] --> B[Model]; B[Model] --> C[Label]; What is so special about a ML model? All models take some input and produce a corresponding output. The key difference is that in a classical programming setting, we are usually given the input and the function, we need to find the output. In machine learning we are given both the input and the output, we have to learn a model \\(f\\) . The function or the model is the unknown. This is what has to be learnt. Tip You can think about an ML model as a function that maps a feature-vector to a label.","title":"Model"},{"location":"week-1/ml_problem/#learning","text":"ML is all about learning from data. But who or what is learning? More importantly, who or what enables the learning? There is a learning algorithm which drives the learning. We can think of the model as the outcome of the learning process. During the learning stage, the dataset is fed as input to a learning algorithm, which in turn outputs a model. graph LR A[Labeled-dataset] --> B[Learning-algorithm]; B[Learning-algorithm] --> C[Model]; There is one important detail that is missing in this diagram. There are several models from which we could choose from. Going back to our analogy, there are different ways to understand three digit addition: representing numbers as counts of physical objects representing numbers as abstract entities that can be manipulated Teachers might choose the first model to help kids understand addition. As kids grow up, teachers might move to the second model, which is more sophisticated. Something similar happens in ML. We are the teachers for the machines. Our responsibility is to choose a family or a space of models and present it to the machine. graph LR A[Labeled-dataset] --> B[Learning-algorithm]; D[Model-family] --> B[Learnin-algorithm] B[Learning-algorithm] --> C[Model]; Thus there are two inputs to the learning algorithm: labeled dataset family of models The task of the algorithm is to explore the space of models and pick the one that best fits the labeled dataset. Note For some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in the week-3.","title":"Learning"},{"location":"week-1/ml_problem/#classification","text":"Regression is one type of ML problem. Classification is another kind of ML problem, and perhaps the most popular. We shall begin with an example.","title":"Classification"},{"location":"week-1/ml_problem/#summary","text":"Regression is a classic ML problem where we use labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(X\\) . The labels are arranged in a label vector called \\(y\\) . A model is a function that transforms a feature-vector to a label.","title":"Summary"},{"location":"week-1/outline/","text":"Outline \u00b6 This week will provide a gentle introduction to ML. Why should we study linear algebra in a course on data science? What does a typical ML problem look like? What is a dataset and why is it important?","title":"Outline"},{"location":"week-1/outline/#outline","text":"This week will provide a gentle introduction to ML. Why should we study linear algebra in a course on data science? What does a typical ML problem look like? What is a dataset and why is it important?","title":"Outline"},{"location":"week-3/best_fit/","text":"Best Fit Hyperplane \u00b6 Question What does a linear model look like? 1-D problem \u00b6 Let us return to the housing dataset. We are trying to predict the selling price of a house based on some features or attributes. We started with six of these attributes. Along with the constant term, we have seven parameters for the linear model: \\(\\theta = [\\theta_0, \\cdots, \\theta_6]^T\\) . If we think about the points in the labeled dataset, each of them can be represented as a point in seven dimensional space: \\([x_1, \\cdots, x_6, y]^T\\) . This is impossible to visualize. To make the visualization problem more tractable, let us reduce the number of features to one. In this case, the dataset is a set of points in 2D space. If there are seven houses in the dataset, then we have seven points: $$ (x_1, y_1), \\cdots, (x_{7}, y_{7}) $$ Each house is represented by a pair of numbers: \\((x, y)\\) . Let us say that \\(x\\) is the area and \\(y\\) is the selling price. If we plot these points in 2D space, we have: The linear model is: $$ y = \\theta_0 + \\theta_1 x $$ Here, \\(y\\) and ax$ are used in the sense of general variables and not particular values. So, what does this model look like? This is nothing but the equation of a line: So, the linear model is a line. Recall that we are trying to find that line (model) which minimizes the sum of the squared errors. First, let us see what the errors look like in this diagram. The error is the difference between the predicted selling price ( \\(\\theta_0 + \\theta_1 x\\) ) and the actual selling price \\((y)\\) : $$ e = y - (\\theta_0 + \\theta_1x) $$ The red points represent the predicted selling prices. The errors are the vertical line segments connecting the blue points and the corresponding red points. Note The error is parallel to the \\(y\\) -axis and is not perpendicular to the line. For various values of \\(\\theta\\) , we will get different lines. The line for which the sum of the squared errors is minimum is called the best-fit line. For a problem having one feature: the dataset resides in 2D space the model is a line, a one-dimensional object embedded in a two-dimensional space. n-D problem \u00b6 What about higher dimensions? If there are two features, we will have a best-fit plane: $$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2x_2 $$ Here, \\(x_i\\) s are the features, \\(y\\) is the predicted selling price of a house. Again, note that we are using \\(y\\) and \\(x_i\\) s in the sense of variables to define the equation of a curve. This plane is a two-dimensional object that is embedded in 3D space. In general, if there are \\(n\\) features, we have an \\(n\\) dimensional hyperplane embedded in \\(n + 1\\) dimensional space. The equation of this hyperplane is given by: $$ y = \\theta_0 + \\theta_1x_1 + \\cdots + \\theta_n x_n $$ Visualizing this hyperplane is beyond the scope of human abilities. So, we typically restrict ourselves to 2 or 3 dimensions. Summary \u00b6 In a linear regression problem that has \\(n\\) features and a real valued label, we can think of the points as residing in a \\(n + 1\\) dimensional space. We try to find a \\(n\\) -dimensional hyperplane embedded in this \\(n + 1\\) dimensional space that minimizes the sum of the squared errors between the predicted labels and the actual labels. This hyperplane is the best-fit for the points in the labeled dataset.","title":"Best Fit Hyperplane"},{"location":"week-3/best_fit/#best-fit-hyperplane","text":"Question What does a linear model look like?","title":"Best Fit Hyperplane"},{"location":"week-3/best_fit/#1-d-problem","text":"Let us return to the housing dataset. We are trying to predict the selling price of a house based on some features or attributes. We started with six of these attributes. Along with the constant term, we have seven parameters for the linear model: \\(\\theta = [\\theta_0, \\cdots, \\theta_6]^T\\) . If we think about the points in the labeled dataset, each of them can be represented as a point in seven dimensional space: \\([x_1, \\cdots, x_6, y]^T\\) . This is impossible to visualize. To make the visualization problem more tractable, let us reduce the number of features to one. In this case, the dataset is a set of points in 2D space. If there are seven houses in the dataset, then we have seven points: $$ (x_1, y_1), \\cdots, (x_{7}, y_{7}) $$ Each house is represented by a pair of numbers: \\((x, y)\\) . Let us say that \\(x\\) is the area and \\(y\\) is the selling price. If we plot these points in 2D space, we have: The linear model is: $$ y = \\theta_0 + \\theta_1 x $$ Here, \\(y\\) and ax$ are used in the sense of general variables and not particular values. So, what does this model look like? This is nothing but the equation of a line: So, the linear model is a line. Recall that we are trying to find that line (model) which minimizes the sum of the squared errors. First, let us see what the errors look like in this diagram. The error is the difference between the predicted selling price ( \\(\\theta_0 + \\theta_1 x\\) ) and the actual selling price \\((y)\\) : $$ e = y - (\\theta_0 + \\theta_1x) $$ The red points represent the predicted selling prices. The errors are the vertical line segments connecting the blue points and the corresponding red points. Note The error is parallel to the \\(y\\) -axis and is not perpendicular to the line. For various values of \\(\\theta\\) , we will get different lines. The line for which the sum of the squared errors is minimum is called the best-fit line. For a problem having one feature: the dataset resides in 2D space the model is a line, a one-dimensional object embedded in a two-dimensional space.","title":"1-D problem"},{"location":"week-3/best_fit/#n-d-problem","text":"What about higher dimensions? If there are two features, we will have a best-fit plane: $$ y = \\theta_0 + \\theta_1 x_1 + \\theta_2x_2 $$ Here, \\(x_i\\) s are the features, \\(y\\) is the predicted selling price of a house. Again, note that we are using \\(y\\) and \\(x_i\\) s in the sense of variables to define the equation of a curve. This plane is a two-dimensional object that is embedded in 3D space. In general, if there are \\(n\\) features, we have an \\(n\\) dimensional hyperplane embedded in \\(n + 1\\) dimensional space. The equation of this hyperplane is given by: $$ y = \\theta_0 + \\theta_1x_1 + \\cdots + \\theta_n x_n $$ Visualizing this hyperplane is beyond the scope of human abilities. So, we typically restrict ourselves to 2 or 3 dimensions.","title":"n-D problem"},{"location":"week-3/best_fit/#summary","text":"In a linear regression problem that has \\(n\\) features and a real valued label, we can think of the points as residing in a \\(n + 1\\) dimensional space. We try to find a \\(n\\) -dimensional hyperplane embedded in this \\(n + 1\\) dimensional space that minimizes the sum of the squared errors between the predicted labels and the actual labels. This hyperplane is the best-fit for the points in the labeled dataset.","title":"Summary"},{"location":"week-3/least_squares/","text":"X\u03b8 \u2248 y \u00b6 Question How do we solve for \\(\\theta\\) in \\(X\\theta \\approx y\\) ? Approximation \u00b6 So far we have seen the well behaved case of \\(X\\theta = y\\) . What if \\(y\\) is not in the column space of \\(X\\) ? Then \\(X\\theta \\neq y\\) . This is by far the most interesting and relevant scenario from the perspective of the linear regression problem. Recall that we are trying to estimate a weight vector \\(\\theta\\) , given the labeled dataset \\((X, y)\\) . The relationship between inputs and outputs is assumed to be linear. But the real world doesn't behave exactly like we want it to. There are going to be obvious deviations from our ideal assumptions. In other words, our models can never be an entirely accurate representation of reality. This is a typical scenario observed in engineering and the sciences. We don't abandon the problem because it doesn't admit an exact solution. Instead, we turn to a powerful weapon in our armoury: approximations. Can we find a \\(\\hat{\\theta}\\) such that \\(X \\hat{\\theta} \\approx y\\) ? What do we mean by the symbol \\(\\approx\\) here? We are dealing with two vectors on either side of the symbol. Let us first understand what the symbol means when we have scalars. Let us look at the following statements: \\(1.234 \\approx 1\\) \\(1.234 \\approx 1.2\\) \\(1.234 \\approx 1.23\\) These are three approximations. From experience, we know that the second approximation is better than the first, the third better than the second. If we plot these points on a real line, the goodness of an approximation can be interpreted using the distance between the original point and its approximation: smaller the distance, better the approximation. This very idea is extended to an \\(n\\) dimensional vector space. If \\(y\\) and \\(\\hat{y}\\) are two vectors in \\(\\mathbb{R}^m\\) , then the distance \\(d\\) between them is a measure of the goodness of the approximation. To avoid taking square roots, we write down the expression for \\(d^2\\) : \\[ d^2 = ||\\hat{y} - y||^2 = (\\hat{y}_1 - y_1)^2 + \\cdots + (\\hat{y}_m - y_m)^2 \\] So, the key to solving the problem \\(X\\theta \\approx y\\) , is to find a vector \\(\\hat{\\theta}\\) such that \\(X\\hat{\\theta}\\) is as close to \\(y\\) as possible. This can be framed as an optimization problem: \\[ \\hat{\\theta} = \\arg \\min \\limits_{\\theta} ||X\\theta - y||^2 \\] If you are seeing \\(\\arg \\min\\) for the first time, think about it like a function (in the programming sense): Find the value the minimizes the expression Return this value to the user Objective function \u00b6 Let us go ahead and solve the minimization problem. \\[ \\min \\limits_{\\theta} ||X\\theta - y||^2 \\] Let us call the expression to be minimized the loss, \\(L\\) , which is also called the objective function of the optimization problem. It can be simplified as follows: \\[ \\begin{align} L &= ||X\\theta - y||^2\\\\\\\\ &= (X\\theta - y)^T(X\\theta - y)\\\\\\\\ \\end{align} \\] Normal Equations \u00b6 We now follow the usual procedure of finding the minima of a function. Take the derivatives and set them to zero. Since we are dealing with a multivariable function, we have to consider all the partial derivatives. The vector of such partial derivatives is called the gradient. \\[ \\nabla_{\\theta} L = \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\\\ \\vdots\\\\ \\cfrac{\\partial L}{\\partial \\theta_n} \\end{bmatrix} \\] Let us now compute the gradient and set it to zero. If you want a detailed mathematical explanation of how the gradient is computed, refer to the section on gradients : \\[ \\nabla_{\\theta} L = 2(X^TX)\\theta - 2X^Ty = 0 \\] This gives us the following equation: $$ (X^TX) \\theta = X^Ty $$ This system is called the normal equations . If the matrix \\(X^TX\\) is invertible, then we have the following solution: \\[ \\hat{\\theta} = (X^TX)^{-1} X^Ty \\] We still don't know if this is corresponds to a minima. Rest assured that this is indeed a minima. But we won't take up the proof now. The other worrying part is what happens if the matrix \\(X^TX\\) is singular or non-invertible. This is also something that we skip for the time being. Least Squares Problem \u00b6 This is often called the least squares problem. To see why this name is used, let us revisit the objective function: \\[ L = (X\\theta - y)^T (X\\theta - y) \\] Since \\(X\\theta\\) is an approximation of \\(y\\) , \\(e = X\\theta - y\\) can be seen as an error vector. So: \\[ L = e^T e = e_1^2 + \\cdots + e_m^2 \\] From a ML standpoint, recall the housing dataset with which we started. \\(e_i\\) is the difference between the actual selling price of the \\(i^{th}\\) house and its predicted selling price as given by our linear model. In a sense, we are justified in calling \\(e\\) the error vector. We are minimizing the sum of the squared errors, hence \"least squares\". \\(\\hat{\\theta}\\) is that weight vector for the linear model which will give us the best possible fit. In the next few units, we shall try to understand the least squares problem from the lens of geometry. Summary \u00b6","title":"X\u03b8 \u2248 y"},{"location":"week-3/least_squares/#x-y","text":"Question How do we solve for \\(\\theta\\) in \\(X\\theta \\approx y\\) ?","title":"X\u03b8 \u2248 y"},{"location":"week-3/least_squares/#approximation","text":"So far we have seen the well behaved case of \\(X\\theta = y\\) . What if \\(y\\) is not in the column space of \\(X\\) ? Then \\(X\\theta \\neq y\\) . This is by far the most interesting and relevant scenario from the perspective of the linear regression problem. Recall that we are trying to estimate a weight vector \\(\\theta\\) , given the labeled dataset \\((X, y)\\) . The relationship between inputs and outputs is assumed to be linear. But the real world doesn't behave exactly like we want it to. There are going to be obvious deviations from our ideal assumptions. In other words, our models can never be an entirely accurate representation of reality. This is a typical scenario observed in engineering and the sciences. We don't abandon the problem because it doesn't admit an exact solution. Instead, we turn to a powerful weapon in our armoury: approximations. Can we find a \\(\\hat{\\theta}\\) such that \\(X \\hat{\\theta} \\approx y\\) ? What do we mean by the symbol \\(\\approx\\) here? We are dealing with two vectors on either side of the symbol. Let us first understand what the symbol means when we have scalars. Let us look at the following statements: \\(1.234 \\approx 1\\) \\(1.234 \\approx 1.2\\) \\(1.234 \\approx 1.23\\) These are three approximations. From experience, we know that the second approximation is better than the first, the third better than the second. If we plot these points on a real line, the goodness of an approximation can be interpreted using the distance between the original point and its approximation: smaller the distance, better the approximation. This very idea is extended to an \\(n\\) dimensional vector space. If \\(y\\) and \\(\\hat{y}\\) are two vectors in \\(\\mathbb{R}^m\\) , then the distance \\(d\\) between them is a measure of the goodness of the approximation. To avoid taking square roots, we write down the expression for \\(d^2\\) : \\[ d^2 = ||\\hat{y} - y||^2 = (\\hat{y}_1 - y_1)^2 + \\cdots + (\\hat{y}_m - y_m)^2 \\] So, the key to solving the problem \\(X\\theta \\approx y\\) , is to find a vector \\(\\hat{\\theta}\\) such that \\(X\\hat{\\theta}\\) is as close to \\(y\\) as possible. This can be framed as an optimization problem: \\[ \\hat{\\theta} = \\arg \\min \\limits_{\\theta} ||X\\theta - y||^2 \\] If you are seeing \\(\\arg \\min\\) for the first time, think about it like a function (in the programming sense): Find the value the minimizes the expression Return this value to the user","title":"Approximation"},{"location":"week-3/least_squares/#objective-function","text":"Let us go ahead and solve the minimization problem. \\[ \\min \\limits_{\\theta} ||X\\theta - y||^2 \\] Let us call the expression to be minimized the loss, \\(L\\) , which is also called the objective function of the optimization problem. It can be simplified as follows: \\[ \\begin{align} L &= ||X\\theta - y||^2\\\\\\\\ &= (X\\theta - y)^T(X\\theta - y)\\\\\\\\ \\end{align} \\]","title":"Objective function"},{"location":"week-3/least_squares/#normal-equations","text":"We now follow the usual procedure of finding the minima of a function. Take the derivatives and set them to zero. Since we are dealing with a multivariable function, we have to consider all the partial derivatives. The vector of such partial derivatives is called the gradient. \\[ \\nabla_{\\theta} L = \\begin{bmatrix} \\cfrac{\\partial L}{\\partial \\theta_1}\\\\ \\vdots\\\\ \\cfrac{\\partial L}{\\partial \\theta_n} \\end{bmatrix} \\] Let us now compute the gradient and set it to zero. If you want a detailed mathematical explanation of how the gradient is computed, refer to the section on gradients : \\[ \\nabla_{\\theta} L = 2(X^TX)\\theta - 2X^Ty = 0 \\] This gives us the following equation: $$ (X^TX) \\theta = X^Ty $$ This system is called the normal equations . If the matrix \\(X^TX\\) is invertible, then we have the following solution: \\[ \\hat{\\theta} = (X^TX)^{-1} X^Ty \\] We still don't know if this is corresponds to a minima. Rest assured that this is indeed a minima. But we won't take up the proof now. The other worrying part is what happens if the matrix \\(X^TX\\) is singular or non-invertible. This is also something that we skip for the time being.","title":"Normal Equations"},{"location":"week-3/least_squares/#least-squares-problem","text":"This is often called the least squares problem. To see why this name is used, let us revisit the objective function: \\[ L = (X\\theta - y)^T (X\\theta - y) \\] Since \\(X\\theta\\) is an approximation of \\(y\\) , \\(e = X\\theta - y\\) can be seen as an error vector. So: \\[ L = e^T e = e_1^2 + \\cdots + e_m^2 \\] From a ML standpoint, recall the housing dataset with which we started. \\(e_i\\) is the difference between the actual selling price of the \\(i^{th}\\) house and its predicted selling price as given by our linear model. In a sense, we are justified in calling \\(e\\) the error vector. We are minimizing the sum of the squared errors, hence \"least squares\". \\(\\hat{\\theta}\\) is that weight vector for the linear model which will give us the best possible fit. In the next few units, we shall try to understand the least squares problem from the lens of geometry.","title":"Least Squares Problem"},{"location":"week-3/least_squares/#summary","text":"","title":"Summary"},{"location":"week-3/linear_model/","text":"Linear Regression \u00b6 Question What is a linear regression model? Motivation \u00b6 Let us return to the housing dataset . Consider two houses, one which has \\(1000\\) square feet and the other which has \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows: $$ \\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant} $$ This is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature is negatively correlated with the selling-price, but it is not as important as the area. Note We might be totally wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: Quote All models are wrong, but some are useful. Vector form \u00b6 Generalizing this, let us say that we have a feature vector \\(x\\) and a weight vector \\(\\theta\\) . Recall that the housing data has six features: \\[ x = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6 \\end{bmatrix}, \\theta = \\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4\\\\ \\theta_5\\\\ \\theta_6 \\end{bmatrix} \\] The function or model that maps a data-point to the label \\(y\\) (selling-price) is: \\[ y = x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 + \\text{constant} \\] We can rewrite the constant as one more weight, say \\(\\theta_0\\) : \\[ y = x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 + \\theta_0 \\] Going back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(\\theta_0\\) to the weights: \\[ x = \\begin{bmatrix} 1\\\\ x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6 \\end{bmatrix}, \\theta = \\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4\\\\ \\theta_5\\\\ \\theta_6 \\end{bmatrix} \\] If we now look at the expression for \\(y\\) , it is nothing but the dot-product of the two vectors: \\[ \\begin{aligned} y &= 1\\cdot \\theta_0 + x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 \\\\\\\\ &= \\theta^T x \\end{aligned} \\] \\(\\theta^Tx\\) is the dot product between the two vectors \\(\\theta\\) and \\(x\\) . This is the notation that we will be using for the dot product from now on. This is the same as the matrix-product of a row-vector and a column-vector: \\[ y = \\theta^T x = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & \\theta_3 & \\theta_4 & \\theta_5 & \\theta_6 \\end{bmatrix}\\begin{bmatrix} 1\\\\ x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6 \\end{bmatrix} \\] Note All vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(x^T\\) , where \\(x\\) is some column-vector. Matrix form \u00b6 So much for one house. But we have several houses. All these can be clubbed into a data-matrix. This is nothing but stacking all feature-vectors one below the other. Likewise, we can stack all selling prices into a label-vector: \\[ \\begin{bmatrix} y_{1}\\\\ \\vdots\\\\ y_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} & x_{1,5} & x_{1,6}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & x_{100,1} & x_{100,2} & x_{100,3} & x_{100,4} & x_{100,5} & x_{100,6}\\\\ \\end{bmatrix}\\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4\\\\ \\theta_5\\\\ \\theta_6 \\end{bmatrix} \\] If we call the feature matrix \\(X\\) , the label vector \\(y\\) and the weight vector \\(\\theta\\) , this is the equation we have: \\[ X\\theta = y \\] We are given both \\(X\\) and \\(y\\) . This is nothing but our labeled dataset. We have to find \\(\\theta\\) . This leaves us with two questions: Does the equation \\(X\\theta = y\\) have a solution? If it doesn't have a solution, then how do we estimate \\(\\theta\\) ? We can see how an ML problem has now turned into a linear algebra problem! We will try to answer the first question and then move on to the second question. Note A lot of details about the linear regression model have been skipped. This presentation has tried to bring out the mathematical details. For a more accurate handling of this topic, please refer to week-2 of the MLT course. Summary \u00b6 A linear regression model assumes a linear relationship between inputs and outputs. This results in a system of linear equation of the form \\(X\\theta = y\\) .","title":"Linear Regression"},{"location":"week-3/linear_model/#linear-regression","text":"Question What is a linear regression model?","title":"Linear Regression"},{"location":"week-3/linear_model/#motivation","text":"Let us return to the housing dataset . Consider two houses, one which has \\(1000\\) square feet and the other which has \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows: $$ \\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant} $$ This is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature is negatively correlated with the selling-price, but it is not as important as the area. Note We might be totally wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: Quote All models are wrong, but some are useful.","title":"Motivation"},{"location":"week-3/linear_model/#vector-form","text":"Generalizing this, let us say that we have a feature vector \\(x\\) and a weight vector \\(\\theta\\) . Recall that the housing data has six features: \\[ x = \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6 \\end{bmatrix}, \\theta = \\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4\\\\ \\theta_5\\\\ \\theta_6 \\end{bmatrix} \\] The function or model that maps a data-point to the label \\(y\\) (selling-price) is: \\[ y = x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 + \\text{constant} \\] We can rewrite the constant as one more weight, say \\(\\theta_0\\) : \\[ y = x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 + \\theta_0 \\] Going back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(\\theta_0\\) to the weights: \\[ x = \\begin{bmatrix} 1\\\\ x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6 \\end{bmatrix}, \\theta = \\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4\\\\ \\theta_5\\\\ \\theta_6 \\end{bmatrix} \\] If we now look at the expression for \\(y\\) , it is nothing but the dot-product of the two vectors: \\[ \\begin{aligned} y &= 1\\cdot \\theta_0 + x_1 \\theta_1 + x_2 \\theta_2 + x_3\\theta_3 + x_4 \\theta_4 + x_5 \\theta_5 + x_6 \\theta_6 \\\\\\\\ &= \\theta^T x \\end{aligned} \\] \\(\\theta^Tx\\) is the dot product between the two vectors \\(\\theta\\) and \\(x\\) . This is the notation that we will be using for the dot product from now on. This is the same as the matrix-product of a row-vector and a column-vector: \\[ y = \\theta^T x = \\begin{bmatrix} \\theta_0 & \\theta_1 & \\theta_2 & \\theta_3 & \\theta_4 & \\theta_5 & \\theta_6 \\end{bmatrix}\\begin{bmatrix} 1\\\\ x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6 \\end{bmatrix} \\] Note All vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(x^T\\) , where \\(x\\) is some column-vector.","title":"Vector form"},{"location":"week-3/linear_model/#matrix-form","text":"So much for one house. But we have several houses. All these can be clubbed into a data-matrix. This is nothing but stacking all feature-vectors one below the other. Likewise, we can stack all selling prices into a label-vector: \\[ \\begin{bmatrix} y_{1}\\\\ \\vdots\\\\ y_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & x_{1,1} & x_{1,2} & x_{1,3} & x_{1,4} & x_{1,5} & x_{1,6}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & x_{100,1} & x_{100,2} & x_{100,3} & x_{100,4} & x_{100,5} & x_{100,6}\\\\ \\end{bmatrix}\\begin{bmatrix} \\theta_0\\\\ \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4\\\\ \\theta_5\\\\ \\theta_6 \\end{bmatrix} \\] If we call the feature matrix \\(X\\) , the label vector \\(y\\) and the weight vector \\(\\theta\\) , this is the equation we have: \\[ X\\theta = y \\] We are given both \\(X\\) and \\(y\\) . This is nothing but our labeled dataset. We have to find \\(\\theta\\) . This leaves us with two questions: Does the equation \\(X\\theta = y\\) have a solution? If it doesn't have a solution, then how do we estimate \\(\\theta\\) ? We can see how an ML problem has now turned into a linear algebra problem! We will try to answer the first question and then move on to the second question. Note A lot of details about the linear regression model have been skipped. This presentation has tried to bring out the mathematical details. For a more accurate handling of this topic, please refer to week-2 of the MLT course.","title":"Matrix form"},{"location":"week-3/linear_model/#summary","text":"A linear regression model assumes a linear relationship between inputs and outputs. This results in a system of linear equation of the form \\(X\\theta = y\\) .","title":"Summary"},{"location":"week-3/outline/","text":"Outline \u00b6 In this week we will study regression, a supervised learning problem, and a simple yet powerful ML technique called linear regression that tries to solve the problem. Specifically, we will try to answer the following questions: What is a linear regression model? How do we solve for \\(\\theta\\) in the equation \\(X\\theta = 0\\) ? How do we solve for \\(\\theta\\) in the equation \\(X\\theta = y\\) ? How do we solve for \\(\\theta\\) in \\(X\\theta \\approx y\\) ? What does a linear model look like? Geometrically, what is the relationship between the approximation \\(X \\hat{\\theta}\\) and the vector \\(y\\) ? We will be working with a labeled dataset of the form \\((X, y)\\) .","title":"Outline"},{"location":"week-3/outline/#outline","text":"In this week we will study regression, a supervised learning problem, and a simple yet powerful ML technique called linear regression that tries to solve the problem. Specifically, we will try to answer the following questions: What is a linear regression model? How do we solve for \\(\\theta\\) in the equation \\(X\\theta = 0\\) ? How do we solve for \\(\\theta\\) in the equation \\(X\\theta = y\\) ? How do we solve for \\(\\theta\\) in \\(X\\theta \\approx y\\) ? What does a linear model look like? Geometrically, what is the relationship between the approximation \\(X \\hat{\\theta}\\) and the vector \\(y\\) ? We will be working with a labeled dataset of the form \\((X, y)\\) .","title":"Outline"},{"location":"week-3/projections/","text":"Projections \u00b6 Question Geometrically, what is the relationship between the approximation \\(X \\hat{\\theta}\\) and the vector \\(y\\) ? Setting \u00b6 Recall that we are trying to solve the following: $$ X\\theta \\approx y $$ The best approximation is given by the solution to this equation: \\[ (X^TX)\\hat{\\theta} = X^Ty \\] \\(X\\hat{\\theta}\\) is therefore the best approximation for \\(y\\) . Now, how are these two vectors related? Specifically, note that both vectors reside in \\(\\mathbb{R}^{m}\\) . To keep things simple, let us return to our favourite haunt, \\(\\mathbb{R}^2\\) , with the following configuration: \\[ X = \\begin{bmatrix} 2 & 6\\\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\\\ 4 \\end{bmatrix} \\] Back to Column space \u00b6 We look for an approximation only when \\(y\\) does not lie in the column space of \\(X\\) . So, first, we see what the column space is: \\[ C(X) = \\text{span}\\left( \\left\\{ \\begin{bmatrix}2\\\\1\\end{bmatrix} \\right\\} \\right) \\] The second column of \\(X\\) is just three times the first column. The rank of the matrix is \\(1\\) . The column space of \\(X\\) is a one-dimensional subspace of \\(\\mathbb{R}^2\\) . Geometrically, what does this mean? The column space is a line passing through the origin and the point \\((2, 1)\\) . Clearly, the vector \\(y\\) does not lie in the column space of \\(X\\) . So, we are justified in looking for an approximation. Projections: 2-dimensions \u00b6 The key idea to remember is that the approximation is going to lie in the column space of \\(X\\) . What vector in \\(C(X)\\) is closest to to \\(y\\) ? First up, what do we mean by closest? Recall that the distance between the two vectors is our measure of distance. In our 2D case, this is nothing but the distance between the point \\(y\\) and some point on the line \\(C(X)\\) . The point on the line which is going to have the shortest distance is the projection of \\(y\\) onto the line! Why is that the case? Among all line segments from a point to a line, the perpendicular to it is the shortest. Geometric intuition therefore suggests that \\(X\\hat{\\theta} = \\begin{bmatrix}4\\\\2\\end{bmatrix}\\) . Back to Normal Equations \u00b6 Let us see if algebra agrees with geometry: \\[ \\begin{aligned} (X^TX)\\hat{\\theta} &= X^Ty\\\\\\\\ \\begin{bmatrix} 2 & 1\\\\ 6 & 3 \\end{bmatrix} \\begin{bmatrix} 2 & 6\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} \\hat{x}_1\\\\ \\hat{x}_2 \\end{bmatrix} &= \\begin{bmatrix} 2 & 1\\\\ 6 & 3 \\end{bmatrix}\\begin{bmatrix} 3\\\\ 4 \\end{bmatrix}\\\\\\\\ \\begin{bmatrix} 5 & 15\\\\ 15 & 45 \\end{bmatrix} \\begin{bmatrix} \\hat{x}_1\\\\ \\hat{x}_2 \\end{bmatrix} &= \\begin{bmatrix} 10\\\\ 30 \\end{bmatrix}\\\\\\\\ \\begin{bmatrix} 1 & 3\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} \\hat{x}_1\\\\ \\hat{x}_2 \\end{bmatrix} &= \\begin{bmatrix} 2\\\\ 2 \\end{bmatrix}\\\\ \\end{aligned} \\] We see that \\(X^TX\\) is singular. But thankfully, the system is still solvable. One such solution is: \\[ \\hat{\\theta} = \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\] Therefore, the approximation is: \\[ X\\hat{\\theta} = \\begin{bmatrix} 2 & 6\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4\\\\ 2 \\end{bmatrix} \\] Algebra does agree with geometry! Projections: m-dimensions \u00b6 The main takeaway from the 2D case is this: the vector closest to \\(y\\) in the column space of \\(X\\) is its projection onto the column space of \\(X\\) . This can be extended to any higher dimensional space. First, we note that for a projection, the error vector is orthogonal to the column space of \\(X\\) : The error vector \\(e\\) is: $$ e = y - X\\hat{\\theta} $$ This is orthogonal to the column space of \\(X\\) . This is the same as saying that it is orthogonal to each column of \\(X\\) . If we let \\(X\\) be \\([x_1, \\cdots, x_n]\\) , where \\(x_i\\) is the \\(i^{th}\\) column of \\(X\\) . Then for \\(1 \\leq i \\leq n\\) : $$ x_i^T e = 0 $$ This can be neatly expressed as: \\[ X^Te=0 \\] So, the error vector is in the nullspace of \\(X^T\\) ! Replacing \\(e = y - X\\hat{\\theta}\\) , we get: $$ \\begin{aligned} X^T(y - X\\hat{\\theta}) &= 0\\ \\implies (X^TX)\\hat{\\theta} = X^Ty \\end{aligned} $$ The normal equations again! Summary \u00b6 If the parameters of the linear model obtained by solving the normal equations is \\(\\hat{\\theta}\\) , then the vector \\(A\\hat{\\theta}\\) is the projection of the vector \\(y\\) onto the column space of \\(X\\) .","title":"Projections"},{"location":"week-3/projections/#projections","text":"Question Geometrically, what is the relationship between the approximation \\(X \\hat{\\theta}\\) and the vector \\(y\\) ?","title":"Projections"},{"location":"week-3/projections/#setting","text":"Recall that we are trying to solve the following: $$ X\\theta \\approx y $$ The best approximation is given by the solution to this equation: \\[ (X^TX)\\hat{\\theta} = X^Ty \\] \\(X\\hat{\\theta}\\) is therefore the best approximation for \\(y\\) . Now, how are these two vectors related? Specifically, note that both vectors reside in \\(\\mathbb{R}^{m}\\) . To keep things simple, let us return to our favourite haunt, \\(\\mathbb{R}^2\\) , with the following configuration: \\[ X = \\begin{bmatrix} 2 & 6\\\\ 1 & 3 \\end{bmatrix}, y = \\begin{bmatrix} 3\\\\ 4 \\end{bmatrix} \\]","title":"Setting"},{"location":"week-3/projections/#back-to-column-space","text":"We look for an approximation only when \\(y\\) does not lie in the column space of \\(X\\) . So, first, we see what the column space is: \\[ C(X) = \\text{span}\\left( \\left\\{ \\begin{bmatrix}2\\\\1\\end{bmatrix} \\right\\} \\right) \\] The second column of \\(X\\) is just three times the first column. The rank of the matrix is \\(1\\) . The column space of \\(X\\) is a one-dimensional subspace of \\(\\mathbb{R}^2\\) . Geometrically, what does this mean? The column space is a line passing through the origin and the point \\((2, 1)\\) . Clearly, the vector \\(y\\) does not lie in the column space of \\(X\\) . So, we are justified in looking for an approximation.","title":"Back to Column space"},{"location":"week-3/projections/#projections-2-dimensions","text":"The key idea to remember is that the approximation is going to lie in the column space of \\(X\\) . What vector in \\(C(X)\\) is closest to to \\(y\\) ? First up, what do we mean by closest? Recall that the distance between the two vectors is our measure of distance. In our 2D case, this is nothing but the distance between the point \\(y\\) and some point on the line \\(C(X)\\) . The point on the line which is going to have the shortest distance is the projection of \\(y\\) onto the line! Why is that the case? Among all line segments from a point to a line, the perpendicular to it is the shortest. Geometric intuition therefore suggests that \\(X\\hat{\\theta} = \\begin{bmatrix}4\\\\2\\end{bmatrix}\\) .","title":"Projections: 2-dimensions"},{"location":"week-3/projections/#back-to-normal-equations","text":"Let us see if algebra agrees with geometry: \\[ \\begin{aligned} (X^TX)\\hat{\\theta} &= X^Ty\\\\\\\\ \\begin{bmatrix} 2 & 1\\\\ 6 & 3 \\end{bmatrix} \\begin{bmatrix} 2 & 6\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} \\hat{x}_1\\\\ \\hat{x}_2 \\end{bmatrix} &= \\begin{bmatrix} 2 & 1\\\\ 6 & 3 \\end{bmatrix}\\begin{bmatrix} 3\\\\ 4 \\end{bmatrix}\\\\\\\\ \\begin{bmatrix} 5 & 15\\\\ 15 & 45 \\end{bmatrix} \\begin{bmatrix} \\hat{x}_1\\\\ \\hat{x}_2 \\end{bmatrix} &= \\begin{bmatrix} 10\\\\ 30 \\end{bmatrix}\\\\\\\\ \\begin{bmatrix} 1 & 3\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} \\hat{x}_1\\\\ \\hat{x}_2 \\end{bmatrix} &= \\begin{bmatrix} 2\\\\ 2 \\end{bmatrix}\\\\ \\end{aligned} \\] We see that \\(X^TX\\) is singular. But thankfully, the system is still solvable. One such solution is: \\[ \\hat{\\theta} = \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\] Therefore, the approximation is: \\[ X\\hat{\\theta} = \\begin{bmatrix} 2 & 6\\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4\\\\ 2 \\end{bmatrix} \\] Algebra does agree with geometry!","title":"Back to Normal Equations"},{"location":"week-3/projections/#projections-m-dimensions","text":"The main takeaway from the 2D case is this: the vector closest to \\(y\\) in the column space of \\(X\\) is its projection onto the column space of \\(X\\) . This can be extended to any higher dimensional space. First, we note that for a projection, the error vector is orthogonal to the column space of \\(X\\) : The error vector \\(e\\) is: $$ e = y - X\\hat{\\theta} $$ This is orthogonal to the column space of \\(X\\) . This is the same as saying that it is orthogonal to each column of \\(X\\) . If we let \\(X\\) be \\([x_1, \\cdots, x_n]\\) , where \\(x_i\\) is the \\(i^{th}\\) column of \\(X\\) . Then for \\(1 \\leq i \\leq n\\) : $$ x_i^T e = 0 $$ This can be neatly expressed as: \\[ X^Te=0 \\] So, the error vector is in the nullspace of \\(X^T\\) ! Replacing \\(e = y - X\\hat{\\theta}\\) , we get: $$ \\begin{aligned} X^T(y - X\\hat{\\theta}) &= 0\\ \\implies (X^TX)\\hat{\\theta} = X^Ty \\end{aligned} $$ The normal equations again!","title":"Projections: m-dimensions"},{"location":"week-3/projections/#summary","text":"If the parameters of the linear model obtained by solving the normal equations is \\(\\hat{\\theta}\\) , then the vector \\(A\\hat{\\theta}\\) is the projection of the vector \\(y\\) onto the column space of \\(X\\) .","title":"Summary"},{"location":"week-3/system_1/","text":"X\u03b8 = 0 \u00b6 Question How do we solve for \\(\\theta\\) in the equation \\(X\\theta = 0\\) ? Setting \u00b6 Before we tackle the general problem of \\(X\\theta = y\\) , let us first see if we can solve the system when \\(y\\) is the zero vector. In all the discussions that follow, this will be our setting: \\(X\\) is a data-matrix of dimensions \\(m \\times n\\) \\(y\\) is a column-vector of size \\(m\\) \\(\\theta \\in \\mathbb{R}^n\\) \\(X\\theta \\in \\mathbb{R}^{m}\\) The equation that we have taken up is: $$ X\\theta = 0 $$ Note We have to be careful with the use of \\(0\\) . Depending on the context, it could either mean a scalar or a vector. Nullspace \u00b6 We can immediately see that \\(\\theta = 0\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(\\theta_1\\) . Then, we can see that \\(k \\theta_1\\) is also a solution. This is because \\(X (k\\theta_1) = k \\cdot X\\theta_1 = 0\\) . Also, if \\(\\theta_1\\) and \\(\\theta_2\\) are two solutions to the equation, then \\(\\theta_1 + \\theta_2\\) is also a solution, as \\(X(\\theta_1 + \\theta_2) = X\\theta_1 + X\\theta_2 = 0\\) . From these two observations, we see that the set of all solutions to the equation \\(X \\theta = 0\\) is a subspace of \\(\\mathbb{R}^{n}\\) . We denote this by \\(N(X)\\) and we call it the nullspace of \\(X\\) . The dimension of the nullspace is called nullity. All this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B = \\{v_1, \\cdots, v_k\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(N(X) = \\text{span}(B)\\) . To get to the basis, we first need to revisit Gaussian elimination. Row-Echelon form \u00b6 The central idea in Gaussian elimination is to transform a matrix into its row-echelon form. Let us take up an example and work with that: \\[ X = \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Recall that we can apply a sequence of any of these three row operations on a matrix: swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row Step-1 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Step-2 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\] Step-3 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\] The final matrix that we have is in row-echelon form. Here is a quick reminder of what the row-echelon matrix is: All rows that have only zeros are at the bottom. The first nonzero entry in a row is always to the right of the first nonzero entry in the row above it. The first nonzero entry in a row is called a pivot. Let us call the row-echelon matrix of \\(X\\) as \\(P\\) . We state the following result without a proof. Useful result If \\(P\\) is the row-echelon form of \\(X\\) , then \\(X\\theta = 0\\) if and only if \\(P\\theta = 0\\) Thus, the nullspace of a matrix and its row-echelon matrix are the same. This lets us forget about \\(X\\) and deal with its row-echelon form directly. Recipe for a Basis \u00b6 Now we have to solve the following equation: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\] Columns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called \"dependent variables\", while the others are called \"independent variables\". We can now state the algorithm for finding a basis for the nullspace of \\(X\\) : Algorithm \\(B = \\{ \\}\\) For each independent variable \\(\\theta_i\\) : Set \\(\\theta_i = 1\\) and \\(\\theta_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(\\theta\\) to \\(B\\) \\(B\\) is the required basis. Let us try it out here. \\(\\theta_1\\) and \\(\\theta_2\\) are the dependent variables. \\(\\theta_3\\) and \\(\\theta_4\\) are the independent variables. First, let us set \\(\\theta_3 = 1, \\theta_4 = 0\\) . This gives us \\(\\theta_1 = 1, \\theta_2 = -2\\) . The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^T\\) . Next, we set \\(\\theta_3 = 0, \\theta_4 = 1\\) . This gives us \\(\\theta_1 = 0, \\theta_2 = -1\\) . The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^T\\) . Thus, the basis for \\(N(X)\\) is: \\[ B = \\left \\{ \\begin{bmatrix}1\\\\ -2\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\\\ -1\\\\ 0\\\\ 1\\end{bmatrix} \\right \\} \\] The set of all solutions for the equation \\(X\\theta = 0\\) is \\(\\text{span}(B)\\) . Proof \u00b6 If you are wondering why this algorithm works, note that the rank of the matrix \\(r\\) is equal to the number of non-zero rows. From the rank-nullity theorem, we know that the nullity is going to be \\(n - r\\) . Thus the basis of \\(N(X)\\) will have \\(n - r\\) linearly independent vectors. We have to hunt for these \\(n - r\\) vectors. To get there, we divide the \\(n\\) variables into two parts: \\(r\\) pivot variables: these are also called the dependent variables \\(n - r\\) free variables: these are also called the independent variables To get a vector, we set one of the \\(n - r\\) free variables to \\(1\\) and the rest to \\(0\\) . Then, we determine the \\(r\\) pivot variables by solving the first \\(r\\) equations. By repeating this process with each of the \\(n - r\\) free variables, we are guaranteed to have \\(n - r\\) linearly independent vectors. Summary \u00b6 In order to solve the equation \\(X\\theta = 0\\) , we first reduce the matrix \\(X\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the nullspace of \\(X\\) . The span of the basis is the set of all solutions to this equation.","title":"X\u03b8 = 0"},{"location":"week-3/system_1/#x-0","text":"Question How do we solve for \\(\\theta\\) in the equation \\(X\\theta = 0\\) ?","title":"X\u03b8 = 0"},{"location":"week-3/system_1/#setting","text":"Before we tackle the general problem of \\(X\\theta = y\\) , let us first see if we can solve the system when \\(y\\) is the zero vector. In all the discussions that follow, this will be our setting: \\(X\\) is a data-matrix of dimensions \\(m \\times n\\) \\(y\\) is a column-vector of size \\(m\\) \\(\\theta \\in \\mathbb{R}^n\\) \\(X\\theta \\in \\mathbb{R}^{m}\\) The equation that we have taken up is: $$ X\\theta = 0 $$ Note We have to be careful with the use of \\(0\\) . Depending on the context, it could either mean a scalar or a vector.","title":"Setting"},{"location":"week-3/system_1/#nullspace","text":"We can immediately see that \\(\\theta = 0\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(\\theta_1\\) . Then, we can see that \\(k \\theta_1\\) is also a solution. This is because \\(X (k\\theta_1) = k \\cdot X\\theta_1 = 0\\) . Also, if \\(\\theta_1\\) and \\(\\theta_2\\) are two solutions to the equation, then \\(\\theta_1 + \\theta_2\\) is also a solution, as \\(X(\\theta_1 + \\theta_2) = X\\theta_1 + X\\theta_2 = 0\\) . From these two observations, we see that the set of all solutions to the equation \\(X \\theta = 0\\) is a subspace of \\(\\mathbb{R}^{n}\\) . We denote this by \\(N(X)\\) and we call it the nullspace of \\(X\\) . The dimension of the nullspace is called nullity. All this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B = \\{v_1, \\cdots, v_k\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(N(X) = \\text{span}(B)\\) . To get to the basis, we first need to revisit Gaussian elimination.","title":"Nullspace"},{"location":"week-3/system_1/#row-echelon-form","text":"The central idea in Gaussian elimination is to transform a matrix into its row-echelon form. Let us take up an example and work with that: \\[ X = \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Recall that we can apply a sequence of any of these three row operations on a matrix: swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row Step-1 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Step-2 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\] Step-3 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\] The final matrix that we have is in row-echelon form. Here is a quick reminder of what the row-echelon matrix is: All rows that have only zeros are at the bottom. The first nonzero entry in a row is always to the right of the first nonzero entry in the row above it. The first nonzero entry in a row is called a pivot. Let us call the row-echelon matrix of \\(X\\) as \\(P\\) . We state the following result without a proof. Useful result If \\(P\\) is the row-echelon form of \\(X\\) , then \\(X\\theta = 0\\) if and only if \\(P\\theta = 0\\) Thus, the nullspace of a matrix and its row-echelon matrix are the same. This lets us forget about \\(X\\) and deal with its row-echelon form directly.","title":"Row-Echelon form"},{"location":"week-3/system_1/#recipe-for-a-basis","text":"Now we have to solve the following equation: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\] Columns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called \"dependent variables\", while the others are called \"independent variables\". We can now state the algorithm for finding a basis for the nullspace of \\(X\\) : Algorithm \\(B = \\{ \\}\\) For each independent variable \\(\\theta_i\\) : Set \\(\\theta_i = 1\\) and \\(\\theta_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(\\theta\\) to \\(B\\) \\(B\\) is the required basis. Let us try it out here. \\(\\theta_1\\) and \\(\\theta_2\\) are the dependent variables. \\(\\theta_3\\) and \\(\\theta_4\\) are the independent variables. First, let us set \\(\\theta_3 = 1, \\theta_4 = 0\\) . This gives us \\(\\theta_1 = 1, \\theta_2 = -2\\) . The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^T\\) . Next, we set \\(\\theta_3 = 0, \\theta_4 = 1\\) . This gives us \\(\\theta_1 = 0, \\theta_2 = -1\\) . The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^T\\) . Thus, the basis for \\(N(X)\\) is: \\[ B = \\left \\{ \\begin{bmatrix}1\\\\ -2\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\\\ -1\\\\ 0\\\\ 1\\end{bmatrix} \\right \\} \\] The set of all solutions for the equation \\(X\\theta = 0\\) is \\(\\text{span}(B)\\) .","title":"Recipe for a Basis"},{"location":"week-3/system_1/#proof","text":"If you are wondering why this algorithm works, note that the rank of the matrix \\(r\\) is equal to the number of non-zero rows. From the rank-nullity theorem, we know that the nullity is going to be \\(n - r\\) . Thus the basis of \\(N(X)\\) will have \\(n - r\\) linearly independent vectors. We have to hunt for these \\(n - r\\) vectors. To get there, we divide the \\(n\\) variables into two parts: \\(r\\) pivot variables: these are also called the dependent variables \\(n - r\\) free variables: these are also called the independent variables To get a vector, we set one of the \\(n - r\\) free variables to \\(1\\) and the rest to \\(0\\) . Then, we determine the \\(r\\) pivot variables by solving the first \\(r\\) equations. By repeating this process with each of the \\(n - r\\) free variables, we are guaranteed to have \\(n - r\\) linearly independent vectors.","title":"Proof"},{"location":"week-3/system_1/#summary","text":"In order to solve the equation \\(X\\theta = 0\\) , we first reduce the matrix \\(X\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the nullspace of \\(X\\) . The span of the basis is the set of all solutions to this equation.","title":"Summary"},{"location":"week-3/system_2/","text":"X\u03b8 = y \u00b6 Question How do we solve for \\(\\theta\\) in the equation \\(X\\theta = y\\) ? Column space \u00b6 Now we come to the general form of the equation, \\(X\\theta = y\\) . First, we need to know if the equation admits any solution at all. For this, we have to take a closer look at this equation and see what it means: \\[ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\\\ x_1 & \\cdots & x_n\\\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\vdots\\\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_m \\end{bmatrix} \\] Here, \\(x_1, \\cdots, x_n\\) are the columns of \\(X\\) . Recall that the product of a matrix and a vector can be interpreted as a linear combination of the columns of the matrix: \\[ \\theta_1 x_1 + \\cdots + \\theta_n x_n = y \\] \\(X \\theta = y\\) has a solution if and only if \\(y\\) can be expressed as a linear combination of the columns of \\(X\\) . Since the set of all linear combination of \\(X\\) is given by the \\(\\text{span}(\\{x_1, \\cdots, x_n\\})\\) , the equation is solvable if and only if \\(y \\in \\text{span}(\\{x_1, \\cdots, x_n\\})\\) . The span of the columns of \\(X\\) is a subspace and is called the column space of the matrix \\(X\\) . Thus, \\(C(X)=\\text{span}(\\{x_1, \\cdots, x_n\\})\\) . We have now answered the question of when \\(X\\theta = y\\) is solvable. Conditions for Solution \u00b6 Let us reuse the example from the previous unit: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3 \\end{bmatrix} \\] We are back to the row-echelon form, but with the augmented matrix: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\big\\vert & y_1\\\\ 2 & 1 & 0 & 1 & \\big\\vert & y_2 \\\\ 3 & 1 & -1 & 1 & \\big\\vert & y_3 \\end{bmatrix} \\] If we apply the same sequence of row operations as in the previous case, we get: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\vert & y_1\\\\ 0 & 1 & 2 & 1 & \\vert & y_2 - 2y_1 \\\\ 0 & 0 & 0 & 0 & \\vert & y_3 - y_1 - y_2 \\end{bmatrix} \\] We can immediately see that the system has a solution if and only if the following condition is met: \\[ y_3 - y_1 - y_2 = 0 \\] Now that we have the row-echelon matrix, let us rephrase the equation as follows: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} y_1\\\\ y_2 - 2y_1\\\\ y_3 - y_1 - y_2 \\end{bmatrix} \\] Let us call this system \\(P\\theta = z\\) . Note \\(\\theta\\) is a solution to \\(X\\theta = y\\) if and only if \\(\\theta\\) is a solution to \\(P\\theta = z\\) , where \\(P\\ |\\ z\\) is the augmented matrix after Gaussian elimination. General Solution \u00b6 If a solution exists, how do we find it? And what about all possible solutions? First note that the set of pivot columns are linearly independent and form a basis for the column space of \\(P\\) . In the example we are working with, this is quite clear: \\[ C(P) = \\text{span}\\left( \\left\\{\\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\right\\} \\right) \\] So, \\(y\\) can be uniquely expressed as a linear combination of the columns of this basis. Let us call this particular solution \\(\\theta_p\\) . If \\(\\theta_n\\) is some vector in the nullspace of \\(X\\) , then every general solution \\(\\theta_g\\) to the equation can be expressed in this form: $$ \\theta_g = \\theta_p + \\theta_n $$ To see why this might be true, just pre-multiply both sides by \\(X\\) : \\[ X\\theta_g = X(\\theta_p + \\theta_n) = X\\theta_p + X\\theta_n = y \\] This not a complete proof. We have only shown that \\(\\theta_p + \\theta_n\\) is a solution. It may still not be clear why every solution should be of this form. But we will skip this argument in this course. Coming back to the example we are working with, how do we find the particular solution \\(\\theta_p\\) ? We set all independent variables to zero and solve for the dependent variables: \\[ \\theta_p = \\begin{bmatrix} y_1\\\\ y_2 - 2y_1\\\\ 0\\\\ 0 \\end{bmatrix} \\] We already know how to get \\(\\theta_n\\) . Refer to the previous unit on computing a basis for the nullspace of the matrix. Summary \u00b6 \\(X\\theta = y\\) is solvable if and only if \\(y\\) is an element of the column space of \\(X\\) . If this is true, then the general solution to the equation can be expressed as \\(\\theta_p + \\theta_n\\) , where \\(\\theta_p\\) is a particular solution and \\(\\theta_n\\) is some vector in the nullspace of \\(X\\) .","title":"X\u03b8 = y"},{"location":"week-3/system_2/#x-y","text":"Question How do we solve for \\(\\theta\\) in the equation \\(X\\theta = y\\) ?","title":"X\u03b8 = y"},{"location":"week-3/system_2/#column-space","text":"Now we come to the general form of the equation, \\(X\\theta = y\\) . First, we need to know if the equation admits any solution at all. For this, we have to take a closer look at this equation and see what it means: \\[ \\begin{bmatrix} \\big\\vert & & \\big\\vert\\\\ x_1 & \\cdots & x_n\\\\ \\big\\vert & & \\big\\vert \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\vdots\\\\ \\theta_n \\end{bmatrix} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_m \\end{bmatrix} \\] Here, \\(x_1, \\cdots, x_n\\) are the columns of \\(X\\) . Recall that the product of a matrix and a vector can be interpreted as a linear combination of the columns of the matrix: \\[ \\theta_1 x_1 + \\cdots + \\theta_n x_n = y \\] \\(X \\theta = y\\) has a solution if and only if \\(y\\) can be expressed as a linear combination of the columns of \\(X\\) . Since the set of all linear combination of \\(X\\) is given by the \\(\\text{span}(\\{x_1, \\cdots, x_n\\})\\) , the equation is solvable if and only if \\(y \\in \\text{span}(\\{x_1, \\cdots, x_n\\})\\) . The span of the columns of \\(X\\) is a subspace and is called the column space of the matrix \\(X\\) . Thus, \\(C(X)=\\text{span}(\\{x_1, \\cdots, x_n\\})\\) . We have now answered the question of when \\(X\\theta = y\\) is solvable.","title":"Column space"},{"location":"week-3/system_2/#conditions-for-solution","text":"Let us reuse the example from the previous unit: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} y_1\\\\ y_2\\\\ y_3 \\end{bmatrix} \\] We are back to the row-echelon form, but with the augmented matrix: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\big\\vert & y_1\\\\ 2 & 1 & 0 & 1 & \\big\\vert & y_2 \\\\ 3 & 1 & -1 & 1 & \\big\\vert & y_3 \\end{bmatrix} \\] If we apply the same sequence of row operations as in the previous case, we get: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\vert & y_1\\\\ 0 & 1 & 2 & 1 & \\vert & y_2 - 2y_1 \\\\ 0 & 0 & 0 & 0 & \\vert & y_3 - y_1 - y_2 \\end{bmatrix} \\] We can immediately see that the system has a solution if and only if the following condition is met: \\[ y_3 - y_1 - y_2 = 0 \\] Now that we have the row-echelon matrix, let us rephrase the equation as follows: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} \\theta_1\\\\ \\theta_2\\\\ \\theta_3\\\\ \\theta_4 \\end{bmatrix} = \\begin{bmatrix} y_1\\\\ y_2 - 2y_1\\\\ y_3 - y_1 - y_2 \\end{bmatrix} \\] Let us call this system \\(P\\theta = z\\) . Note \\(\\theta\\) is a solution to \\(X\\theta = y\\) if and only if \\(\\theta\\) is a solution to \\(P\\theta = z\\) , where \\(P\\ |\\ z\\) is the augmented matrix after Gaussian elimination.","title":"Conditions for Solution"},{"location":"week-3/system_2/#general-solution","text":"If a solution exists, how do we find it? And what about all possible solutions? First note that the set of pivot columns are linearly independent and form a basis for the column space of \\(P\\) . In the example we are working with, this is quite clear: \\[ C(P) = \\text{span}\\left( \\left\\{\\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\right\\} \\right) \\] So, \\(y\\) can be uniquely expressed as a linear combination of the columns of this basis. Let us call this particular solution \\(\\theta_p\\) . If \\(\\theta_n\\) is some vector in the nullspace of \\(X\\) , then every general solution \\(\\theta_g\\) to the equation can be expressed in this form: $$ \\theta_g = \\theta_p + \\theta_n $$ To see why this might be true, just pre-multiply both sides by \\(X\\) : \\[ X\\theta_g = X(\\theta_p + \\theta_n) = X\\theta_p + X\\theta_n = y \\] This not a complete proof. We have only shown that \\(\\theta_p + \\theta_n\\) is a solution. It may still not be clear why every solution should be of this form. But we will skip this argument in this course. Coming back to the example we are working with, how do we find the particular solution \\(\\theta_p\\) ? We set all independent variables to zero and solve for the dependent variables: \\[ \\theta_p = \\begin{bmatrix} y_1\\\\ y_2 - 2y_1\\\\ 0\\\\ 0 \\end{bmatrix} \\] We already know how to get \\(\\theta_n\\) . Refer to the previous unit on computing a basis for the nullspace of the matrix.","title":"General Solution"},{"location":"week-3/system_2/#summary","text":"\\(X\\theta = y\\) is solvable if and only if \\(y\\) is an element of the column space of \\(X\\) . If this is true, then the general solution to the equation can be expressed as \\(\\theta_p + \\theta_n\\) , where \\(\\theta_p\\) is a particular solution and \\(\\theta_n\\) is some vector in the nullspace of \\(X\\) .","title":"Summary"},{"location":"week-4/char_polynomial/","text":"Characteristic Polynomial \u00b6 Question How do we find the eigenvalues of a matrix? Necessary condition \u00b6 If \\(x\\) is an eigenvector of a matrix \\(A\\) with eigenvalue \\(\\lambda\\) , then we have: \\[ Ax = \\lambda x \\] Let us rearrange this a bit. First, observe that if \\(I\\) is the identity matrix, then: \\[ Ix = x \\] Using this fact, we can express the eigenvalue equation as: \\[ \\begin{aligned} Ax &= \\lambda Ix\\\\\\\\ Ax - \\lambda I x &= 0\\\\\\\\ (A - \\lambda I)x &= 0 \\end{aligned} \\] If \\((\\lambda, x)\\) is an eigenpair, then \\((A - \\lambda I) x = 0\\) . Another way of expressing this is: Necessary condition \\((A - \\lambda I)x = 0\\) is a necessary condition for \\(x\\) to be an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) . Sufficient condition \u00b6 Let us try the other direction. What if \\((A - \\lambda I)x = 0\\) ? We have: \\[ \\begin{aligned} (A - \\lambda I)x &= 0\\\\\\\\ Ax - \\lambda Ix &= 0\\\\\\\\ Ax &= \\lambda x \\end{aligned} \\] Thus we see that \\(x\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) . Sufficient condition \\((A - \\lambda I)x = 0\\) is a sufficient condition for \\(x\\) to be an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) . Nullspace argument \u00b6 Thus we see that \\((\\lambda, x)\\) is an eigenpair of \\(A\\) if and only if \\((A - \\lambda I)x = 0\\) . From this, we deduce that \\(x\\) has to be in the nullspace of \\(A - \\lambda I\\) . Thus the nullspace of \\(A - \\lambda I\\) is non-trivial. If the nullspace is non-trivial, we can conclude that the matrix is not invertible. To see why this is true, think about the linear transformation corresponding to this matrix. If the nullspace is non-trivial, then the transformation is not one-one. Therefore, it doesn't have an inverse. If the transformation doesn't have an inverse, the corresponding matrix is not invertible. If the matrix is not invertible, then its determinant is zero. So, we have the following result: Important \\(\\lambda\\) is an eigenvalue of \\(A\\) if and only if \\(|A - \\lambda I| = 0\\) Notice how we end up with a result that doesn't involve the eigenvector and only the matrix and its eigenvalue. Note The eigenspace corresponding to an eigenvalue \\(\\lambda\\) is the nullspace of \\(A - \\lambda I\\) . The Polynomial \u00b6 At this stage, let us take up an example: \\[ A = \\begin{bmatrix} 3 & 1\\\\ 0 & 2 \\end{bmatrix} \\] For \\(\\lambda\\) to be an eigenvalue of the matrix \\(A\\) , \\(|A - \\lambda I| = 0\\) . This translates to: \\[ \\begin{vmatrix} 3 - \\lambda & 1\\\\ 0 & 2 - \\lambda \\end{vmatrix} = 0 \\] Expanding the determinant, we get: \\[ \\begin{aligned} (3 - \\lambda)(2 - \\lambda) = 0 \\end{aligned} \\] The expression on the LHS is a polynomial of degree \\(2\\) and is called the characteristic polynomial of \\(A\\) . We see that \\(\\lambda = 2, 3\\) are the eigenvalues of the matrix \\(A\\) . Properties \u00b6 In general, for an \\(n \\times n\\) matrix \\(A\\) , the characteristic polynomial has degree \\(n\\) . The roots of the characteristic polynomial are the eigenvalues of the matrix \\(A\\) . There are two properties of the polynomial that are worth knowing: the sum of the eigenvalues of the matrix is equal to its trace the product of the eigenvalues of the matrix is equal to its determinant The second property is easy to see. Assume that the polynomial splits into \\(n\\) factors corresponding to \\(n\\) eigenvalues \\(\\lambda_1, \\cdots, \\lambda_n\\) , then: \\[ |A - \\lambda I| = (\\lambda - \\lambda_1) \\cdots (\\lambda - \\lambda_n) \\] If we set \\(\\lambda = 0\\) , then: \\[ |A| = \\lambda_1 \\cdots \\lambda_n \\] Summary \u00b6 To find the eigenvalues of a matrix \\(A\\) , we first compute the characteristic polynomial \\(|A - \\lambda I|\\) . The roots of this polynomial are the eigenvalues of the matrix \\(A\\) . There are two important properties: sum of the eigenvalues is equal to the trace of the matrix product of the eigenvalues is equal to the determinant of the matrix","title":"Characteristic Polynomial"},{"location":"week-4/char_polynomial/#characteristic-polynomial","text":"Question How do we find the eigenvalues of a matrix?","title":"Characteristic Polynomial"},{"location":"week-4/char_polynomial/#necessary-condition","text":"If \\(x\\) is an eigenvector of a matrix \\(A\\) with eigenvalue \\(\\lambda\\) , then we have: \\[ Ax = \\lambda x \\] Let us rearrange this a bit. First, observe that if \\(I\\) is the identity matrix, then: \\[ Ix = x \\] Using this fact, we can express the eigenvalue equation as: \\[ \\begin{aligned} Ax &= \\lambda Ix\\\\\\\\ Ax - \\lambda I x &= 0\\\\\\\\ (A - \\lambda I)x &= 0 \\end{aligned} \\] If \\((\\lambda, x)\\) is an eigenpair, then \\((A - \\lambda I) x = 0\\) . Another way of expressing this is: Necessary condition \\((A - \\lambda I)x = 0\\) is a necessary condition for \\(x\\) to be an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) .","title":"Necessary condition"},{"location":"week-4/char_polynomial/#sufficient-condition","text":"Let us try the other direction. What if \\((A - \\lambda I)x = 0\\) ? We have: \\[ \\begin{aligned} (A - \\lambda I)x &= 0\\\\\\\\ Ax - \\lambda Ix &= 0\\\\\\\\ Ax &= \\lambda x \\end{aligned} \\] Thus we see that \\(x\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) . Sufficient condition \\((A - \\lambda I)x = 0\\) is a sufficient condition for \\(x\\) to be an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) .","title":"Sufficient condition"},{"location":"week-4/char_polynomial/#nullspace-argument","text":"Thus we see that \\((\\lambda, x)\\) is an eigenpair of \\(A\\) if and only if \\((A - \\lambda I)x = 0\\) . From this, we deduce that \\(x\\) has to be in the nullspace of \\(A - \\lambda I\\) . Thus the nullspace of \\(A - \\lambda I\\) is non-trivial. If the nullspace is non-trivial, we can conclude that the matrix is not invertible. To see why this is true, think about the linear transformation corresponding to this matrix. If the nullspace is non-trivial, then the transformation is not one-one. Therefore, it doesn't have an inverse. If the transformation doesn't have an inverse, the corresponding matrix is not invertible. If the matrix is not invertible, then its determinant is zero. So, we have the following result: Important \\(\\lambda\\) is an eigenvalue of \\(A\\) if and only if \\(|A - \\lambda I| = 0\\) Notice how we end up with a result that doesn't involve the eigenvector and only the matrix and its eigenvalue. Note The eigenspace corresponding to an eigenvalue \\(\\lambda\\) is the nullspace of \\(A - \\lambda I\\) .","title":"Nullspace argument"},{"location":"week-4/char_polynomial/#the-polynomial","text":"At this stage, let us take up an example: \\[ A = \\begin{bmatrix} 3 & 1\\\\ 0 & 2 \\end{bmatrix} \\] For \\(\\lambda\\) to be an eigenvalue of the matrix \\(A\\) , \\(|A - \\lambda I| = 0\\) . This translates to: \\[ \\begin{vmatrix} 3 - \\lambda & 1\\\\ 0 & 2 - \\lambda \\end{vmatrix} = 0 \\] Expanding the determinant, we get: \\[ \\begin{aligned} (3 - \\lambda)(2 - \\lambda) = 0 \\end{aligned} \\] The expression on the LHS is a polynomial of degree \\(2\\) and is called the characteristic polynomial of \\(A\\) . We see that \\(\\lambda = 2, 3\\) are the eigenvalues of the matrix \\(A\\) .","title":"The Polynomial"},{"location":"week-4/char_polynomial/#properties","text":"In general, for an \\(n \\times n\\) matrix \\(A\\) , the characteristic polynomial has degree \\(n\\) . The roots of the characteristic polynomial are the eigenvalues of the matrix \\(A\\) . There are two properties of the polynomial that are worth knowing: the sum of the eigenvalues of the matrix is equal to its trace the product of the eigenvalues of the matrix is equal to its determinant The second property is easy to see. Assume that the polynomial splits into \\(n\\) factors corresponding to \\(n\\) eigenvalues \\(\\lambda_1, \\cdots, \\lambda_n\\) , then: \\[ |A - \\lambda I| = (\\lambda - \\lambda_1) \\cdots (\\lambda - \\lambda_n) \\] If we set \\(\\lambda = 0\\) , then: \\[ |A| = \\lambda_1 \\cdots \\lambda_n \\]","title":"Properties"},{"location":"week-4/char_polynomial/#summary","text":"To find the eigenvalues of a matrix \\(A\\) , we first compute the characteristic polynomial \\(|A - \\lambda I|\\) . The roots of this polynomial are the eigenvalues of the matrix \\(A\\) . There are two important properties: sum of the eigenvalues is equal to the trace of the matrix product of the eigenvalues is equal to the determinant of the matrix","title":"Summary"},{"location":"week-4/computation/","text":"Computing Eigenvectors \u00b6 Question How do we find the eigenvectors of a matrix? Example \u00b6 Once we have the eigenvalues of a matrix, the next logical step is to find the eigenvectors for each of these eigenvalues. Let us continue with the example that we have been working with so far: \\[ A = \\begin{bmatrix} 3 & 1\\\\ 0 & 2 \\end{bmatrix} \\] We already know that the eigenvalues are \\(2\\) and \\(3\\) . What are the eigenvectors corresponding to \\(\\lambda = 2\\) ? First, we note that if \\(x\\) is an eigenvector corresponding to \\(\\lambda = 2\\) , then: \\[ \\begin{aligned} (A - 2I) x &= 0\\\\\\\\ \\begin{bmatrix} 1 & 1\\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} &= \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix} \\end{aligned} \\] From this, we have \\(x_1 + x_2 = 0\\) . \\(x_2\\) is an independent variabe and \\(x_1\\) is a dependent variable. The rank of this matrix is \\(1\\) , so every eigenvector is of the form: \\[ k\\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\] Therefore, the eigenspace corresponding to the eigenvalue \\(\\lambda = 2\\) is given by: \\[ \\text{span}\\left( \\left\\{ \\begin{bmatrix}-1\\\\1\\end{bmatrix} \\right\\} \\right) \\] If we repeat the same process for \\(\\lambda = 3\\) , the eigenspace turns out to be: \\[ \\text{span}\\left( \\left\\{ \\begin{bmatrix}1\\\\0\\end{bmatrix} \\right\\} \\right) \\] What have we done so far? We have just computed the nullspace of \\(A - \\lambda I\\) . So, this is the recipe to follow: compute a basis for the nullspace of the matrix \\(A - \\lambda I\\) . The span of this basis (exculding the zero vector) is the set of all eigenvectors of the matrix \\(A\\) corresponding to the eigenvalue \\(\\lambda\\) . We have already discussed an algorithm to compute the basis for the nullspace of a matrix. Note For any matrix \\(A\\) , the nullspace of \\(A\\) (excluding the vector \\(0\\) ) has all the eigenvectors of \\(A\\) with eigenvalue \\(0\\) . This is because \\(Ax = 0 = 0x\\) if \\(x \\in N(A)\\) . Special examples \u00b6 Let us take up some special examples and see how the eigenvectors and eigenvalues look. Diagonal matrix \u00b6 If \\(D\\) is a diagonal matrix, then the eigenvalues are the diagonal entries of the matrix. We can express it as: $$ D = \\text{diag}(\\lambda_1, \\cdots, \\lambda_n) $$ The characteristic polynomial is \\((\\lambda - \\lambda_1) \\cdots (\\lambda - \\lambda_n)\\) . The eigenvectors of \\(D\\) are going to be elements of the standard basis \\(\\{e_1, \\cdots, e_n\\}\\) . Diagonal matrices are the most interesting when it comes to the study of eigenvalues and eigenvectors because of the ease with which we can read off the eigenvalues. Identity matrix \u00b6 If \\(I\\) is an identity matrix of size \\(n \\times n\\) , then every vector in \\(\\mathbb{R}^{n}\\) is an eigenvector with eigenvalue \\(1\\) . The characteristic polynomial for the identity matrix is of the form \\((\\lambda - 1)^n\\) . There is exactly one eigenvalue that is repeated \\(n\\) times. This is en extreme case of the diagonal matrix. Projection matrix \u00b6 If \\(P\\) is a projection matrix corresponding to a matrix \\(A\\) , then every vector that is in the column space of \\(A\\) is an eigenvector with eigenvalue \\(1\\) . To see why this is true, note that \\(Px = x\\) for any vector in the column space of \\(A\\) . Also, every vector that is orthogonal to the column space of \\(A\\) will be an eigenvector of \\(P\\) with eigenvalue \\(0\\) . This is because \\(Px = 0\\) for any vector in this space.","title":"Computing Eigenvectors"},{"location":"week-4/computation/#computing-eigenvectors","text":"Question How do we find the eigenvectors of a matrix?","title":"Computing Eigenvectors"},{"location":"week-4/computation/#example","text":"Once we have the eigenvalues of a matrix, the next logical step is to find the eigenvectors for each of these eigenvalues. Let us continue with the example that we have been working with so far: \\[ A = \\begin{bmatrix} 3 & 1\\\\ 0 & 2 \\end{bmatrix} \\] We already know that the eigenvalues are \\(2\\) and \\(3\\) . What are the eigenvectors corresponding to \\(\\lambda = 2\\) ? First, we note that if \\(x\\) is an eigenvector corresponding to \\(\\lambda = 2\\) , then: \\[ \\begin{aligned} (A - 2I) x &= 0\\\\\\\\ \\begin{bmatrix} 1 & 1\\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} &= \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix} \\end{aligned} \\] From this, we have \\(x_1 + x_2 = 0\\) . \\(x_2\\) is an independent variabe and \\(x_1\\) is a dependent variable. The rank of this matrix is \\(1\\) , so every eigenvector is of the form: \\[ k\\begin{bmatrix} -1\\\\ 1 \\end{bmatrix} \\] Therefore, the eigenspace corresponding to the eigenvalue \\(\\lambda = 2\\) is given by: \\[ \\text{span}\\left( \\left\\{ \\begin{bmatrix}-1\\\\1\\end{bmatrix} \\right\\} \\right) \\] If we repeat the same process for \\(\\lambda = 3\\) , the eigenspace turns out to be: \\[ \\text{span}\\left( \\left\\{ \\begin{bmatrix}1\\\\0\\end{bmatrix} \\right\\} \\right) \\] What have we done so far? We have just computed the nullspace of \\(A - \\lambda I\\) . So, this is the recipe to follow: compute a basis for the nullspace of the matrix \\(A - \\lambda I\\) . The span of this basis (exculding the zero vector) is the set of all eigenvectors of the matrix \\(A\\) corresponding to the eigenvalue \\(\\lambda\\) . We have already discussed an algorithm to compute the basis for the nullspace of a matrix. Note For any matrix \\(A\\) , the nullspace of \\(A\\) (excluding the vector \\(0\\) ) has all the eigenvectors of \\(A\\) with eigenvalue \\(0\\) . This is because \\(Ax = 0 = 0x\\) if \\(x \\in N(A)\\) .","title":"Example"},{"location":"week-4/computation/#special-examples","text":"Let us take up some special examples and see how the eigenvectors and eigenvalues look.","title":"Special examples"},{"location":"week-4/computation/#diagonal-matrix","text":"If \\(D\\) is a diagonal matrix, then the eigenvalues are the diagonal entries of the matrix. We can express it as: $$ D = \\text{diag}(\\lambda_1, \\cdots, \\lambda_n) $$ The characteristic polynomial is \\((\\lambda - \\lambda_1) \\cdots (\\lambda - \\lambda_n)\\) . The eigenvectors of \\(D\\) are going to be elements of the standard basis \\(\\{e_1, \\cdots, e_n\\}\\) . Diagonal matrices are the most interesting when it comes to the study of eigenvalues and eigenvectors because of the ease with which we can read off the eigenvalues.","title":"Diagonal matrix"},{"location":"week-4/computation/#identity-matrix","text":"If \\(I\\) is an identity matrix of size \\(n \\times n\\) , then every vector in \\(\\mathbb{R}^{n}\\) is an eigenvector with eigenvalue \\(1\\) . The characteristic polynomial for the identity matrix is of the form \\((\\lambda - 1)^n\\) . There is exactly one eigenvalue that is repeated \\(n\\) times. This is en extreme case of the diagonal matrix.","title":"Identity matrix"},{"location":"week-4/computation/#projection-matrix","text":"If \\(P\\) is a projection matrix corresponding to a matrix \\(A\\) , then every vector that is in the column space of \\(A\\) is an eigenvector with eigenvalue \\(1\\) . To see why this is true, note that \\(Px = x\\) for any vector in the column space of \\(A\\) . Also, every vector that is orthogonal to the column space of \\(A\\) will be an eigenvector of \\(P\\) with eigenvalue \\(0\\) . This is because \\(Px = 0\\) for any vector in this space.","title":"Projection matrix"},{"location":"week-4/diagonalization/","text":"Diagonalization \u00b6 Question What is diagonalization? Diagonal matrices \u00b6 Recall that diagonal matrices are quite interesting to study from the point of view of eigenvectors and eigenvalues. So let us first explore some properties of the same. Let \\(D\\) be an arbitrary diagonal matrix: \\[ D = \\begin{bmatrix} a_1 & \\\\ & \\ddots &\\\\ & & a_n \\end{bmatrix} = \\text{diag}(a_1, \\cdots, a_n) \\] Determinant \u00b6 The determinant of a diagonal matrix is given by the product of the values along the diagonal: \\[ |D| = a_1 \\cdots a_n \\] Invertibility \u00b6 From the above equation, it follows that a diagonal matrix is invertible if and only if none of the diagonal elements are zero. Matrix power \u00b6 Consider the product \\(D^k\\) . This is extremely simple in the case of a diagonal matrix: $$ D^k = \\text{diag}(a_1^k, \\cdots, a_n^k) $$ We won't prove this statement now. But this is quite easy to see. Eigenvalues \u00b6 This is something that we have already seen. The eigenvalues of a diagonal matrix are the diagonal elements. For the next two properties, let us express the diagonal matrix in terms of its eigenvalues: $$ D = \\text{diag}(\\lambda_1, \\cdots, \\lambda_n) $$ A diagonal matrix has \\(n\\) eigenvalues, not necessarily distinct. The characteristic polynomial corresponding to it is: \\[ (\\lambda - \\lambda_1) \\cdots (\\lambda - \\lambda_n) \\] Eigenvectors \u00b6 It is easy to see that the elements of the standard basis \\(\\beta = \\{e_1, \\cdots, e_n\\}\\) are the eigenvectors of \\(D\\) . Specifically, \\(e_i\\) is an eigenvector of \\(D\\) corresponding to the eigenvalue \\(\\lambda_i\\) as \\(De_i = \\lambda_i e_i\\) . Can we characterize every possible eigenvector corresponding to eigenvalue \\(\\lambda_i\\) ? For this particular case, let us assume that the eigenvalues are all distinct. Now, let \\(v\\) be some eigenvector of \\(D\\) corresponding to \\(\\lambda_i\\) : \\[ \\begin{bmatrix} \\lambda_1 & &\\\\ & \\ddots & &\\\\ & & \\lambda_n \\end{bmatrix} \\begin{bmatrix} v_1\\\\ \\vdots\\\\ v_n \\end{bmatrix} = \\begin{bmatrix} \\lambda_i v_1\\\\ \\vdots\\\\ \\lambda_iv_n \\end{bmatrix} \\] This can be further simplified as: \\[ \\begin{bmatrix} (\\lambda_1 - \\lambda_i) v_1\\\\ \\vdots\\\\ (\\lambda_n - \\lambda_i) v_n \\end{bmatrix} = \\begin{bmatrix} 0\\\\ \\vdots\\\\ 0 \\end{bmatrix} \\] Since, \\(\\lambda_i \\neq \\lambda_j\\) for \\(i \\neq j\\) , we have, \\(v_j = 0\\) for \\(j \\neq i\\) . Thus, the eigenvector corresponding to \\(\\lambda_i\\) is some non-zero vector in \\(\\text{span}(\\{e_i\\})\\) . Diagonalizability \u00b6 Now that we have seen some interesting properties concerning diagonal matrices, we shall study a special kind of matrices called diagonalizable matrices . These matrices are not necessarily diagonal, yet share a close kinship with them. Even if a matrix is not diagonal, can we hope that it is related in some way to a diagonal matrix? In the last section, we noted that the vectors in the standard basis are eigenvectors of a diagonal matrix. We will generalize this property. Assume that an \\(n \\times n\\) matrix has a set of \\(n\\) linearly independent eigenvectors: \\(\\beta = \\{v_1, \\cdots, v_n\\}\\) . Then \\(\\beta\\) is a basis for \\(\\mathbb{R}^n\\) . From this, we have a definition for diagonalizable matrices: Definition-1 An \\(n \\times n\\) matrix \\(A\\) is diagonalizable if there is a basis of eigenvectors for \\(\\mathbb{R}^n\\) . Taking off from this definition, for \\(1 \\leq i \\leq n\\) , we have: \\[ Av_i = \\lambda_i v_i \\] We can rearrange all these \\(n\\) equations as follows: \\[ A \\begin{bmatrix} \\vert & & \\vert\\\\ v_1 & \\cdots & v_n\\\\ \\vert & & \\vert \\end{bmatrix} = \\begin{bmatrix} \\vert & & \\vert\\\\ v_1 & \\cdots & v_n\\\\ \\vert & & \\vert \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & &\\\\ & \\ddots &\\\\ & & \\lambda_n \\end{bmatrix} \\] Let \\(Q\\) be the matrix of eigenvectors and \\(D\\) be the diagonal matrix of eigenvalues: \\[ Q = \\begin{bmatrix} \\vert & & \\vert\\\\ v_1 & \\cdots & v_n\\\\ \\vert & & \\vert \\end{bmatrix}, D = \\begin{bmatrix} \\lambda_1 & & \\\\ & \\ddots & \\\\ & & \\lambda_n \\end{bmatrix} \\] We can now express the equation as: \\[ A Q = Q D \\] \\(Q\\) is invertible as the columns are linearly independent. With this, we can now rewrite the equation as: \\[ A = QDQ^{-1} \\] We have managed to establish a bridge from the matrix \\(A\\) to a diagonal matrix \\(D\\) . Matrices \\(A\\) and \\(D\\) are similar. Similar matrices Two \\(n \\times n\\) matrices \\(A\\) and \\(B\\) are similar if there exists an invertible matrix \\(Q\\) such that \\(A = QBQ^{-1}\\) . This gives an alternative definition for diagonalizable matrices: Definition-2 A matrix \\(A\\) is diagonalizable if it is similar to a diagonal matrix. Examples \u00b6 All diagonal matrices are diagonalizable. These are trivial examples of diagonalizable matrices. Let us take up a slightly non-trivial example of a diaongalizable matrix: \\[ A = \\begin{bmatrix} 4 & 3\\\\ 1 & 2 \\end{bmatrix} \\] The eigenvalues are \\(1\\) and \\(5\\) . The eigenvectors corresponding to these two eigenvalues are \\([1, -1]^T\\) and \\([3, 1]^T\\) . If we let these two matrices be the columns of a matrix \\(Q\\) , then: \\[ Q = \\begin{bmatrix} 1 & 3\\\\ -1 & 1 \\end{bmatrix}, Q^{-1} = \\begin{bmatrix} \\frac{1}{2} & -\\frac{1}{2}\\\\ \\frac{1}{6} & \\frac{1}{6} \\end{bmatrix}, D = \\begin{bmatrix} 1 & 0\\\\ 0 & 5 \\end{bmatrix} \\] We have: \\[ A = Q D Q^{-1} \\] Properties \u00b6 Let us look at two observations which exemplify the relationship between diagonalizable matrices and their diagonal counterparts. Eigenvalues \u00b6 The following observation will be useful for the rest of the analysis: Note If two matrices \\(A\\) and \\(B\\) are similar then they have the same eigenvalues. There exists some invertible matrix \\(Q\\) such that \\(A = QBQ^{-1}\\) . We can now look at the characteristic polynomial of \\(A\\) : \\[ \\begin{aligned} |A - \\lambda I| &= |QBQ^{-1} - \\lambda I|\\\\\\\\ &= |QBQ^{-1} - \\lambda Q Q^{-1}|\\\\\\\\ &= |Q(B - \\lambda I)Q^{-1}|\\\\\\\\ &= |Q| \\cdot |B - \\lambda I| \\cdot |Q^{-1}|\\\\\\\\ &= |B - \\lambda I| \\end{aligned} \\] The two matrices have the same characteristic polynomial and hence the same eigenvalues. Thus, a diagonalizable matrix has \\(n\\) eigenvalues, not necessarily distinct. These eigenvalues can be directly read off if we know the diagonal counterpart corresponding to \\(A\\) . Matrix power \u00b6 If \\(A\\) is a diagonalizable matrix, then: \\[ \\begin{aligned} A^k &= (QDQ^{-1})^k\\\\\\\\ &= \\underbrace{(QDQ^{-1}) \\cdots (QDQ^{-1})}_{k \\text{ blocks}}\\\\\\\\ &= Q D^k Q^{-1} \\end{aligned} \\] We see that \\(A^{k}\\) is also diagonalizable. The eigenvlaues of \\(A^k\\) are the \\(k^{th}\\) powers of the eigenvalues of \\(A\\) . Summary \u00b6 Diagonalizable matrices are those matrices that are not necessarily diagonal, yet share a close kinship with them. The following are equivalent definitions. An \\(n \\times n\\) matrix \\(A\\) is diagonalizable if: \\(\\mathbb{R}^n\\) has a basis of eigenvectors of \\(A\\) . \\(A\\) is similar to a diagonal matrix.","title":"Diagonalization"},{"location":"week-4/diagonalization/#diagonalization","text":"Question What is diagonalization?","title":"Diagonalization"},{"location":"week-4/diagonalization/#diagonal-matrices","text":"Recall that diagonal matrices are quite interesting to study from the point of view of eigenvectors and eigenvalues. So let us first explore some properties of the same. Let \\(D\\) be an arbitrary diagonal matrix: \\[ D = \\begin{bmatrix} a_1 & \\\\ & \\ddots &\\\\ & & a_n \\end{bmatrix} = \\text{diag}(a_1, \\cdots, a_n) \\]","title":"Diagonal matrices"},{"location":"week-4/diagonalization/#determinant","text":"The determinant of a diagonal matrix is given by the product of the values along the diagonal: \\[ |D| = a_1 \\cdots a_n \\]","title":"Determinant"},{"location":"week-4/diagonalization/#invertibility","text":"From the above equation, it follows that a diagonal matrix is invertible if and only if none of the diagonal elements are zero.","title":"Invertibility"},{"location":"week-4/diagonalization/#matrix-power","text":"Consider the product \\(D^k\\) . This is extremely simple in the case of a diagonal matrix: $$ D^k = \\text{diag}(a_1^k, \\cdots, a_n^k) $$ We won't prove this statement now. But this is quite easy to see.","title":"Matrix power"},{"location":"week-4/diagonalization/#eigenvalues","text":"This is something that we have already seen. The eigenvalues of a diagonal matrix are the diagonal elements. For the next two properties, let us express the diagonal matrix in terms of its eigenvalues: $$ D = \\text{diag}(\\lambda_1, \\cdots, \\lambda_n) $$ A diagonal matrix has \\(n\\) eigenvalues, not necessarily distinct. The characteristic polynomial corresponding to it is: \\[ (\\lambda - \\lambda_1) \\cdots (\\lambda - \\lambda_n) \\]","title":"Eigenvalues"},{"location":"week-4/diagonalization/#eigenvectors","text":"It is easy to see that the elements of the standard basis \\(\\beta = \\{e_1, \\cdots, e_n\\}\\) are the eigenvectors of \\(D\\) . Specifically, \\(e_i\\) is an eigenvector of \\(D\\) corresponding to the eigenvalue \\(\\lambda_i\\) as \\(De_i = \\lambda_i e_i\\) . Can we characterize every possible eigenvector corresponding to eigenvalue \\(\\lambda_i\\) ? For this particular case, let us assume that the eigenvalues are all distinct. Now, let \\(v\\) be some eigenvector of \\(D\\) corresponding to \\(\\lambda_i\\) : \\[ \\begin{bmatrix} \\lambda_1 & &\\\\ & \\ddots & &\\\\ & & \\lambda_n \\end{bmatrix} \\begin{bmatrix} v_1\\\\ \\vdots\\\\ v_n \\end{bmatrix} = \\begin{bmatrix} \\lambda_i v_1\\\\ \\vdots\\\\ \\lambda_iv_n \\end{bmatrix} \\] This can be further simplified as: \\[ \\begin{bmatrix} (\\lambda_1 - \\lambda_i) v_1\\\\ \\vdots\\\\ (\\lambda_n - \\lambda_i) v_n \\end{bmatrix} = \\begin{bmatrix} 0\\\\ \\vdots\\\\ 0 \\end{bmatrix} \\] Since, \\(\\lambda_i \\neq \\lambda_j\\) for \\(i \\neq j\\) , we have, \\(v_j = 0\\) for \\(j \\neq i\\) . Thus, the eigenvector corresponding to \\(\\lambda_i\\) is some non-zero vector in \\(\\text{span}(\\{e_i\\})\\) .","title":"Eigenvectors"},{"location":"week-4/diagonalization/#diagonalizability","text":"Now that we have seen some interesting properties concerning diagonal matrices, we shall study a special kind of matrices called diagonalizable matrices . These matrices are not necessarily diagonal, yet share a close kinship with them. Even if a matrix is not diagonal, can we hope that it is related in some way to a diagonal matrix? In the last section, we noted that the vectors in the standard basis are eigenvectors of a diagonal matrix. We will generalize this property. Assume that an \\(n \\times n\\) matrix has a set of \\(n\\) linearly independent eigenvectors: \\(\\beta = \\{v_1, \\cdots, v_n\\}\\) . Then \\(\\beta\\) is a basis for \\(\\mathbb{R}^n\\) . From this, we have a definition for diagonalizable matrices: Definition-1 An \\(n \\times n\\) matrix \\(A\\) is diagonalizable if there is a basis of eigenvectors for \\(\\mathbb{R}^n\\) . Taking off from this definition, for \\(1 \\leq i \\leq n\\) , we have: \\[ Av_i = \\lambda_i v_i \\] We can rearrange all these \\(n\\) equations as follows: \\[ A \\begin{bmatrix} \\vert & & \\vert\\\\ v_1 & \\cdots & v_n\\\\ \\vert & & \\vert \\end{bmatrix} = \\begin{bmatrix} \\vert & & \\vert\\\\ v_1 & \\cdots & v_n\\\\ \\vert & & \\vert \\end{bmatrix} \\begin{bmatrix} \\lambda_1 & &\\\\ & \\ddots &\\\\ & & \\lambda_n \\end{bmatrix} \\] Let \\(Q\\) be the matrix of eigenvectors and \\(D\\) be the diagonal matrix of eigenvalues: \\[ Q = \\begin{bmatrix} \\vert & & \\vert\\\\ v_1 & \\cdots & v_n\\\\ \\vert & & \\vert \\end{bmatrix}, D = \\begin{bmatrix} \\lambda_1 & & \\\\ & \\ddots & \\\\ & & \\lambda_n \\end{bmatrix} \\] We can now express the equation as: \\[ A Q = Q D \\] \\(Q\\) is invertible as the columns are linearly independent. With this, we can now rewrite the equation as: \\[ A = QDQ^{-1} \\] We have managed to establish a bridge from the matrix \\(A\\) to a diagonal matrix \\(D\\) . Matrices \\(A\\) and \\(D\\) are similar. Similar matrices Two \\(n \\times n\\) matrices \\(A\\) and \\(B\\) are similar if there exists an invertible matrix \\(Q\\) such that \\(A = QBQ^{-1}\\) . This gives an alternative definition for diagonalizable matrices: Definition-2 A matrix \\(A\\) is diagonalizable if it is similar to a diagonal matrix.","title":"Diagonalizability"},{"location":"week-4/diagonalization/#examples","text":"All diagonal matrices are diagonalizable. These are trivial examples of diagonalizable matrices. Let us take up a slightly non-trivial example of a diaongalizable matrix: \\[ A = \\begin{bmatrix} 4 & 3\\\\ 1 & 2 \\end{bmatrix} \\] The eigenvalues are \\(1\\) and \\(5\\) . The eigenvectors corresponding to these two eigenvalues are \\([1, -1]^T\\) and \\([3, 1]^T\\) . If we let these two matrices be the columns of a matrix \\(Q\\) , then: \\[ Q = \\begin{bmatrix} 1 & 3\\\\ -1 & 1 \\end{bmatrix}, Q^{-1} = \\begin{bmatrix} \\frac{1}{2} & -\\frac{1}{2}\\\\ \\frac{1}{6} & \\frac{1}{6} \\end{bmatrix}, D = \\begin{bmatrix} 1 & 0\\\\ 0 & 5 \\end{bmatrix} \\] We have: \\[ A = Q D Q^{-1} \\]","title":"Examples"},{"location":"week-4/diagonalization/#properties","text":"Let us look at two observations which exemplify the relationship between diagonalizable matrices and their diagonal counterparts.","title":"Properties"},{"location":"week-4/diagonalization/#eigenvalues_1","text":"The following observation will be useful for the rest of the analysis: Note If two matrices \\(A\\) and \\(B\\) are similar then they have the same eigenvalues. There exists some invertible matrix \\(Q\\) such that \\(A = QBQ^{-1}\\) . We can now look at the characteristic polynomial of \\(A\\) : \\[ \\begin{aligned} |A - \\lambda I| &= |QBQ^{-1} - \\lambda I|\\\\\\\\ &= |QBQ^{-1} - \\lambda Q Q^{-1}|\\\\\\\\ &= |Q(B - \\lambda I)Q^{-1}|\\\\\\\\ &= |Q| \\cdot |B - \\lambda I| \\cdot |Q^{-1}|\\\\\\\\ &= |B - \\lambda I| \\end{aligned} \\] The two matrices have the same characteristic polynomial and hence the same eigenvalues. Thus, a diagonalizable matrix has \\(n\\) eigenvalues, not necessarily distinct. These eigenvalues can be directly read off if we know the diagonal counterpart corresponding to \\(A\\) .","title":"Eigenvalues"},{"location":"week-4/diagonalization/#matrix-power_1","text":"If \\(A\\) is a diagonalizable matrix, then: \\[ \\begin{aligned} A^k &= (QDQ^{-1})^k\\\\\\\\ &= \\underbrace{(QDQ^{-1}) \\cdots (QDQ^{-1})}_{k \\text{ blocks}}\\\\\\\\ &= Q D^k Q^{-1} \\end{aligned} \\] We see that \\(A^{k}\\) is also diagonalizable. The eigenvlaues of \\(A^k\\) are the \\(k^{th}\\) powers of the eigenvalues of \\(A\\) .","title":"Matrix power"},{"location":"week-4/diagonalization/#summary","text":"Diagonalizable matrices are those matrices that are not necessarily diagonal, yet share a close kinship with them. The following are equivalent definitions. An \\(n \\times n\\) matrix \\(A\\) is diagonalizable if: \\(\\mathbb{R}^n\\) has a basis of eigenvectors of \\(A\\) . \\(A\\) is similar to a diagonal matrix.","title":"Summary"},{"location":"week-4/eigenvectors/","text":"Eigenvectors and Eigenvalues \u00b6 Question What are eigenvectors and eigenvalues? Let us consider the following linear transformation 1 from \\(\\mathbb{R}^{2}\\) to itself: \\[ T = \\begin{bmatrix} 3 & 1\\\\ 0 & 2 \\end{bmatrix} \\] We will look at how this linear transformation operates on vectors from the geometric point of view. Specifically, we will record these two quantities: direction of the vector before the transformation direction of the vector after the transformation Case-1 \u00b6 The vector \\(u = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\) is transformed into \\(Tu = \\begin{bmatrix}4\\\\2\\end{bmatrix}\\) . The vector \\(u\\) was initially pointing in a particular direction. After the transformation, it points in a different direction. This case corresponds to vectors whose direction changes after the linear transformation. Case-2 \u00b6 The vector \\(u = \\begin{bmatrix}-1\\\\1\\end{bmatrix}\\) is transformed into \\(Tu = \\begin{bmatrix}-2\\\\2\\end{bmatrix}\\) . The direction of the vector \\(Tu\\) is the same as the direction of the vector \\(u\\) . In other words, the linear transformation has preserved the direction of this vector. However, its magnitude has changed. In this case, the vector \\(u\\) has been stretched by a factor of \\(2\\) . \\[ Tu = \\begin{bmatrix}-2\\\\2\\end{bmatrix} = 2u \\] Another example for this case is the basis vector \\(\\begin{bmatrix}1\\\\0\\end{bmatrix}\\) . For this vector: \\[ Tu = \\begin{bmatrix}3\\\\0\\end{bmatrix} = 3u \\] This case corresponds to vectors whose direction remains unchanged after the linear transformation. Eigenvectors and Eigenvalues \u00b6 A non-zero vector which points in the same direction before and after the linear transformation is called an eigenvector . Since the direction of an eigenvector is unchanged by the transformation, it makes sense to look at the magnitude by which it is scaled after the transformation. This scalar value is called the eigenvalue . The eigenvector and the corresponding eigenvalue make up an eigenpair. For the matrix (linear transformation) that we have been working with, \\(\\left (2, \\begin{bmatrix}-1\\\\1\\end{bmatrix} \\right)\\) and \\(\\left (3, \\begin{bmatrix}1\\\\0\\end{bmatrix} \\right)\\) are two eigenpairs. Eigenspace \u00b6 We will now shift to matrices from linear transformations. Consider an arbitrary \\(n \\times n\\) matrix \\(A\\) . How big is the space of eigenvectors? Let \\(A\\) be a matrix. If \\(u\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) , what can we say about the vector \\(2u\\) ? \\[ A(2u) = 2 Au = 2 \\lambda u = \\lambda(2u) \\] We see that \\(2u\\) is also an eigenvector with eigenvalue \\(\\lambda\\) . In fact, \\(ku\\) is an eigenvector for every non-zero \\(k\\) . Coming from another direction, let \\(u\\) and \\(v\\) be two eigenvectors for the same eigenvalue \\(\\lambda\\) . Then: \\[ A(u + v) = Au + Av = \\lambda u + \\lambda v = \\lambda(u + v) \\] Therefore, \\(u + v\\) is also an eigenvector of \\(A\\) with eigenvalue of \\(\\lambda\\) . From these two observations, we see that the set of all eigenvectors with eigenvalue \\(\\lambda\\) , along with the zero vector, is a subspace of \\(\\mathbb{R}^{n}\\) . Note \\(0\\) can never be an eigenvector. This is because if \\(A0 = \\lambda 0\\) then there is no fixed \\(\\lambda\\) that we can associate with \\(0\\) . Therefore, eigenvectors are non-zero vectors. Summary \u00b6 For a matrix \\(A\\) , a non-zero vector \\(x\\) is an eigenvector with eigenvalue \\(\\lambda\\) if \\(Ax = \\lambda x\\) . The entire visual presentation has been borrowed from Grant Sanderson's videos on linear algebra. The specific example has been borrowed from this video . \u21a9","title":"Eigenvectors and Eigenvalues"},{"location":"week-4/eigenvectors/#eigenvectors-and-eigenvalues","text":"Question What are eigenvectors and eigenvalues? Let us consider the following linear transformation 1 from \\(\\mathbb{R}^{2}\\) to itself: \\[ T = \\begin{bmatrix} 3 & 1\\\\ 0 & 2 \\end{bmatrix} \\] We will look at how this linear transformation operates on vectors from the geometric point of view. Specifically, we will record these two quantities: direction of the vector before the transformation direction of the vector after the transformation","title":"Eigenvectors and Eigenvalues"},{"location":"week-4/eigenvectors/#case-1","text":"The vector \\(u = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\) is transformed into \\(Tu = \\begin{bmatrix}4\\\\2\\end{bmatrix}\\) . The vector \\(u\\) was initially pointing in a particular direction. After the transformation, it points in a different direction. This case corresponds to vectors whose direction changes after the linear transformation.","title":"Case-1"},{"location":"week-4/eigenvectors/#case-2","text":"The vector \\(u = \\begin{bmatrix}-1\\\\1\\end{bmatrix}\\) is transformed into \\(Tu = \\begin{bmatrix}-2\\\\2\\end{bmatrix}\\) . The direction of the vector \\(Tu\\) is the same as the direction of the vector \\(u\\) . In other words, the linear transformation has preserved the direction of this vector. However, its magnitude has changed. In this case, the vector \\(u\\) has been stretched by a factor of \\(2\\) . \\[ Tu = \\begin{bmatrix}-2\\\\2\\end{bmatrix} = 2u \\] Another example for this case is the basis vector \\(\\begin{bmatrix}1\\\\0\\end{bmatrix}\\) . For this vector: \\[ Tu = \\begin{bmatrix}3\\\\0\\end{bmatrix} = 3u \\] This case corresponds to vectors whose direction remains unchanged after the linear transformation.","title":"Case-2"},{"location":"week-4/eigenvectors/#eigenvectors-and-eigenvalues_1","text":"A non-zero vector which points in the same direction before and after the linear transformation is called an eigenvector . Since the direction of an eigenvector is unchanged by the transformation, it makes sense to look at the magnitude by which it is scaled after the transformation. This scalar value is called the eigenvalue . The eigenvector and the corresponding eigenvalue make up an eigenpair. For the matrix (linear transformation) that we have been working with, \\(\\left (2, \\begin{bmatrix}-1\\\\1\\end{bmatrix} \\right)\\) and \\(\\left (3, \\begin{bmatrix}1\\\\0\\end{bmatrix} \\right)\\) are two eigenpairs.","title":"Eigenvectors and Eigenvalues"},{"location":"week-4/eigenvectors/#eigenspace","text":"We will now shift to matrices from linear transformations. Consider an arbitrary \\(n \\times n\\) matrix \\(A\\) . How big is the space of eigenvectors? Let \\(A\\) be a matrix. If \\(u\\) is an eigenvector of \\(A\\) with eigenvalue \\(\\lambda\\) , what can we say about the vector \\(2u\\) ? \\[ A(2u) = 2 Au = 2 \\lambda u = \\lambda(2u) \\] We see that \\(2u\\) is also an eigenvector with eigenvalue \\(\\lambda\\) . In fact, \\(ku\\) is an eigenvector for every non-zero \\(k\\) . Coming from another direction, let \\(u\\) and \\(v\\) be two eigenvectors for the same eigenvalue \\(\\lambda\\) . Then: \\[ A(u + v) = Au + Av = \\lambda u + \\lambda v = \\lambda(u + v) \\] Therefore, \\(u + v\\) is also an eigenvector of \\(A\\) with eigenvalue of \\(\\lambda\\) . From these two observations, we see that the set of all eigenvectors with eigenvalue \\(\\lambda\\) , along with the zero vector, is a subspace of \\(\\mathbb{R}^{n}\\) . Note \\(0\\) can never be an eigenvector. This is because if \\(A0 = \\lambda 0\\) then there is no fixed \\(\\lambda\\) that we can associate with \\(0\\) . Therefore, eigenvectors are non-zero vectors.","title":"Eigenspace"},{"location":"week-4/eigenvectors/#summary","text":"For a matrix \\(A\\) , a non-zero vector \\(x\\) is an eigenvector with eigenvalue \\(\\lambda\\) if \\(Ax = \\lambda x\\) . The entire visual presentation has been borrowed from Grant Sanderson's videos on linear algebra. The specific example has been borrowed from this video . \u21a9","title":"Summary"},{"location":"week-4/outline/","text":"Outline \u00b6 For the next three weeks, our setting will be unsupervised learning. We won't have access to the label vector \\(y\\) . The only thing that is in our possession is the data matrix \\(X\\) . Our goal for the next three weeks will be to develop tools to \"understand data\". The notion of \"understanding data\" is rather vague, but we will try to arrive at a reasonable understanding of it. In this week, we will focus on concepts called eigenvalues and eigenvectors which are the building blocks for the methods that we will develop in weeks 5 and 6. Though we have been using the variable \\(X\\) for the data-matrix all through last week, it would be convenient to use the variable \\(A\\) for the next few weeks.","title":"Outline"},{"location":"week-4/outline/#outline","text":"For the next three weeks, our setting will be unsupervised learning. We won't have access to the label vector \\(y\\) . The only thing that is in our possession is the data matrix \\(X\\) . Our goal for the next three weeks will be to develop tools to \"understand data\". The notion of \"understanding data\" is rather vague, but we will try to arrive at a reasonable understanding of it. In this week, we will focus on concepts called eigenvalues and eigenvectors which are the building blocks for the methods that we will develop in weeks 5 and 6. Though we have been using the variable \\(X\\) for the data-matrix all through last week, it would be convenient to use the variable \\(A\\) for the next few weeks.","title":"Outline"},{"location":"week-4/tests_diagonal/","text":"Test for Diagonalizability \u00b6 Question How do we know if a matrix is diagonalizable? Negative test \u00b6 Are all matrices diagonalizable 1 ? Not really. As a counterexample, consider the following matrix: \\[ A = \\begin{bmatrix} 0 & 1\\\\ -1 & 0 \\end{bmatrix} \\] The characteristic polynomial for \\(A\\) is: \\[ \\begin{aligned} |A - \\lambda I| &= \\begin{vmatrix} -\\lambda & 1\\\\ -1 & -\\lambda \\end{vmatrix}\\\\\\\\ &= \\lambda^2 + 1 \\end{aligned} \\] We see that the polynomial has no real roots. Thus the matrix \\(A\\) has no real eigenvalues. Using this example, we can formulate the following negative test for diagonalizability: Negative test If the characteristic polynomial of a matrix \\(A\\) does not have \\(n\\) eigenvalues, then the matrix is not diagonalizable. Positive test \u00b6 Now that we know that not all matrices are diagonalizable, is there a way to find out which matrices are? We will look at one such test which is based on the following observation. Note The eigenvectors corresponding to distinct eigenvalues of \\(A\\) are linearly independent. We shall prove this for the case of two eigenvlaues. Let \\((\\lambda_1, v_1)\\) and \\((\\lambda_2, v_2)\\) be two eigenpairs of \\(A\\) such that \\(\\lambda_1 \\neq \\lambda_2\\) . Consider any linear combination of \\(v_1\\) and \\(v_2\\) : \\[ \\alpha_1v_1 + \\alpha_2 v_2 = 0 \\] Pre-multiplying by \\(A - \\lambda_1 I\\) on both sides: \\[ \\begin{aligned} (A - \\lambda_1I)(\\alpha_1 v_1 + \\alpha_2 v_2) &= 0\\\\\\\\ \\alpha_2(\\lambda_2 - \\lambda_1) &= 0 \\end{aligned} \\] Since \\(\\lambda_1 \\neq \\lambda_2\\) , we have \\(\\alpha_2 = 0\\) . From this it follows that \\(\\alpha_1 = 0\\) . So, \\(v_1\\) and \\(v_2\\) are linearly independent. So, one positive test for diagonalizability is this: Positive test If an \\(n \\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then it is diagonalizable. No man's land \u00b6 A natural question now arises. What about the case where a matrix has \\(n\\) \u200b eigenvalues, but in which some eigenvalues repeat. Is it always diagonalizable? Even if a matrix has eigenvalues, there is no guarantee that it will have a basis of eigenvectors. For example, consider the matrix: \\[ A = \\begin{bmatrix} 2 & 1\\\\ 0 & 2 \\end{bmatrix} \\] The characteristic polynomial is \\((\\lambda - 2)^2\\) . So, \\(\\lambda = 2\\) is the only eigenvalue, but repeated twice. To get the eigenvectors, we solve \\((A - 2I)v = 0\\) : \\[ \\begin{bmatrix} 0 & 1\\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} v_1\\\\v_2 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix} \\] This gives us \\(v_2 = 0\\) and \\(v_1 = k\\) , where \\(k\\) is some non-zero real number. Thus, every eigenvector of \\(A\\) is some non-zero vector in \\(\\text{span}\\left( \\left\\{ \\begin{bmatrix}1\\\\0 \\end{bmatrix} \\right\\} \\right)\\) . Since we don't have two linearly independent eigenvectors, \\(A\\) is not diagonalizable. Summary \u00b6 Not all matrices are diagonalizable. There are two tests for diagonlizability: Negative test: if a matrix doesn't have \\(n\\) eigenvalues, it is not diagonalizable. Positive test: if a matrix has \\(n\\) distinct eigenvalues, then it is diagonalizable. If a matrix has \\(n\\) eigenvalues, but with repetitions, then the test is inconclusive. In all the discussions in this unit, we will be only concerned with real vector spaces. The matrices and their eigenvalues, if they exist, will be real. \u21a9","title":"Test for Diagonalizability"},{"location":"week-4/tests_diagonal/#test-for-diagonalizability","text":"Question How do we know if a matrix is diagonalizable?","title":"Test for Diagonalizability"},{"location":"week-4/tests_diagonal/#negative-test","text":"Are all matrices diagonalizable 1 ? Not really. As a counterexample, consider the following matrix: \\[ A = \\begin{bmatrix} 0 & 1\\\\ -1 & 0 \\end{bmatrix} \\] The characteristic polynomial for \\(A\\) is: \\[ \\begin{aligned} |A - \\lambda I| &= \\begin{vmatrix} -\\lambda & 1\\\\ -1 & -\\lambda \\end{vmatrix}\\\\\\\\ &= \\lambda^2 + 1 \\end{aligned} \\] We see that the polynomial has no real roots. Thus the matrix \\(A\\) has no real eigenvalues. Using this example, we can formulate the following negative test for diagonalizability: Negative test If the characteristic polynomial of a matrix \\(A\\) does not have \\(n\\) eigenvalues, then the matrix is not diagonalizable.","title":"Negative test"},{"location":"week-4/tests_diagonal/#positive-test","text":"Now that we know that not all matrices are diagonalizable, is there a way to find out which matrices are? We will look at one such test which is based on the following observation. Note The eigenvectors corresponding to distinct eigenvalues of \\(A\\) are linearly independent. We shall prove this for the case of two eigenvlaues. Let \\((\\lambda_1, v_1)\\) and \\((\\lambda_2, v_2)\\) be two eigenpairs of \\(A\\) such that \\(\\lambda_1 \\neq \\lambda_2\\) . Consider any linear combination of \\(v_1\\) and \\(v_2\\) : \\[ \\alpha_1v_1 + \\alpha_2 v_2 = 0 \\] Pre-multiplying by \\(A - \\lambda_1 I\\) on both sides: \\[ \\begin{aligned} (A - \\lambda_1I)(\\alpha_1 v_1 + \\alpha_2 v_2) &= 0\\\\\\\\ \\alpha_2(\\lambda_2 - \\lambda_1) &= 0 \\end{aligned} \\] Since \\(\\lambda_1 \\neq \\lambda_2\\) , we have \\(\\alpha_2 = 0\\) . From this it follows that \\(\\alpha_1 = 0\\) . So, \\(v_1\\) and \\(v_2\\) are linearly independent. So, one positive test for diagonalizability is this: Positive test If an \\(n \\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then it is diagonalizable.","title":"Positive test"},{"location":"week-4/tests_diagonal/#no-mans-land","text":"A natural question now arises. What about the case where a matrix has \\(n\\) \u200b eigenvalues, but in which some eigenvalues repeat. Is it always diagonalizable? Even if a matrix has eigenvalues, there is no guarantee that it will have a basis of eigenvectors. For example, consider the matrix: \\[ A = \\begin{bmatrix} 2 & 1\\\\ 0 & 2 \\end{bmatrix} \\] The characteristic polynomial is \\((\\lambda - 2)^2\\) . So, \\(\\lambda = 2\\) is the only eigenvalue, but repeated twice. To get the eigenvectors, we solve \\((A - 2I)v = 0\\) : \\[ \\begin{bmatrix} 0 & 1\\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} v_1\\\\v_2 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix} \\] This gives us \\(v_2 = 0\\) and \\(v_1 = k\\) , where \\(k\\) is some non-zero real number. Thus, every eigenvector of \\(A\\) is some non-zero vector in \\(\\text{span}\\left( \\left\\{ \\begin{bmatrix}1\\\\0 \\end{bmatrix} \\right\\} \\right)\\) . Since we don't have two linearly independent eigenvectors, \\(A\\) is not diagonalizable.","title":"No man's land"},{"location":"week-4/tests_diagonal/#summary","text":"Not all matrices are diagonalizable. There are two tests for diagonlizability: Negative test: if a matrix doesn't have \\(n\\) eigenvalues, it is not diagonalizable. Positive test: if a matrix has \\(n\\) distinct eigenvalues, then it is diagonalizable. If a matrix has \\(n\\) eigenvalues, but with repetitions, then the test is inconclusive. In all the discussions in this unit, we will be only concerned with real vector spaces. The matrices and their eigenvalues, if they exist, will be real. \u21a9","title":"Summary"},{"location":"week-4/transformations/","text":"Linear Transformations \u00b6 Question Geometrically, what does a linear transformation do to vectors? Algebraic view \u00b6 Recall that a linear transformation is a linear mapping between two vector spaces \\(V\\) and \\(W\\) : $$ T: V \\rightarrow W $$ In this course, we will be dealing with \\(\\mathbb{R}^n\\) . So, our transformations will be of the form: $$ T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $$ To get a better idea about what a linear transformation does, let us restrict our attention to a map from \\(\\mathbb{R}^{2}\\) to itself. If \\(u\\) is a vector in \\(\\mathbb{R}^{2}\\) , then the transformation returns another vector in the same space. We can associate a matrix for every linear transformation. Assuming that we use the standard ordered basis \\(\\beta = \\{e_1, e_2\\}\\) for \\(\\mathbb{R}^2\\) : \\[ [T]_{\\beta}^{\\beta} = \\begin{bmatrix} \\big\\vert & \\big\\vert\\\\ T(e_1) & T(e_2)\\\\ \\big\\vert & \\big\\vert \\end{bmatrix} \\] The basis vectors \\(e_1\\) and \\(e_2\\) are mapped to \\(T(e_1)\\) and \\(T(e_2)\\) respectively. The action of the linear transformation on the basis vectors gives us complete information on what happens to any vector in \\(\\mathbb{R}^{2}\\) . Geometric view \u00b6 Geometrically, what does all this mean? Let us begin with a simple example: \\[ T = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix} \\] This is the identity transformation. It doesn't disturb the vectors and leaves them as they are. This is not all that interesting. Next: \\[ T = \\begin{bmatrix} 2 & 0\\\\ 0 & 2 \\end{bmatrix} \\] Let us see what this does to the basis vectors 1 : Notice the effect it has. Each vector is scaled. In this case, it is stretched. It becomes twice as long as the input. To see why this is true algebraically, consider an arbitrary vector \\(x = \\begin{bmatrix}x_1 & x_2\\end{bmatrix}^T\\) : \\[ Tx = \\begin{bmatrix} 2 & 0\\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} = 2 \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} = 2x \\] Let us now consider another matrix: \\[ T = \\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix} \\] This is a rotation matrix. That is, it rotates the input vector without changing its magnitude. Moving on, let us take up another matrix. This time, let us compose the two linear transformations that we have seen. Composition of linear transformations is equivalent to matrix multiplication: \\[ T = \\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 & 0\\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 0 & -2\\\\ 2 & 0 \\end{bmatrix} \\] What do you expect this matrix to do? It stretches the vectors and rotates them by \\(90^{\\circ}\\) . Note that the two matrices involved in the product are commutative. That is, \\(T_1 T_2 = T_2 T_1\\) . Intuitively, we can see why this is true. We could either stretch a vector and then rotate it (OR) we could rotate it and then stretch it. Let us now move to a more complex linear transformation: \\[ T = \\begin{bmatrix} 2 & 1\\\\ 1 & 2 \\end{bmatrix} \\] This is not simple rotation or stretching. It is an example of a shear transformation. Now that we have a good idea of what linear transformations do, we are ready to explore the idea of eigenvalues and eigenvectors. The library Manim Community was used to render these animations. The code for this can be found in the GitHub repository. \u21a9","title":"Linear Transformations"},{"location":"week-4/transformations/#linear-transformations","text":"Question Geometrically, what does a linear transformation do to vectors?","title":"Linear Transformations"},{"location":"week-4/transformations/#algebraic-view","text":"Recall that a linear transformation is a linear mapping between two vector spaces \\(V\\) and \\(W\\) : $$ T: V \\rightarrow W $$ In this course, we will be dealing with \\(\\mathbb{R}^n\\) . So, our transformations will be of the form: $$ T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m $$ To get a better idea about what a linear transformation does, let us restrict our attention to a map from \\(\\mathbb{R}^{2}\\) to itself. If \\(u\\) is a vector in \\(\\mathbb{R}^{2}\\) , then the transformation returns another vector in the same space. We can associate a matrix for every linear transformation. Assuming that we use the standard ordered basis \\(\\beta = \\{e_1, e_2\\}\\) for \\(\\mathbb{R}^2\\) : \\[ [T]_{\\beta}^{\\beta} = \\begin{bmatrix} \\big\\vert & \\big\\vert\\\\ T(e_1) & T(e_2)\\\\ \\big\\vert & \\big\\vert \\end{bmatrix} \\] The basis vectors \\(e_1\\) and \\(e_2\\) are mapped to \\(T(e_1)\\) and \\(T(e_2)\\) respectively. The action of the linear transformation on the basis vectors gives us complete information on what happens to any vector in \\(\\mathbb{R}^{2}\\) .","title":"Algebraic view"},{"location":"week-4/transformations/#geometric-view","text":"Geometrically, what does all this mean? Let us begin with a simple example: \\[ T = \\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix} \\] This is the identity transformation. It doesn't disturb the vectors and leaves them as they are. This is not all that interesting. Next: \\[ T = \\begin{bmatrix} 2 & 0\\\\ 0 & 2 \\end{bmatrix} \\] Let us see what this does to the basis vectors 1 : Notice the effect it has. Each vector is scaled. In this case, it is stretched. It becomes twice as long as the input. To see why this is true algebraically, consider an arbitrary vector \\(x = \\begin{bmatrix}x_1 & x_2\\end{bmatrix}^T\\) : \\[ Tx = \\begin{bmatrix} 2 & 0\\\\ 0 & 2 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} = 2 \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix} = 2x \\] Let us now consider another matrix: \\[ T = \\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix} \\] This is a rotation matrix. That is, it rotates the input vector without changing its magnitude. Moving on, let us take up another matrix. This time, let us compose the two linear transformations that we have seen. Composition of linear transformations is equivalent to matrix multiplication: \\[ T = \\begin{bmatrix} 0 & -1\\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 & 0\\\\ 0 & 2 \\end{bmatrix} = \\begin{bmatrix} 0 & -2\\\\ 2 & 0 \\end{bmatrix} \\] What do you expect this matrix to do? It stretches the vectors and rotates them by \\(90^{\\circ}\\) . Note that the two matrices involved in the product are commutative. That is, \\(T_1 T_2 = T_2 T_1\\) . Intuitively, we can see why this is true. We could either stretch a vector and then rotate it (OR) we could rotate it and then stretch it. Let us now move to a more complex linear transformation: \\[ T = \\begin{bmatrix} 2 & 1\\\\ 1 & 2 \\end{bmatrix} \\] This is not simple rotation or stretching. It is an example of a shear transformation. Now that we have a good idea of what linear transformations do, we are ready to explore the idea of eigenvalues and eigenvectors. The library Manim Community was used to render these animations. The code for this can be found in the GitHub repository. \u21a9","title":"Geometric view"}]}