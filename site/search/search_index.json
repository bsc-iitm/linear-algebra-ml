{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Foundations \u00b6 Notes for the MLF course","title":"Home"},{"location":"#machine-learning-foundations","text":"Notes for the MLF course","title":"Machine Learning Foundations"},{"location":"appendix/0-gradients/","text":"Gradients \u00b6 Question How do we compute the gradient of the expression \\(||Ax - b||^2\\) with respect to \\(x\\) ? Setting \u00b6 Let us consider the objective function in the minimzation problem for linear regression: \\[ E = ||Ax - b||^2 \\] This can be expressed as: \\[ E = (Ax - b)^T(Ax - b) \\] The gradient of \\(E\\) with respect to \\(x\\) is given by: \\[ \\nabla_{x} E = \\begin{bmatrix} \\cfrac{\\partial E}{\\partial x_1}\\\\ \\vdots\\\\ \\cfrac{\\partial E}{\\partial x_n} \\end{bmatrix} \\] How do we compute this gradient? First, let us rewrite the expression by introducing an error vector, \\(e = Ax - b\\) : \\[ E = e^Te \\] The gradient of \\(E\\) with respect to \\(x\\) can be obtained in a two step process: \\(\\nabla_e E\\) : first compute the gradient of \\(E\\) with respect to the error Use the chain rule of differentiation to compute the gradient of \\(E\\) with respect to \\(x\\) The chain rule when multiple variables are involved can be a bit tricky. This is what will take up next. Chain rule for multiple variables \u00b6 Warning Heavy use of algebra. Go slowly. Intuitively, what does \\(\\frac{\\partial E}{\\partial x_i}\\) mean? This partial derivative measures the effect on the variable \\(E\\) when \\(x_i\\) is perturbed a little keeping all other variables constant. This perturbation propagates to \\(E\\) via \\(e\\) . There is a chain reaction: \\(x_1\\) affects some of the variables in the vector \\(e\\) , which in turn affects \\(E\\) . To get a better feel for this, let us take an example: \\[ \\begin{aligned} e &= Ax - b\\\\\\\\ \\begin{bmatrix} e_1\\\\ e_2\\\\ e_3 \\end{bmatrix} &= \\begin{bmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{12}\\\\ a_{11} & a_{12}\\\\ \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix}\\\\\\\\ E &= e^T e = e_1^2 + e_2^2 + e_3^2 \\end{aligned} \\] The variable \\(x_1\\) influences \\(E\\) via three independent routes: \\(x_1 \\rightarrow e_1 \\rightarrow E\\) \\(x_1 \\rightarrow e_2 \\rightarrow E\\) \\(x_1 \\rightarrow e_3 \\rightarrow E\\) The partial derivative needs to take into account the contributions from each of these three routes. Pictorially: Symbolically: \\[ \\cfrac{\\partial E}{\\partial x_1} = \\cfrac{\\partial E}{\\partial e_1} \\cfrac{\\partial e_1}{\\partial x_1} + \\cfrac{\\partial E}{\\partial e_2} \\cfrac{\\partial e_2}{\\partial x_1} + \\cfrac{\\partial E}{\\partial e_3} \\cfrac{\\partial e_3}{\\partial x_1} \\] Observe that that the RHS is in the form of a dot product between two vectors: \\[ \\cfrac{\\partial E}{\\partial x_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_1} & \\cfrac{\\partial e_2}{\\partial x_1} & \\cfrac{\\partial e_3}{\\partial x_1} \\end{bmatrix} \\begin{bmatrix} \\cfrac{\\partial E}{\\partial e_1}\\\\ \\cfrac{\\partial E}{\\partial e_2}\\\\ \\cfrac{\\partial E}{\\partial e_3} \\end{bmatrix} \\] Notice, that the second vector is nothing but \\(\\nabla_e E\\) : $$ \\cfrac{\\partial E}{\\partial x_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_1} & \\cfrac{\\partial e_2}{\\partial x_1} & \\cfrac{\\partial e_3}{\\partial x_1} \\end{bmatrix} \\nabla_e E $$ We can now add \\(\\cfrac{\\partial E}{\\partial x_2}\\) to the mix: \\[ \\begin{aligned} \\nabla_x E &= \\begin{bmatrix} \\cfrac{\\partial E}{\\partial x_1}\\\\ \\cfrac{\\partial E}{\\partial x_2} \\end{bmatrix}\\\\ \\\\ &= \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_1} & \\cfrac{\\partial e_2}{\\partial x_1} & \\cfrac{\\partial e_3}{\\partial x_1}\\\\ \\cfrac{\\partial e_1}{\\partial x_2} & \\cfrac{\\partial e_2}{\\partial x_2} & \\cfrac{\\partial e_3}{\\partial x_3} \\end{bmatrix} \\nabla_e E\\\\\\\\ &= J\\ \\nabla_e E \\end{aligned} \\] The matrix \\(J\\) is called the Jacobian matrix. It represents the partial derivatives of each component of the vector \\(e\\) with respect to each component of the vector, \\(x\\) . That is, every element of the Jacobian is of the form: \\[ J_{ij} = \\cfrac{\\partial e_j}{\\partial x_i} \\] Back to the gradients \u00b6 Let us resume our computation, but this time with a general \\(m \\times n\\) matrix \\(A\\) , \\(m\\) dimensional vector \\(b\\) and \\(n\\) dimensional vector \\(x\\) : \\[ \\nabla_e E = 2 e \\] We now need to compute the Jacobian. We have \\(e = Ax - b\\) . Since \\(b\\) is a constant, it will vanish while computing the derivatives. We only need to worry about \\(Ax\\) for which we can use the following relationship. If \\(a_i\\) is the \\(i^{th}\\) column of \\(A\\) , then: \\[ Ax = x_1 a_1 + \\cdots + x_n a_n \\] The \\(i^{th}\\) row of the Jacobian is: \\[ \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_i} & \\cdots & \\cfrac{\\partial e_m}{\\partial x_i} \\end{bmatrix} = a_i^T \\] As simple as that: the \\(i^{th}\\) row of the Jacobian is just the \\(i^{th}\\) column of \\(A\\) . So, the Jacobian is nothing but \\(A^T\\) . We now have the complete expression for \\(\\nabla_x E\\) : \\[ \\nabla_x E = 2 A^T (Ax - b) = 2(A^TA x - A^T b) \\]","title":"Gradients"},{"location":"appendix/0-gradients/#gradients","text":"Question How do we compute the gradient of the expression \\(||Ax - b||^2\\) with respect to \\(x\\) ?","title":"Gradients"},{"location":"appendix/0-gradients/#setting","text":"Let us consider the objective function in the minimzation problem for linear regression: \\[ E = ||Ax - b||^2 \\] This can be expressed as: \\[ E = (Ax - b)^T(Ax - b) \\] The gradient of \\(E\\) with respect to \\(x\\) is given by: \\[ \\nabla_{x} E = \\begin{bmatrix} \\cfrac{\\partial E}{\\partial x_1}\\\\ \\vdots\\\\ \\cfrac{\\partial E}{\\partial x_n} \\end{bmatrix} \\] How do we compute this gradient? First, let us rewrite the expression by introducing an error vector, \\(e = Ax - b\\) : \\[ E = e^Te \\] The gradient of \\(E\\) with respect to \\(x\\) can be obtained in a two step process: \\(\\nabla_e E\\) : first compute the gradient of \\(E\\) with respect to the error Use the chain rule of differentiation to compute the gradient of \\(E\\) with respect to \\(x\\) The chain rule when multiple variables are involved can be a bit tricky. This is what will take up next.","title":"Setting"},{"location":"appendix/0-gradients/#chain-rule-for-multiple-variables","text":"Warning Heavy use of algebra. Go slowly. Intuitively, what does \\(\\frac{\\partial E}{\\partial x_i}\\) mean? This partial derivative measures the effect on the variable \\(E\\) when \\(x_i\\) is perturbed a little keeping all other variables constant. This perturbation propagates to \\(E\\) via \\(e\\) . There is a chain reaction: \\(x_1\\) affects some of the variables in the vector \\(e\\) , which in turn affects \\(E\\) . To get a better feel for this, let us take an example: \\[ \\begin{aligned} e &= Ax - b\\\\\\\\ \\begin{bmatrix} e_1\\\\ e_2\\\\ e_3 \\end{bmatrix} &= \\begin{bmatrix} a_{11} & a_{12}\\\\ a_{21} & a_{12}\\\\ a_{11} & a_{12}\\\\ \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\end{bmatrix}\\\\\\\\ E &= e^T e = e_1^2 + e_2^2 + e_3^2 \\end{aligned} \\] The variable \\(x_1\\) influences \\(E\\) via three independent routes: \\(x_1 \\rightarrow e_1 \\rightarrow E\\) \\(x_1 \\rightarrow e_2 \\rightarrow E\\) \\(x_1 \\rightarrow e_3 \\rightarrow E\\) The partial derivative needs to take into account the contributions from each of these three routes. Pictorially: Symbolically: \\[ \\cfrac{\\partial E}{\\partial x_1} = \\cfrac{\\partial E}{\\partial e_1} \\cfrac{\\partial e_1}{\\partial x_1} + \\cfrac{\\partial E}{\\partial e_2} \\cfrac{\\partial e_2}{\\partial x_1} + \\cfrac{\\partial E}{\\partial e_3} \\cfrac{\\partial e_3}{\\partial x_1} \\] Observe that that the RHS is in the form of a dot product between two vectors: \\[ \\cfrac{\\partial E}{\\partial x_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_1} & \\cfrac{\\partial e_2}{\\partial x_1} & \\cfrac{\\partial e_3}{\\partial x_1} \\end{bmatrix} \\begin{bmatrix} \\cfrac{\\partial E}{\\partial e_1}\\\\ \\cfrac{\\partial E}{\\partial e_2}\\\\ \\cfrac{\\partial E}{\\partial e_3} \\end{bmatrix} \\] Notice, that the second vector is nothing but \\(\\nabla_e E\\) : $$ \\cfrac{\\partial E}{\\partial x_1} = \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_1} & \\cfrac{\\partial e_2}{\\partial x_1} & \\cfrac{\\partial e_3}{\\partial x_1} \\end{bmatrix} \\nabla_e E $$ We can now add \\(\\cfrac{\\partial E}{\\partial x_2}\\) to the mix: \\[ \\begin{aligned} \\nabla_x E &= \\begin{bmatrix} \\cfrac{\\partial E}{\\partial x_1}\\\\ \\cfrac{\\partial E}{\\partial x_2} \\end{bmatrix}\\\\ \\\\ &= \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_1} & \\cfrac{\\partial e_2}{\\partial x_1} & \\cfrac{\\partial e_3}{\\partial x_1}\\\\ \\cfrac{\\partial e_1}{\\partial x_2} & \\cfrac{\\partial e_2}{\\partial x_2} & \\cfrac{\\partial e_3}{\\partial x_3} \\end{bmatrix} \\nabla_e E\\\\\\\\ &= J\\ \\nabla_e E \\end{aligned} \\] The matrix \\(J\\) is called the Jacobian matrix. It represents the partial derivatives of each component of the vector \\(e\\) with respect to each component of the vector, \\(x\\) . That is, every element of the Jacobian is of the form: \\[ J_{ij} = \\cfrac{\\partial e_j}{\\partial x_i} \\]","title":"Chain rule for multiple variables"},{"location":"appendix/0-gradients/#back-to-the-gradients","text":"Let us resume our computation, but this time with a general \\(m \\times n\\) matrix \\(A\\) , \\(m\\) dimensional vector \\(b\\) and \\(n\\) dimensional vector \\(x\\) : \\[ \\nabla_e E = 2 e \\] We now need to compute the Jacobian. We have \\(e = Ax - b\\) . Since \\(b\\) is a constant, it will vanish while computing the derivatives. We only need to worry about \\(Ax\\) for which we can use the following relationship. If \\(a_i\\) is the \\(i^{th}\\) column of \\(A\\) , then: \\[ Ax = x_1 a_1 + \\cdots + x_n a_n \\] The \\(i^{th}\\) row of the Jacobian is: \\[ \\begin{bmatrix} \\cfrac{\\partial e_1}{\\partial x_i} & \\cdots & \\cfrac{\\partial e_m}{\\partial x_i} \\end{bmatrix} = a_i^T \\] As simple as that: the \\(i^{th}\\) row of the Jacobian is just the \\(i^{th}\\) column of \\(A\\) . So, the Jacobian is nothing but \\(A^T\\) . We now have the complete expression for \\(\\nabla_x E\\) : \\[ \\nabla_x E = 2 A^T (Ax - b) = 2(A^TA x - A^T b) \\]","title":"Back to the gradients"},{"location":"linear_algebra/0-linear_algebra/","text":"Linear Algebra for ML \u00b6 Question Why should we study linear algebra in a course on data science? Data \u00b6 The simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it: lattitude longitue age num_of_rooms area distance_from_school A typical problem in ML is this: Given these attributes or features of a house, can we predict its selling price? This is called a regression problem : given a set of features, map it to a real number. Vectors \u00b6 Let us take a concrete example of a single data-point : Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 num_of_rooms 2 area 1000 distance_from_nearest_school 3 selling_price 40 Lattitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs. But none of these really matter for an ML algorithm: it is going to abstract out the details (such as units) and look at this as a column of numbers, which is nothing but a vector: \\[ \\begin{bmatrix} 12.9\\\\ 80.2\\\\ 3\\\\ 2\\\\ 1000\\\\ 3\\\\ \\end{bmatrix} \\] Note that the selling price is not included as an element in the vector as that is usually unkown to us. This unkown quantity which we have to estimate or predict is called the target . Note Vectors are usually represented as column vectors or \\(n \\times 1\\) matrices. Matrices \u00b6 We cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tablular form: lattitude longitude age rooms area distance price 1 12.9 80.2 3 2 1000 3 40 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 50 14.3 75.9 30 2 1200 5 20 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 100 20.8 90.5 1 3 1500 2 35 This data for \\(100\\) houses is nothing but a \\(100 \\times 6\\) matrix: \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] Each row of this matrix corresponds to one data-point. In general, if a dataset has \\(n\\) features and \\(m\\) data-points, it is represented as a \\(m \\times n\\) data-matrix. Tip If you find yourself lost when working with matrices, remember that a matrix is a way represent a collection of data-points (dataset). Summary \u00b6 Data is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra \u2014 the study of vectors and matrices \u2014 if we wish to understand how ML algorithms work.","title":"Linear Algebra for ML"},{"location":"linear_algebra/0-linear_algebra/#linear-algebra-for-ml","text":"Question Why should we study linear algebra in a course on data science?","title":"Linear Algebra for ML"},{"location":"linear_algebra/0-linear_algebra/#data","text":"The simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it: lattitude longitue age num_of_rooms area distance_from_school A typical problem in ML is this: Given these attributes or features of a house, can we predict its selling price? This is called a regression problem : given a set of features, map it to a real number.","title":"Data"},{"location":"linear_algebra/0-linear_algebra/#vectors","text":"Let us take a concrete example of a single data-point : Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 num_of_rooms 2 area 1000 distance_from_nearest_school 3 selling_price 40 Lattitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs. But none of these really matter for an ML algorithm: it is going to abstract out the details (such as units) and look at this as a column of numbers, which is nothing but a vector: \\[ \\begin{bmatrix} 12.9\\\\ 80.2\\\\ 3\\\\ 2\\\\ 1000\\\\ 3\\\\ \\end{bmatrix} \\] Note that the selling price is not included as an element in the vector as that is usually unkown to us. This unkown quantity which we have to estimate or predict is called the target . Note Vectors are usually represented as column vectors or \\(n \\times 1\\) matrices.","title":"Vectors"},{"location":"linear_algebra/0-linear_algebra/#matrices","text":"We cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tablular form: lattitude longitude age rooms area distance price 1 12.9 80.2 3 2 1000 3 40 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 50 14.3 75.9 30 2 1200 5 20 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 100 20.8 90.5 1 3 1500 2 35 This data for \\(100\\) houses is nothing but a \\(100 \\times 6\\) matrix: \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] Each row of this matrix corresponds to one data-point. In general, if a dataset has \\(n\\) features and \\(m\\) data-points, it is represented as a \\(m \\times n\\) data-matrix. Tip If you find yourself lost when working with matrices, remember that a matrix is a way represent a collection of data-points (dataset).","title":"Matrices"},{"location":"linear_algebra/0-linear_algebra/#summary","text":"Data is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra \u2014 the study of vectors and matrices \u2014 if we wish to understand how ML algorithms work.","title":"Summary"},{"location":"linear_algebra/1-ml_problem/","text":"ML problem \u00b6 Question What does a typical ML problem look like? Regression \u00b6 Analogy \u00b6 Think about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and learns how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers: \\(103 + 205 = 308\\) \\(123 + 409 = 532\\) \\(185 + 483 = 668\\) The important point to note is that the student has access to both the questions and the answers. In the exam, she will not have access to the answers! But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. In other words, she would have to learn a function from the input (question) to the output (answer). This is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels . A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset . A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy. Data Representation \u00b6 We are given a collection of \\(m\\) data-points and \\(m\\) labels which are real numbers. Each data-point is described by \\(n\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as lattitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(n\\) . Arranging the \\(m\\) data-points in a matrix, we get a \\(m \\times n\\) data-matrix. Let us call this matrix \\(A\\) : \\[ A = \\begin{bmatrix} a_{11} & \\cdots & a_{1n}\\\\ \\vdots & a_{ij} & \\vdots\\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\] The labels can be put together in a vector of size \\(n\\) . Let us call this \\(b\\) : \\[ b = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_m \\end{bmatrix} \\] Model \u00b6 As stated earlier, a regression model is a function that transforms a data-point into a label. Formally: $$ f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} $$ Each feature-vector is of size \\(n\\) . So, the feature-vectors reside in the \\(n\\) dimensional space \\(\\mathbb{R}^{n}\\) . The labels are real numbers, so they reside in \\(\\mathbb{R}\\) . Tip You can think about an ML model as a function that maps a feature-vector to a label. Learning \u00b6 The heart of ML is learning from data. But who or what is learning? We can think of the model as the outcome of the learning process. For some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. This is exactly what a model does. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in the next unit. Summary \u00b6 Regression is a classic ML problem where we use labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(A\\) . The labels are arranged in a label vector called \\(b\\) . A model is a function that transforms a feature-vector to a label.","title":"ML problem"},{"location":"linear_algebra/1-ml_problem/#ml-problem","text":"Question What does a typical ML problem look like?","title":"ML problem"},{"location":"linear_algebra/1-ml_problem/#regression","text":"","title":"Regression"},{"location":"linear_algebra/1-ml_problem/#analogy","text":"Think about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and learns how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers: \\(103 + 205 = 308\\) \\(123 + 409 = 532\\) \\(185 + 483 = 668\\) The important point to note is that the student has access to both the questions and the answers. In the exam, she will not have access to the answers! But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. In other words, she would have to learn a function from the input (question) to the output (answer). This is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels . A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset . A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy.","title":"Analogy"},{"location":"linear_algebra/1-ml_problem/#data-representation","text":"We are given a collection of \\(m\\) data-points and \\(m\\) labels which are real numbers. Each data-point is described by \\(n\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as lattitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(n\\) . Arranging the \\(m\\) data-points in a matrix, we get a \\(m \\times n\\) data-matrix. Let us call this matrix \\(A\\) : \\[ A = \\begin{bmatrix} a_{11} & \\cdots & a_{1n}\\\\ \\vdots & a_{ij} & \\vdots\\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\] The labels can be put together in a vector of size \\(n\\) . Let us call this \\(b\\) : \\[ b = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_m \\end{bmatrix} \\]","title":"Data Representation"},{"location":"linear_algebra/1-ml_problem/#model","text":"As stated earlier, a regression model is a function that transforms a data-point into a label. Formally: $$ f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} $$ Each feature-vector is of size \\(n\\) . So, the feature-vectors reside in the \\(n\\) dimensional space \\(\\mathbb{R}^{n}\\) . The labels are real numbers, so they reside in \\(\\mathbb{R}\\) . Tip You can think about an ML model as a function that maps a feature-vector to a label.","title":"Model"},{"location":"linear_algebra/1-ml_problem/#learning","text":"The heart of ML is learning from data. But who or what is learning? We can think of the model as the outcome of the learning process. For some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. This is exactly what a model does. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in the next unit.","title":"Learning"},{"location":"linear_algebra/1-ml_problem/#summary","text":"Regression is a classic ML problem where we use labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(A\\) . The labels are arranged in a label vector called \\(b\\) . A model is a function that transforms a feature-vector to a label.","title":"Summary"},{"location":"linear_algebra/2-linear_model/","text":"Linear Regression \u00b6 Question What is a linear regression model? Motivation \u00b6 Let us return to the housing dataset. Consider two houses, one which has \\(1000\\) square feet and the other which has \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows: $$ \\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant} $$ This is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature has an adverse effect on the selling-price, but is not as important as the area. Note We might be totally wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: Quote All models are wrong, but some are useful. Vector form \u00b6 Generalizing this, let us say that we have a feature vector \\(f\\) and a weight vector \\(w\\) . Recall that the housing data has six features: \\[ f = \\begin{bmatrix} f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] The function or model that maps a data-point to the label \\(b\\) (selling-price) is: \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + \\text{constant} \\] We can rewrite the constant as one more weight, say \\(w_0\\) : \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + w_0 \\] Going back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(w_0\\) to the weights: \\[ f = \\begin{bmatrix} 1\\\\ f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we now look at the expression for \\(b\\) , it is nothing but the dot-product of the two vectors: \\[ \\begin{aligned} b &= 1 \\cdot w_0 + f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6\\\\ &= f. w\\\\ \\end{aligned} \\] The dot-product can also be written as a \\(f^Tw\\) . The matrix-product of a row-vector and a column-vector: \\[ b = f^T w = \\begin{bmatrix} 1 & f_1 & f_2 & f_3 & f_4 & f_5 & f_6 \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] Note All vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(x^T\\) , where \\(x\\) is some column-vector. Matrix form \u00b6 So much for one house. But we have several houses. All these can be clubbed into a data-matrix. This is nothing but stacking all feature-vectors one below the other. Likewise, we can stack all selling prices into a label-vector: \\[ \\begin{bmatrix} b_{1}\\\\ \\vdots\\\\ b_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & f_{1,1} & f_{1,2} & f_{1,3} & f_{1,4} & f_{1,5} & f_{1,6}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & f_{100,1} & f_{100,2} & f_{100,3} & f_{100,4} & f_{100,5} & f_{100,6}\\\\ \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we call the feature matrix \\(A\\) , the label vector \\(b\\) and the weight vector \\(w\\) , this is the equation we have: \\[ Ax = b \\] We are given both \\(A\\) and \\(b\\) . This is nothing but our labeled dataset. We have to find \\(x\\) . This leaves us with two questions: Does the equation \\(Ax = b\\) have a solution? If it doesn't have a solution, then how do we estimate \\(x\\) ? We can see how a ML problem has now turned into a linear algebra problem! We will try to answer the first question and then move on to the second question. Note A lot of details about the linear regression model have been skipped. This presentation has tried to bring out the mathematical details. For a more accurate handling of this topic, please refer to week-2 of the MLT course. Summary \u00b6 A linear regression model assumes a linear relationship between inputs and outputs. This results in a system of linear equation of the form \\(Ax = b\\) .","title":"Linear Regression"},{"location":"linear_algebra/2-linear_model/#linear-regression","text":"Question What is a linear regression model?","title":"Linear Regression"},{"location":"linear_algebra/2-linear_model/#motivation","text":"Let us return to the housing dataset. Consider two houses, one which has \\(1000\\) square feet and the other which has \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows: $$ \\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant} $$ This is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature has an adverse effect on the selling-price, but is not as important as the area. Note We might be totally wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: Quote All models are wrong, but some are useful.","title":"Motivation"},{"location":"linear_algebra/2-linear_model/#vector-form","text":"Generalizing this, let us say that we have a feature vector \\(f\\) and a weight vector \\(w\\) . Recall that the housing data has six features: \\[ f = \\begin{bmatrix} f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] The function or model that maps a data-point to the label \\(b\\) (selling-price) is: \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + \\text{constant} \\] We can rewrite the constant as one more weight, say \\(w_0\\) : \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + w_0 \\] Going back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(w_0\\) to the weights: \\[ f = \\begin{bmatrix} 1\\\\ f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we now look at the expression for \\(b\\) , it is nothing but the dot-product of the two vectors: \\[ \\begin{aligned} b &= 1 \\cdot w_0 + f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6\\\\ &= f. w\\\\ \\end{aligned} \\] The dot-product can also be written as a \\(f^Tw\\) . The matrix-product of a row-vector and a column-vector: \\[ b = f^T w = \\begin{bmatrix} 1 & f_1 & f_2 & f_3 & f_4 & f_5 & f_6 \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] Note All vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(x^T\\) , where \\(x\\) is some column-vector.","title":"Vector form"},{"location":"linear_algebra/2-linear_model/#matrix-form","text":"So much for one house. But we have several houses. All these can be clubbed into a data-matrix. This is nothing but stacking all feature-vectors one below the other. Likewise, we can stack all selling prices into a label-vector: \\[ \\begin{bmatrix} b_{1}\\\\ \\vdots\\\\ b_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & f_{1,1} & f_{1,2} & f_{1,3} & f_{1,4} & f_{1,5} & f_{1,6}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & f_{100,1} & f_{100,2} & f_{100,3} & f_{100,4} & f_{100,5} & f_{100,6}\\\\ \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we call the feature matrix \\(A\\) , the label vector \\(b\\) and the weight vector \\(w\\) , this is the equation we have: \\[ Ax = b \\] We are given both \\(A\\) and \\(b\\) . This is nothing but our labeled dataset. We have to find \\(x\\) . This leaves us with two questions: Does the equation \\(Ax = b\\) have a solution? If it doesn't have a solution, then how do we estimate \\(x\\) ? We can see how a ML problem has now turned into a linear algebra problem! We will try to answer the first question and then move on to the second question. Note A lot of details about the linear regression model have been skipped. This presentation has tried to bring out the mathematical details. For a more accurate handling of this topic, please refer to week-2 of the MLT course.","title":"Matrix form"},{"location":"linear_algebra/2-linear_model/#summary","text":"A linear regression model assumes a linear relationship between inputs and outputs. This results in a system of linear equation of the form \\(Ax = b\\) .","title":"Summary"},{"location":"linear_algebra/3-system_1/","text":"Ax = 0 \u00b6 Question How do we solve for \\(x\\) in the equation \\(Ax = 0\\) ? Setting \u00b6 Before we tackle the general problem of \\(Ax = b\\) , let us first see if we can solve the system when \\(b\\) is the zero vector. In all the discussions that follow, this will be our setting: \\(A\\) is a matrix of dimensions \\(m \\times n\\) \\(x\\) is a column-vector of size \\(n\\) \\(x \\in \\mathbb{R}^n\\) \\(Ax \\in \\mathbb{R}^{m}\\) The equation that we have taken up is: $$ Ax = 0 $$ Note We have to be careful with the use of \\(0\\) . Depending on the context, it could either mean a scalar or a vector. Nullspace \u00b6 We can immediately see that \\(x = 0\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(x_1\\) . Then, we can see that \\(k x_1\\) is also a solution. This is because \\(A (kx_1) = k \\cdot Ax_1 = 0\\) . Also, if \\(x_1\\) and \\(x_2\\) are two solutions to the equation, then \\(x_1 + x_2\\) is also a solution, as \\(A(x_1 + x_2) = Ax_1 + Ax_2 = 0\\) . From these two observations, we see that the set of all solutions to the equation \\(Ax = 0\\) is a subspace of \\(\\mathbb{R}^{n}\\) . We denote this by \\(N(A)\\) and call it the nullspace of \\(A\\) . The dimension of the nullspace is called nullity. All this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B = \\{v_1, \\cdots, v_k\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(N(A) = \\text{span}(B)\\) . Row-Echelon form \u00b6 Let us take up an example and work with that: \\[ A = \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Recall that we can apply a sequence of any of these three row operations on a matrix: swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row Step-1 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Step-2 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\] Step-3 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\] The final matrix that we have is in row-echelon form. Here is a quick reminder of what the row-echelon matrix is: All rows that have only zeros are at the bottom. The first nonzero entry in a row is always to the right of the first nonzero entry in the row above it. The first nonzero entry in a row is called a pivot. Let us call the row-echelon matrix of \\(A\\) as \\(B\\) . We state the following result without a proof. Useful result \\(Bx = 0\\) if and only if \\(Ax = 0\\) Thus, the nullspace of a matrix and its row-echelon matrix are the same. This lets us forget about \\(A\\) and deal with its row-echelon form directly. Recipe for a Basis \u00b6 Now we have to solve the following equation: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\] Columns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called \"dependent variables\", while the others are called \"independent variables\". Algorithm \\(B = \\{ \\}\\) For each independent variable \\(x_i\\) : Set \\(x_i = 1\\) and \\(x_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(x\\) to \\(B\\) \\(B\\) is the required basis. Let us try it out here. \\(x_1\\) and \\(x_2\\) are the dependent variables. \\(x_3\\) and \\(x_4\\) are the independent variables. First, let us set \\(x_3 = 1, x_4 = 0\\) . This gives us \\(x_1 = 1, x_2 = -2\\) . The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^T\\) . Next, we set \\(x_3 = 0, x_4 = 1\\) . This gives us \\(x_1 = 0, x_2 = -1\\) . The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^T\\) . Thus, the basis for \\(N(A)\\) is: \\[ B = \\left \\{ \\begin{bmatrix}1\\\\ -2\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\\\ -1\\\\ 0\\\\ 1\\end{bmatrix} \\right \\} \\] The set of all solutions for the equation \\(Ax = 0\\) is \\(\\text{span}(B)\\) . Summary \u00b6 In order to solve the equation \\(Ax = 0\\) , we first reduce the matrix \\(A\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the null space. The span of the basis is the set of all solutions to this equation.","title":"Ax = 0"},{"location":"linear_algebra/3-system_1/#ax-0","text":"Question How do we solve for \\(x\\) in the equation \\(Ax = 0\\) ?","title":"Ax = 0"},{"location":"linear_algebra/3-system_1/#setting","text":"Before we tackle the general problem of \\(Ax = b\\) , let us first see if we can solve the system when \\(b\\) is the zero vector. In all the discussions that follow, this will be our setting: \\(A\\) is a matrix of dimensions \\(m \\times n\\) \\(x\\) is a column-vector of size \\(n\\) \\(x \\in \\mathbb{R}^n\\) \\(Ax \\in \\mathbb{R}^{m}\\) The equation that we have taken up is: $$ Ax = 0 $$ Note We have to be careful with the use of \\(0\\) . Depending on the context, it could either mean a scalar or a vector.","title":"Setting"},{"location":"linear_algebra/3-system_1/#nullspace","text":"We can immediately see that \\(x = 0\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(x_1\\) . Then, we can see that \\(k x_1\\) is also a solution. This is because \\(A (kx_1) = k \\cdot Ax_1 = 0\\) . Also, if \\(x_1\\) and \\(x_2\\) are two solutions to the equation, then \\(x_1 + x_2\\) is also a solution, as \\(A(x_1 + x_2) = Ax_1 + Ax_2 = 0\\) . From these two observations, we see that the set of all solutions to the equation \\(Ax = 0\\) is a subspace of \\(\\mathbb{R}^{n}\\) . We denote this by \\(N(A)\\) and call it the nullspace of \\(A\\) . The dimension of the nullspace is called nullity. All this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B = \\{v_1, \\cdots, v_k\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(N(A) = \\text{span}(B)\\) .","title":"Nullspace"},{"location":"linear_algebra/3-system_1/#row-echelon-form","text":"Let us take up an example and work with that: \\[ A = \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Recall that we can apply a sequence of any of these three row operations on a matrix: swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row Step-1 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Step-2 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\] Step-3 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\] The final matrix that we have is in row-echelon form. Here is a quick reminder of what the row-echelon matrix is: All rows that have only zeros are at the bottom. The first nonzero entry in a row is always to the right of the first nonzero entry in the row above it. The first nonzero entry in a row is called a pivot. Let us call the row-echelon matrix of \\(A\\) as \\(B\\) . We state the following result without a proof. Useful result \\(Bx = 0\\) if and only if \\(Ax = 0\\) Thus, the nullspace of a matrix and its row-echelon matrix are the same. This lets us forget about \\(A\\) and deal with its row-echelon form directly.","title":"Row-Echelon form"},{"location":"linear_algebra/3-system_1/#recipe-for-a-basis","text":"Now we have to solve the following equation: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\] Columns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called \"dependent variables\", while the others are called \"independent variables\". Algorithm \\(B = \\{ \\}\\) For each independent variable \\(x_i\\) : Set \\(x_i = 1\\) and \\(x_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(x\\) to \\(B\\) \\(B\\) is the required basis. Let us try it out here. \\(x_1\\) and \\(x_2\\) are the dependent variables. \\(x_3\\) and \\(x_4\\) are the independent variables. First, let us set \\(x_3 = 1, x_4 = 0\\) . This gives us \\(x_1 = 1, x_2 = -2\\) . The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^T\\) . Next, we set \\(x_3 = 0, x_4 = 1\\) . This gives us \\(x_1 = 0, x_2 = -1\\) . The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^T\\) . Thus, the basis for \\(N(A)\\) is: \\[ B = \\left \\{ \\begin{bmatrix}1\\\\ -2\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\\\ -1\\\\ 0\\\\ 1\\end{bmatrix} \\right \\} \\] The set of all solutions for the equation \\(Ax = 0\\) is \\(\\text{span}(B)\\) .","title":"Recipe for a Basis"},{"location":"linear_algebra/3-system_1/#summary","text":"In order to solve the equation \\(Ax = 0\\) , we first reduce the matrix \\(A\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the null space. The span of the basis is the set of all solutions to this equation.","title":"Summary"},{"location":"linear_algebra/4-system_2/","text":"Ax = b \u00b6 Question How do we solve for \\(x\\) in the equation \\(Ax = b\\) ? Column space \u00b6 Now we come to the general form of the equation, \\(Ax = b\\) . First, we need to know if the equation admits any solution at all. For this, we have to take a closer look at this equation and see what it means: \\[ \\begin{bmatrix} \\vdots & \\vdots & \\vdots\\\\ a_1 & \\cdots & a_n\\\\ \\vdots & \\vdots & \\vdots \\end{bmatrix} \\begin{bmatrix} x_1\\\\ \\vdots\\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_m \\end{bmatrix} \\] Here, \\(a_1, \\cdots, a_n\\) are the columns of \\(A\\) . Recall that the product of a matrix and a vector can be interpreted as a linear combination of the columns of the matrix: \\[ x_1 a_1 + \\cdots x_na_n=b \\] \\(Ax = b\\) has a solution if and only if \\(b\\) can be expressed as a linear combination of the columns of \\(A\\) . Since the set of all linear combination of \\(A\\) is given by the \\(\\text{span}(\\{a_1, \\cdots, a_n\\})\\) , the equation is solvable if and only if \\(b \\in \\text{span}(\\{a_1, \\cdots, a_n\\})\\) . The span of the columns of \\(A\\) is a subspace and is called the column space of the matrix \\(A\\) . Thus, \\(C(A)=\\text{span}(\\{a_1, \\cdots, a_n\\})\\) . We have now answered the question of when \\(Ax = b\\) is solvable. Note The dimension of the column space is called the rank. You must be aware of the rank-nullity theorem: $$ \\text{rank} + \\text{nullity} = n $$ Conditions for Solution \u00b6 Let us reuse the example from the previous unit: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3 \\end{bmatrix} \\] We are back to the row-echelon form, but with the augmented matrix: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\vert & b_1\\\\ 2 & 1 & 0 & 1 & \\vert & b_2 \\\\ 3 & 1 & -1 & 1 & \\vert & b_3 \\end{bmatrix} \\] If we apply the same sequence of row operations as in the previous case, we get: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\vert & b_1\\\\ 0 & 1 & 2 & 1 & \\vert & b_2 - 2b_1 \\\\ 0 & 0 & 0 & 0 & \\vert & b_3 - b_1 - b_2 \\end{bmatrix} \\] We can immediately see that the system has a solution if and only if the following condition is met: \\[ b_3 - b_1 - b_2 = 0 \\] Now that we have the row-echelon matrix, let us rephrase the equation as follows: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ b_2 - 2b_1\\\\ b_3 - b_1 - b_2 \\end{bmatrix} \\] Let us call this system \\(Bx = c\\) . Note \\(x\\) is a solution to \\(Ax = b\\) if and only if \\(x\\) is a solution to \\(Bx = c\\) General Solution \u00b6 If a solution exists, how do we find it? And what about all possible solutions? First note that the set of pivot columns are linearly independent and form a basis for the column space of \\(B\\) . In the example we are working with, this is quite clear: \\[ C(B) = \\text{span}\\left( \\left\\{\\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\right\\} \\right) \\] So, \\(b\\) can be uniquely expressed as a linear combination of the columns of this basis. Let us call this particular solution \\(x_p\\) . If \\(x_n\\) is some vector in the nullspace of \\(A\\) , then every general solution \\(x_g\\) to the equation can be expressed in this form: $$ x_g = x_p + x_n $$ To see why this might be true, just pre-multiply both sides by \\(A\\) : \\[ Ax_g = A(x_p + x_n) = Ax_p + Ax_n = b \\] This not a complete proof. We have only shown that \\(x_p + x_n\\) is a solution. It may still not be clear why every solution should be of this form. But we will skip this argument in this course. Coming back to the example we are working with, how do we find the particular solution \\(x_p\\) ? We set all independent variables to zero and solve for the dependent variables: \\[ x_p = \\begin{bmatrix} b_1\\\\ b_2 - 2b_1\\\\ 0\\\\ 0 \\end{bmatrix} \\] We already know how to get \\(x_n\\) . Refer to the previous unit on computing a basis for the nullspace of the matrix. Summary \u00b6 \\(Ax = b\\) is solvable if and only if \\(b\\) is an element of the column space of \\(A\\) . If this is true, then the general solution to the equation can be expressed as \\(x_p + x_n\\) , where \\(x_p\\) is a particular solution and \\(x_n\\) is some vector in the nullspace of \\(A\\) .","title":"Ax = b"},{"location":"linear_algebra/4-system_2/#ax-b","text":"Question How do we solve for \\(x\\) in the equation \\(Ax = b\\) ?","title":"Ax = b"},{"location":"linear_algebra/4-system_2/#column-space","text":"Now we come to the general form of the equation, \\(Ax = b\\) . First, we need to know if the equation admits any solution at all. For this, we have to take a closer look at this equation and see what it means: \\[ \\begin{bmatrix} \\vdots & \\vdots & \\vdots\\\\ a_1 & \\cdots & a_n\\\\ \\vdots & \\vdots & \\vdots \\end{bmatrix} \\begin{bmatrix} x_1\\\\ \\vdots\\\\ x_n \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_m \\end{bmatrix} \\] Here, \\(a_1, \\cdots, a_n\\) are the columns of \\(A\\) . Recall that the product of a matrix and a vector can be interpreted as a linear combination of the columns of the matrix: \\[ x_1 a_1 + \\cdots x_na_n=b \\] \\(Ax = b\\) has a solution if and only if \\(b\\) can be expressed as a linear combination of the columns of \\(A\\) . Since the set of all linear combination of \\(A\\) is given by the \\(\\text{span}(\\{a_1, \\cdots, a_n\\})\\) , the equation is solvable if and only if \\(b \\in \\text{span}(\\{a_1, \\cdots, a_n\\})\\) . The span of the columns of \\(A\\) is a subspace and is called the column space of the matrix \\(A\\) . Thus, \\(C(A)=\\text{span}(\\{a_1, \\cdots, a_n\\})\\) . We have now answered the question of when \\(Ax = b\\) is solvable. Note The dimension of the column space is called the rank. You must be aware of the rank-nullity theorem: $$ \\text{rank} + \\text{nullity} = n $$","title":"Column space"},{"location":"linear_algebra/4-system_2/#conditions-for-solution","text":"Let us reuse the example from the previous unit: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3 \\end{bmatrix} \\] We are back to the row-echelon form, but with the augmented matrix: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\vert & b_1\\\\ 2 & 1 & 0 & 1 & \\vert & b_2 \\\\ 3 & 1 & -1 & 1 & \\vert & b_3 \\end{bmatrix} \\] If we apply the same sequence of row operations as in the previous case, we get: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0 & \\vert & b_1\\\\ 0 & 1 & 2 & 1 & \\vert & b_2 - 2b_1 \\\\ 0 & 0 & 0 & 0 & \\vert & b_3 - b_1 - b_2 \\end{bmatrix} \\] We can immediately see that the system has a solution if and only if the following condition is met: \\[ b_3 - b_1 - b_2 = 0 \\] Now that we have the row-echelon matrix, let us rephrase the equation as follows: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} b_1\\\\ b_2 - 2b_1\\\\ b_3 - b_1 - b_2 \\end{bmatrix} \\] Let us call this system \\(Bx = c\\) . Note \\(x\\) is a solution to \\(Ax = b\\) if and only if \\(x\\) is a solution to \\(Bx = c\\)","title":"Conditions for Solution"},{"location":"linear_algebra/4-system_2/#general-solution","text":"If a solution exists, how do we find it? And what about all possible solutions? First note that the set of pivot columns are linearly independent and form a basis for the column space of \\(B\\) . In the example we are working with, this is quite clear: \\[ C(B) = \\text{span}\\left( \\left\\{\\begin{bmatrix} 1\\\\ 0\\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix} \\right\\} \\right) \\] So, \\(b\\) can be uniquely expressed as a linear combination of the columns of this basis. Let us call this particular solution \\(x_p\\) . If \\(x_n\\) is some vector in the nullspace of \\(A\\) , then every general solution \\(x_g\\) to the equation can be expressed in this form: $$ x_g = x_p + x_n $$ To see why this might be true, just pre-multiply both sides by \\(A\\) : \\[ Ax_g = A(x_p + x_n) = Ax_p + Ax_n = b \\] This not a complete proof. We have only shown that \\(x_p + x_n\\) is a solution. It may still not be clear why every solution should be of this form. But we will skip this argument in this course. Coming back to the example we are working with, how do we find the particular solution \\(x_p\\) ? We set all independent variables to zero and solve for the dependent variables: \\[ x_p = \\begin{bmatrix} b_1\\\\ b_2 - 2b_1\\\\ 0\\\\ 0 \\end{bmatrix} \\] We already know how to get \\(x_n\\) . Refer to the previous unit on computing a basis for the nullspace of the matrix.","title":"General Solution"},{"location":"linear_algebra/4-system_2/#summary","text":"\\(Ax = b\\) is solvable if and only if \\(b\\) is an element of the column space of \\(A\\) . If this is true, then the general solution to the equation can be expressed as \\(x_p + x_n\\) , where \\(x_p\\) is a particular solution and \\(x_n\\) is some vector in the nullspace of \\(A\\) .","title":"Summary"},{"location":"linear_algebra/5-least_squares/","text":"Ax \u2248 b \u00b6 Question How do we solve for \\(x\\) in \\(Ax \\approx b\\) ? Approximation \u00b6 So far we have seen the well behaved case of \\(Ax = b\\) . What if \\(b\\) is not in the column space of \\(A\\) ? Then \\(Ax \\neq b\\) . This is by far the most interesting and relevant scenario from the perspective of the linear regression problem. Recall that we are trying to estimate a weight vector \\(x\\) , given the labeled dataset \\((A, b)\\) . The relationship between inputs and outputs is assumed to be linear. But the real world doesn't behave exactly like we want it to. There are going to be obvious deviations from our ideal assumptions. In other words, our models can never be an entirely accurate representation of reality. This is a typical scenario observed in engineering and the sciences. We don't abandon the problem because it doesn't admit an exact solution. Instead, we turn to a powerful weapon in our armoury: approximations. Can we find a \\(\\hat{x}\\) such that \\(A \\hat{x} \\approx b\\) ? What do we mean by the symbol \\(\\approx\\) here? We are dealing with two vectors on either side of the symbol. Let us first understand what the symbol means when we have scalars. Let us look at the following statements: \\(1.234 \\approx 1\\) \\(1.234 \\approx 1.2\\) \\(1.234 \\approx 1.23\\) These are three approximations. From experience, we know that the second approximation is better than the first, the third better than the second. If we plot these points on a real line, the goodness of an approximation can be interpreted using the distance between the original point and its approximation: smaller the distance, better the approximation. This very idea is extended to an \\(n\\) dimensional vector space. If \\(b\\) and \\(\\hat{b}\\) are two vectors in \\(\\mathbb{R}^m\\) , then the distance \\(d\\) between them is a measure of the goodness of the approximation. To avoid taking square roots, we write down the expression for \\(d^2\\) : \\[ d^2 = ||\\hat{b} - b||^2 = (\\hat{b}_1 - b_1)^2 + \\cdots + (\\hat{b}_m - b_m)^2 \\] So, the key to solving the problem \\(Ax \\approx b\\) , is to find a vector \\(\\hat{x}\\) such that \\(A\\hat{x}\\) is as close to \\(b\\) as possible. This can be framed as an optimization problem: \\[ \\hat{x} = \\arg \\min \\limits_{x} ||Ax - b||^2 \\] If you are seeing \\(\\arg \\min\\) for the first time, think about it like a function (in the programming sense): Find the value the minimizes the expression Return this value to the user Objective function \u00b6 Let us go ahead and solve the minimization problem. \\[ \\min \\limits_{x} ||Ax - b||^2 \\] Let us call the expression to be minimized, \\(E\\) , this is also called the objective function of the optimization problem. It can be simplified as follows: \\[ \\begin{align} E &= ||Ax - b||^2\\\\\\\\ &= (Ax - b)^T(Ax - b)\\\\\\\\ \\end{align} \\] Normal Equations \u00b6 We now follow the usual procedure of finding the minima of a function. Take the derivatives and set them to zero. Since we are dealing with a multivariable function, we have to consider all the partial derivatives. The vector of such partial derivatives is called the gradient. \\[ \\nabla E = \\begin{bmatrix} \\cfrac{\\partial E}{\\partial x_1}\\\\ \\vdots\\\\ \\cfrac{\\partial E}{\\partial x_n} \\end{bmatrix} \\] Let us now compute the gradient and set it to zero. If you want a detailed mathematical explanation of how the gradient is computed, refer to the appendix : \\[ \\nabla E = 2(A^TA)x - 2A^Tb = 0 \\] This gives us the following equation: $$ (A^TA) x = A^Tb $$ This system is called the normal equations . If the matrix \\(A^TA\\) is invertible, then we have the following solution: \\[ \\hat{x} = (A^TA)^{-1} A^Tb \\] We still don't know if this is corresponds to a minima. Rest assured that this is indeed a minima. But we won't take up the proof now. The other worrying part is what happens if the matrix \\(A^TA\\) is singular or non-invertible. This is also something that we skip for the time being.","title":"Ax \u2248 b"},{"location":"linear_algebra/5-least_squares/#ax-b","text":"Question How do we solve for \\(x\\) in \\(Ax \\approx b\\) ?","title":"Ax \u2248 b"},{"location":"linear_algebra/5-least_squares/#approximation","text":"So far we have seen the well behaved case of \\(Ax = b\\) . What if \\(b\\) is not in the column space of \\(A\\) ? Then \\(Ax \\neq b\\) . This is by far the most interesting and relevant scenario from the perspective of the linear regression problem. Recall that we are trying to estimate a weight vector \\(x\\) , given the labeled dataset \\((A, b)\\) . The relationship between inputs and outputs is assumed to be linear. But the real world doesn't behave exactly like we want it to. There are going to be obvious deviations from our ideal assumptions. In other words, our models can never be an entirely accurate representation of reality. This is a typical scenario observed in engineering and the sciences. We don't abandon the problem because it doesn't admit an exact solution. Instead, we turn to a powerful weapon in our armoury: approximations. Can we find a \\(\\hat{x}\\) such that \\(A \\hat{x} \\approx b\\) ? What do we mean by the symbol \\(\\approx\\) here? We are dealing with two vectors on either side of the symbol. Let us first understand what the symbol means when we have scalars. Let us look at the following statements: \\(1.234 \\approx 1\\) \\(1.234 \\approx 1.2\\) \\(1.234 \\approx 1.23\\) These are three approximations. From experience, we know that the second approximation is better than the first, the third better than the second. If we plot these points on a real line, the goodness of an approximation can be interpreted using the distance between the original point and its approximation: smaller the distance, better the approximation. This very idea is extended to an \\(n\\) dimensional vector space. If \\(b\\) and \\(\\hat{b}\\) are two vectors in \\(\\mathbb{R}^m\\) , then the distance \\(d\\) between them is a measure of the goodness of the approximation. To avoid taking square roots, we write down the expression for \\(d^2\\) : \\[ d^2 = ||\\hat{b} - b||^2 = (\\hat{b}_1 - b_1)^2 + \\cdots + (\\hat{b}_m - b_m)^2 \\] So, the key to solving the problem \\(Ax \\approx b\\) , is to find a vector \\(\\hat{x}\\) such that \\(A\\hat{x}\\) is as close to \\(b\\) as possible. This can be framed as an optimization problem: \\[ \\hat{x} = \\arg \\min \\limits_{x} ||Ax - b||^2 \\] If you are seeing \\(\\arg \\min\\) for the first time, think about it like a function (in the programming sense): Find the value the minimizes the expression Return this value to the user","title":"Approximation"},{"location":"linear_algebra/5-least_squares/#objective-function","text":"Let us go ahead and solve the minimization problem. \\[ \\min \\limits_{x} ||Ax - b||^2 \\] Let us call the expression to be minimized, \\(E\\) , this is also called the objective function of the optimization problem. It can be simplified as follows: \\[ \\begin{align} E &= ||Ax - b||^2\\\\\\\\ &= (Ax - b)^T(Ax - b)\\\\\\\\ \\end{align} \\]","title":"Objective function"},{"location":"linear_algebra/5-least_squares/#normal-equations","text":"We now follow the usual procedure of finding the minima of a function. Take the derivatives and set them to zero. Since we are dealing with a multivariable function, we have to consider all the partial derivatives. The vector of such partial derivatives is called the gradient. \\[ \\nabla E = \\begin{bmatrix} \\cfrac{\\partial E}{\\partial x_1}\\\\ \\vdots\\\\ \\cfrac{\\partial E}{\\partial x_n} \\end{bmatrix} \\] Let us now compute the gradient and set it to zero. If you want a detailed mathematical explanation of how the gradient is computed, refer to the appendix : \\[ \\nabla E = 2(A^TA)x - 2A^Tb = 0 \\] This gives us the following equation: $$ (A^TA) x = A^Tb $$ This system is called the normal equations . If the matrix \\(A^TA\\) is invertible, then we have the following solution: \\[ \\hat{x} = (A^TA)^{-1} A^Tb \\] We still don't know if this is corresponds to a minima. Rest assured that this is indeed a minima. But we won't take up the proof now. The other worrying part is what happens if the matrix \\(A^TA\\) is singular or non-invertible. This is also something that we skip for the time being.","title":"Normal Equations"}]}