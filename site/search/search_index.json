{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning Foundations \u00b6 Notes for the MLF course","title":"Machine Learning Foundations"},{"location":"#machine-learning-foundations","text":"Notes for the MLF course","title":"Machine Learning Foundations"},{"location":"week-3/0-linear_algebra/","text":"Linear Algebra for ML \u00b6 Question Why should we study linear algebra in a course on data science? Data \u00b6 The simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it: lattitude longitue age num_of_rooms area distance_from_school A typical problem in ML is this: Given these attributes or features of a house, can we predict its selling price? This is called a regression problem : given a set of features, map it to a real number. Vectors \u00b6 Let us take a concrete example of a single data-point : Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 num_of_rooms 2 area 1000 distance_from_nearest_school 3 selling_price 40 Lattitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs. But none of these really matter for an ML algorithm: it is going to abstract out the details (such as units) and look at this as a column of numbers, which is nothing but a vector: \\[ \\begin{bmatrix} 12.9\\\\ 80.2\\\\ 3\\\\ 2\\\\ 1000\\\\ 3\\\\ \\end{bmatrix} \\] Note that the selling price is not included as an element in the vector as that is usually unkown to us. This unkown quantity which we have to estimate or predict is called the target . Note Vectors are usually represented as column vectors or \\(n \\times 1\\) matrices. Matrices \u00b6 We cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tablular form: lattitude longitude age rooms area distance price 1 12.9 80.2 3 2 1000 3 40 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 50 14.3 75.9 30 2 1200 5 20 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 100 20.8 90.5 1 3 1500 2 35 This data for \\(100\\) houses is nothing but a \\(100 \\times 6\\) matrix: \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] Each row of this matrix corresponds to one data-point. In general, if a dataset has \\(n\\) features and \\(m\\) data-points, it is represented as a \\(m \\times n\\) data-matrix. Tip If you find yourself lost when working with matrices, remember that a matrix is a way represent a collection of data-points (dataset). Summary \u00b6 Data is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra \u2014 the study of vectors and matrices \u2014 if we wish to understand how ML algorithms work.","title":"Linear Algebra for ML"},{"location":"week-3/0-linear_algebra/#linear-algebra-for-ml","text":"Question Why should we study linear algebra in a course on data science?","title":"Linear Algebra for ML"},{"location":"week-3/0-linear_algebra/#data","text":"The simplest answer to this question is that we choose vectors and matrices to represent data. Let us take a simple example of data coming from the housing industry. Every house in a locality has the following features or attributes associated with it: lattitude longitue age num_of_rooms area distance_from_school A typical problem in ML is this: Given these attributes or features of a house, can we predict its selling price? This is called a regression problem : given a set of features, map it to a real number.","title":"Data"},{"location":"week-3/0-linear_algebra/#vectors","text":"Let us take a concrete example of a single data-point : Attributes/Target Values lattitude 12.9 longitude 80.2 age 3 num_of_rooms 2 area 1000 distance_from_nearest_school 3 selling_price 40 Lattitude and longitude are in degrees, age is in years, area is in square feet, distance is in kilometers, selling price is in lakhs. But none of these really matter for an ML algorithm: it is going to abstract out the details (such as units) and look at this as a column of numbers, which is nothing but a vector: \\[ \\begin{bmatrix} 12.9\\\\ 80.2\\\\ 3\\\\ 2\\\\ 1000\\\\ 3\\\\ \\end{bmatrix} \\] Note that the selling price is not included as an element in the vector as that is usually unkown to us. This unkown quantity which we have to estimate or predict is called the target . Note Vectors are usually represented as column vectors or \\(n \\times 1\\) matrices.","title":"Vectors"},{"location":"week-3/0-linear_algebra/#matrices","text":"We cannot learn anything substantial from looking at the data of one house. We have to look at the data of multiple houses. That will give us an idea of the general picture. Rather than look at each vector in isolation, we can arrange them in a tablular form: lattitude longitude age rooms area distance price 1 12.9 80.2 3 2 1000 3 40 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 50 14.3 75.9 30 2 1200 5 20 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 \u2026 100 20.8 90.5 1 3 1500 2 35 This data for \\(100\\) houses is nothing but a \\(100 \\times 6\\) matrix: \\[ \\begin{bmatrix} 12.9 & 80.2 & 3 & 2 & 1000 & 3\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 14.3 & 75.9 & 30 & 2 & 1200 & 5\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 20.8 & 90.5 & 1 & 3 & 1500 & 2 \\end{bmatrix} \\] Each row of this matrix corresponds to one data-point. In general, if a dataset has \\(n\\) features and \\(m\\) data-points, it is represented as a \\(m \\times n\\) data-matrix. Tip If you find yourself lost when working with matrices, remember that a matrix is a way represent a collection of data-points (dataset).","title":"Matrices"},{"location":"week-3/0-linear_algebra/#summary","text":"Data is represented as vectors and matrices. If we wish to extract insights from data, we need to know how to manipulate vectors and matrices. Therefore, we need to have a reasonable understanding of linear algebra \u2014 the study of vectors and matrices \u2014 if we wish to understand how ML algorithms work.","title":"Summary"},{"location":"week-3/1-ml_problem/","text":"ML problem \u00b6 Question What does a typical ML problem look like? Regression \u00b6 Analogy \u00b6 Think about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and learns how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers: \\(103 + 205 = 308\\) \\(123 + 409 = 532\\) \\(185 + 483 = 668\\) The important point to note is that the student has access to both the questions and the answers. In the exam, she will not have access to the answers! But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. In other words, she would have to learn a function from the input (question) to the output (answer). This is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels . A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset . A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy. Data Representation \u00b6 We are given a collection of \\(m\\) data-points and \\(m\\) labels which are real numbers. Each data-point is described by \\(n\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as lattitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(n\\) . Arranging the \\(m\\) data-points in a matrix, we get a \\(m \\times n\\) data-matrix. Let us call this matrix \\(A\\) : \\[ A = \\begin{bmatrix} a_{11} & \\cdots & a_{1n}\\\\ \\vdots & a_{ij} & \\vdots\\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\] The labels can be put together in a vector of size \\(n\\) . Let us call this \\(b\\) : \\[ b = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_m \\end{bmatrix} \\] Model \u00b6 As stated earlier, a regression model is a function that transforms a data-point into a label. Formally: $$ f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} $$ Each feature-vector is of size \\(n\\) . So, the feature-vectors reside in the \\(n\\) dimensional space \\(\\mathbb{R}^{n}\\) . The labels are real numbers, so they reside in \\(\\mathbb{R}\\) . Tip You can think about an ML model as a function that maps a feature-vector to a label. Learning \u00b6 The heart of ML is learning from data. But who or what is learning? We can think of the model as the outcome of the learning process. For some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. This is exactly what a model does. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in the next unit. Summary \u00b6 Regression is a classic ML problem where we use labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(A\\) . The labels are arranged in a label vector called \\(b\\) . A model is a function that transforms a feature-vector to a label.","title":"ML problem"},{"location":"week-3/1-ml_problem/#ml-problem","text":"Question What does a typical ML problem look like?","title":"ML problem"},{"location":"week-3/1-ml_problem/#regression","text":"","title":"Regression"},{"location":"week-3/1-ml_problem/#analogy","text":"Think about arithmetic classes in primary school. During the class hours, a student looks at solved examples in a textbook and learns how to solve simple three digit addition problems. Let us say that her textbook has the following problems along with the answers: \\(103 + 205 = 308\\) \\(123 + 409 = 532\\) \\(185 + 483 = 668\\) The important point to note is that the student has access to both the questions and the answers. In the exam, she will not have access to the answers! But more importantly, she will not even be asked the same questions! So, just memorizing the answers will not help. She would have to learn how addition works. In other words, she would have to learn a function from the input (question) to the output (answer). This is exactly what happens in a regression problem. The inputs are a set of data-points. The outputs corresponding to these inputs are real numbers called targets or labels . A regression model has to learn the mapping from input to output. Once this mapping or function is learnt, the model can then be used to predict the output on unseen inputs. The collection of data-points along with their targets is called a labeled dataset . A regression model makes use of this dataset to learn a function. A labeled dataset is nothing but the textbook problems in our analogy.","title":"Analogy"},{"location":"week-3/1-ml_problem/#data-representation","text":"We are given a collection of \\(m\\) data-points and \\(m\\) labels which are real numbers. Each data-point is described by \\(n\\) features. For example, in the housing dataset, each house is a data-point and is described by features such as lattitude, longitude, area and so on. These features are clubbed together in a feature-vector of size \\(n\\) . Arranging the \\(m\\) data-points in a matrix, we get a \\(m \\times n\\) data-matrix. Let us call this matrix \\(A\\) : \\[ A = \\begin{bmatrix} a_{11} & \\cdots & a_{1n}\\\\ \\vdots & a_{ij} & \\vdots\\\\ a_{m1} & \\cdots & a_{mn} \\end{bmatrix} \\] The labels can be put together in a vector of size \\(n\\) . Let us call this \\(b\\) : \\[ b = \\begin{bmatrix} b_1\\\\ \\vdots\\\\ b_m \\end{bmatrix} \\]","title":"Data Representation"},{"location":"week-3/1-ml_problem/#model","text":"As stated earlier, a regression model is a function that transforms a data-point into a label. Formally: $$ f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R} $$ Each feature-vector is of size \\(n\\) . So, the feature-vectors reside in the \\(n\\) dimensional space \\(\\mathbb{R}^{n}\\) . The labels are real numbers, so they reside in \\(\\mathbb{R}\\) . Tip You can think about an ML model as a function that maps a feature-vector to a label.","title":"Model"},{"location":"week-3/1-ml_problem/#learning","text":"The heart of ML is learning from data. But who or what is learning? We can think of the model as the outcome of the learning process. For some regression models, once you have learnt the model, you can throw away the dataset (textbook). This is not true of all regression models though! Think about how you learnt three-digit addition. Do you still carry your primary school textbooks around? No! Your mind has a representation of what addition is. This is exactly what a model does. ML scientists have come up with a variety of models. The simplest such model is a linear model. We shall take this up in the next unit.","title":"Learning"},{"location":"week-3/1-ml_problem/#summary","text":"Regression is a classic ML problem where we use labeled data to learn a mapping from a feature-vector to a real number. The data-points are arranged in a data-matrix called \\(A\\) . The labels are arranged in a label vector called \\(b\\) . A model is a function that transforms a feature-vector to a label.","title":"Summary"},{"location":"week-3/2-linear_model/","text":"Linear Regression \u00b6 Question What is a linear regression model? Motivation \u00b6 Let us return to the housing dataset. Consider two houses, one which has \\(1000\\) square feet and the other which has \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows: $$ \\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant} $$ This is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature has an adverse effect on the selling-price, but is not as important as the area. Note We might be totally wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: Quote All models are wrong, but some are useful. Vector form \u00b6 Generalizing this, let us say that we have a feature vector \\(f\\) and a weight vector \\(w\\) . Recall that the housing data has six features: \\[ f = \\begin{bmatrix} f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] The function or model that maps a data-point to the label \\(b\\) (selling-price) is: \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + \\text{constant} \\] We can rewrite the constant as one more weight, say \\(w_0\\) : \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + w_0 \\] Going back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(w_0\\) to the weights: \\[ f = \\begin{bmatrix} 1\\\\ f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we now look at the expression for \\(b\\) , it is nothing but the dot-product of the two vectors: \\[ \\begin{aligned} b &= 1 \\cdot w_0 + f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6\\\\ &= f. w\\\\ \\end{aligned} \\] The dot-product can also be written as a \\(f^Tw\\) . The matrix-product of a row-vector and a column-vector: \\[ b = f^T w = \\begin{bmatrix} 1 & f_1 & f_2 & f_3 & f_4 & f_5 & f_6 \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] Note All vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(x^T\\) , where \\(x\\) is some column-vector. Matrix form \u00b6 So much for one house. But we have several houses. All these can be clubbed into a data-matrix. This is nothing but stacking all feature-vectors one below the other. Likewise, we can stack all selling prices into a label-vector: \\[ \\begin{bmatrix} b_{1}\\\\ \\vdots\\\\ b_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & f_{1,1} & f_{1,2} & f_{1,3} & f_{1,4} & f_{1,5} & f_{1,6}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & f_{100,1} & f_{100,2} & f_{100,3} & f_{100,4} & f_{100,5} & f_{100,6}\\\\ \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we call the feature matrix \\(A\\) , the label vector \\(b\\) and the weight vector \\(w\\) , this is the equation we have: \\[ Ax = b \\] We are given both \\(A\\) and \\(b\\) . This is nothing but our labeled dataset. We have to find \\(x\\) . This leaves us with two questions: Does the equation \\(Ax = b\\) have a solution? If it doesn't have a solution, then how do we estimate \\(x\\) ? We can see how a ML problem has now turned into a linear algebra problem! We will try to answer the first question and then move on to the second question. Note A lot of details about the linear regression model have been skipped. This presentation has tried to bring out the mathematical details. For a more accurate handling of this topic, please refer to week-2 of the MLT course. Summary \u00b6 A linear regression model assumes a linear relationship between inputs and outputs. This results in a system of linear equation of the form \\(Ax = b\\) .","title":"Linear Regression"},{"location":"week-3/2-linear_model/#linear-regression","text":"Question What is a linear regression model?","title":"Linear Regression"},{"location":"week-3/2-linear_model/#motivation","text":"Let us return to the housing dataset. Consider two houses, one which has \\(1000\\) square feet and the other which has \\(2000\\) square feet. As the area of the house increases, the selling price is going to go up. Take another feature, say the distance from the nearest school. If the distance increases, then the selling price goes down. Perhaps the effect may not be as drastic. If we have access to only these two features, one function or model could be as follows: $$ \\text{Selling-price} = 2 \\times \\text{Area} - 0.2 \\times \\text{Distance} + \\text{Constant} $$ This is what is called a linear model. The values \\(2\\) and \\(-0.2\\) are called the weights. The magnitude of a weight denotes the importance of the corresponding feature. Its sign denotes the effect it has on the output. For example, the distance feature has an adverse effect on the selling-price, but is not as important as the area. Note We might be totally wrong about the choice of weights. Even worse, the relationship between the selling price and the two features may not even be linear! It is important to understand that a model is some approximation of the underlying reality. There is a nice quote that summarizes this idea: Quote All models are wrong, but some are useful.","title":"Motivation"},{"location":"week-3/2-linear_model/#vector-form","text":"Generalizing this, let us say that we have a feature vector \\(f\\) and a weight vector \\(w\\) . Recall that the housing data has six features: \\[ f = \\begin{bmatrix} f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] The function or model that maps a data-point to the label \\(b\\) (selling-price) is: \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + \\text{constant} \\] We can rewrite the constant as one more weight, say \\(w_0\\) : \\[ b = f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6 + w_0 \\] Going back to the vectors, we add a feature \\(1\\) to the feature vector and a \\(w_0\\) to the weights: \\[ f = \\begin{bmatrix} 1\\\\ f_1\\\\ f_2\\\\ f_3\\\\ f_4\\\\ f_5\\\\ f_6 \\end{bmatrix}, w = \\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we now look at the expression for \\(b\\) , it is nothing but the dot-product of the two vectors: \\[ \\begin{aligned} b &= 1 \\cdot w_0 + f_1w_1 + f_2w_2 + f_3w_3 + f_4w_4 + f_5w_5 + f_6w_6\\\\ &= f. w\\\\ \\end{aligned} \\] The dot-product can also be written as a \\(f^Tw\\) . The matrix-product of a row-vector and a column-vector: \\[ b = f^T w = \\begin{bmatrix} 1 & f_1 & f_2 & f_3 & f_4 & f_5 & f_6 \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_4\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] Note All vectors will be expressed as column vectors. If we want to express a row-vector, then it will be denoted as \\(x^T\\) , where \\(x\\) is some column-vector.","title":"Vector form"},{"location":"week-3/2-linear_model/#matrix-form","text":"So much for one house. But we have several houses. All these can be clubbed into a data-matrix. This is nothing but stacking all feature-vectors one below the other. Likewise, we can stack all selling prices into a label-vector: \\[ \\begin{bmatrix} b_{1}\\\\ \\vdots\\\\ b_{100} \\end{bmatrix}= \\begin{bmatrix} 1 & f_{1,1} & f_{1,2} & f_{1,3} & f_{1,4} & f_{1,5} & f_{1,6}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 1 & f_{100,1} & f_{100,2} & f_{100,3} & f_{100,4} & f_{100,5} & f_{100,6}\\\\ \\end{bmatrix}\\begin{bmatrix} w_0\\\\ w_1\\\\ w_2\\\\ w_3\\\\ w_5\\\\ w_6 \\end{bmatrix} \\] If we call the feature matrix \\(A\\) , the label vector \\(b\\) and the weight vector \\(w\\) , this is the equation we have: \\[ Ax = b \\] We are given both \\(A\\) and \\(b\\) . This is nothing but our labeled dataset. We have to find \\(x\\) . This leaves us with two questions: Does the equation \\(Ax = b\\) have a solution? If it doesn't have a solution, then how do we estimate \\(x\\) ? We can see how a ML problem has now turned into a linear algebra problem! We will try to answer the first question and then move on to the second question. Note A lot of details about the linear regression model have been skipped. This presentation has tried to bring out the mathematical details. For a more accurate handling of this topic, please refer to week-2 of the MLT course.","title":"Matrix form"},{"location":"week-3/2-linear_model/#summary","text":"A linear regression model assumes a linear relationship between inputs and outputs. This results in a system of linear equation of the form \\(Ax = b\\) .","title":"Summary"},{"location":"week-3/3-homogenous/","text":"System of Equations-(1) \u00b6 Question How do we solve for \\(x\\) in the equation \\(Ax = 0\\) ? Setting \u00b6 Before we tackle the general problem of \\(Ax = b\\) , let us first see if we can solve the system when \\(b\\) is the zero vector. In all the discussions that follow, this will be our setting: \\(A\\) is a matrix of dimensions \\(m \\times n\\) \\(x\\) is a column-vector of size \\(n\\) \\(x \\in \\mathbb{R}^n\\) \\(Ax \\in \\mathbb{R}^{m}\\) The equation that we have taken up is: $$ Ax = 0 $$ Note We have to be careful with the use of \\(0\\) . Depending on the context, it could either mean a scalar or a vector. Nullspace \u00b6 We can immediately see that \\(x = 0\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(x_1\\) . Then, we can see that \\(k x_1\\) is also a solution. This is because \\(A (kx_1) = k \\cdot Ax_1 = 0\\) . Also, if \\(x_1\\) and \\(x_2\\) are two solutions to the equation, then \\(x_1 + x_2\\) is also a solution, as \\(A(x_1 + x_2) = Ax_1 + Ax_2 = 0\\) . From these two observations, we see that the set of all solutions to the equation \\(Ax = 0\\) is a subspace of \\(\\mathbb{R}^{n}\\) . We denote this by \\(N(A)\\) and call it the nullspace of \\(A\\) . The dimension of the nullspace is called nullity. All this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B = \\{v_1, \\cdots, v_k\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(N(A) = \\text{span}(B)\\) . Row-Echelon form \u00b6 Let us take up an example and work with that: \\[ A = \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Recall that we can apply a sequence of any of these three row operations on a matrix: swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row Step-1 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Step-2 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\] Step-3 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\] The final matrix that we have is in row-echelon form. Here is a quick reminder of what the row-echelon matrix is: All rows that have only zeros are at the bottom. The first nonzero entry in a row is always to the right of the nonzero entry in the row above it. The first nonzero entry in a row is called a pivot. Let us call the row-echelon matrix of \\(A\\) as \\(B\\) . We state the following result without a proof. Useful result \\(Bx = 0\\) if and only if \\(Ax = 0\\) Thus, the nullspace of a matrix and its row-echelon matrix are the same. This lets us forget about \\(A\\) and deal with its row-echelon form directly. Recipe for a Basis \u00b6 Now we have to solve the following equation: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\] Columns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called \"dependent variables\", while the others are called \"independent variables\". Algorithm \\(B = \\{ \\}\\) For each independent variable \\(x_i\\) : Set \\(x_i = 1\\) and \\(x_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(x\\) to \\(B\\) \\(B\\) is the required basis. Let us try it out here. \\(x_1\\) and \\(x_2\\) are the dependent variables. \\(x_3\\) and \\(x_4\\) are the independent variables. First, let us set \\(x_3 = 1, x_4 = 0\\) . This gives us \\(x_1 = 1, x_2 = -2\\) . The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^T\\) . Next, we set \\(x_3 = 0, x_4 = 1\\) . This gives us \\(x_1 = 0, x_2 = -1\\) . The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^T\\) . Thus, the basis for \\(N(A)\\) is: \\[ B = \\left \\{ \\begin{bmatrix}1\\\\ -2\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\\\ -1\\\\ 0\\\\ 1\\end{bmatrix} \\right \\} \\] The set of all solutions for the equation \\(Ax = 0\\) is \\(\\text{span}(B)\\) . Summary \u00b6 In order to solve the equation \\(Ax = 0\\) , we first reduce the matrix \\(A\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the null space. The span of the basis is the set of all solutions to this equation.","title":"System of Equations-(1)"},{"location":"week-3/3-homogenous/#system-of-equations-1","text":"Question How do we solve for \\(x\\) in the equation \\(Ax = 0\\) ?","title":"System of Equations-(1)"},{"location":"week-3/3-homogenous/#setting","text":"Before we tackle the general problem of \\(Ax = b\\) , let us first see if we can solve the system when \\(b\\) is the zero vector. In all the discussions that follow, this will be our setting: \\(A\\) is a matrix of dimensions \\(m \\times n\\) \\(x\\) is a column-vector of size \\(n\\) \\(x \\in \\mathbb{R}^n\\) \\(Ax \\in \\mathbb{R}^{m}\\) The equation that we have taken up is: $$ Ax = 0 $$ Note We have to be careful with the use of \\(0\\) . Depending on the context, it could either mean a scalar or a vector.","title":"Setting"},{"location":"week-3/3-homogenous/#nullspace","text":"We can immediately see that \\(x = 0\\) is a solution. However, this is a trivial solution and is not particularly interesting. We would like to search for non-trivial solutions. Let us begin optimistically by assuming that at least one such solution exists, say \\(x_1\\) . Then, we can see that \\(k x_1\\) is also a solution. This is because \\(A (kx_1) = k \\cdot Ax_1 = 0\\) . Also, if \\(x_1\\) and \\(x_2\\) are two solutions to the equation, then \\(x_1 + x_2\\) is also a solution, as \\(A(x_1 + x_2) = Ax_1 + Ax_2 = 0\\) . From these two observations, we see that the set of all solutions to the equation \\(Ax = 0\\) is a subspace of \\(\\mathbb{R}^{n}\\) . We denote this by \\(N(A)\\) and call it the nullspace of \\(A\\) . The dimension of the nullspace is called nullity. All this is fine, but how does it help us find all the solutions? If we can find a basis for the nullspace, that will help us characterize all the solutions. If \\(B = \\{v_1, \\cdots, v_k\\}\\) is a basis for the nullspace, then the set of all solutions to the equation is given by \\(N(A) = \\text{span}(B)\\) .","title":"Nullspace"},{"location":"week-3/3-homogenous/#row-echelon-form","text":"Let us take up an example and work with that: \\[ A = \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Recall that we can apply a sequence of any of these three row operations on a matrix: swap two rows scale a row by a non-zero constant add a scalar multiple of a row to another row Step-1 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 2 & 1 & 0 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_2 \\rightarrow R_2-2R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\] Step-2 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 3 & 1 & -1 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3-3R_1} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\] Step-3 \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 1 & 2 & 1 \\end{bmatrix} \\underrightarrow{R_3 \\rightarrow R_3 - R_2} \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\] The final matrix that we have is in row-echelon form. Here is a quick reminder of what the row-echelon matrix is: All rows that have only zeros are at the bottom. The first nonzero entry in a row is always to the right of the nonzero entry in the row above it. The first nonzero entry in a row is called a pivot. Let us call the row-echelon matrix of \\(A\\) as \\(B\\) . We state the following result without a proof. Useful result \\(Bx = 0\\) if and only if \\(Ax = 0\\) Thus, the nullspace of a matrix and its row-echelon matrix are the same. This lets us forget about \\(A\\) and deal with its row-echelon form directly.","title":"Row-Echelon form"},{"location":"week-3/3-homogenous/#recipe-for-a-basis","text":"Now we have to solve the following equation: \\[ \\begin{bmatrix} 1 & 0 & -1 & 0\\\\ 0 & 1 & 2 & 1\\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3\\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix} \\] Columns \\(1\\) and \\(2\\) are called the pivot columns as they contain the pivots. The variables corresponding to the pivots are called \"dependent variables\", while the others are called \"independent variables\". Algorithm \\(B = \\{ \\}\\) For each independent variable \\(x_i\\) : Set \\(x_i = 1\\) and \\(x_j = 0\\) for all independent variables, \\(j \\neq i\\) Solve for the dependent variables Add \\(x\\) to \\(B\\) \\(B\\) is the required basis. Let us try it out here. \\(x_1\\) and \\(x_2\\) are the dependent variables. \\(x_3\\) and \\(x_4\\) are the independent variables. First, let us set \\(x_3 = 1, x_4 = 0\\) . This gives us \\(x_1 = 1, x_2 = -2\\) . The first element of the basis is therefore \\(\\begin{bmatrix}1 & -2 & 1 & 0\\end{bmatrix}^T\\) . Next, we set \\(x_3 = 0, x_4 = 1\\) . This gives us \\(x_1 = 0, x_2 = -1\\) . The second element of the basis is \\(\\begin{bmatrix}0 & -1 & 0 & 1\\end{bmatrix}^T\\) . Thus, the basis for \\(N(A)\\) is: \\[ B = \\left \\{ \\begin{bmatrix}1\\\\ -2\\\\ 1\\\\ 0\\end{bmatrix}, \\begin{bmatrix}0\\\\ -1\\\\ 0\\\\ 1\\end{bmatrix} \\right \\} \\] The set of all solutions for the equation \\(Ax = 0\\) is \\(\\text{span}(B)\\) .","title":"Recipe for a Basis"},{"location":"week-3/3-homogenous/#summary","text":"In order to solve the equation \\(Ax = 0\\) , we first reduce the matrix \\(A\\) to its row-echelon form. Then we use an iterative algorithm to construct a basis for the null space. The span of the basis is the set of all solutions to this equation.","title":"Summary"}]}